Title,URL,Content
Introduction to Machine Learning,https://www.youtube.com/watch?v=h0e2HAPTGF4,"The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. ERIC GRIMSON: OK. Welcome back. You know, it's that time a term when we're all kind of doing this. So let me see if I can get a few smiles by simply noting to you that two weeks from today is the last class. Should be worth at least a little bit of a smile, right? Professor Guttag is smiling. He likes that idea. You're almost there. What are we doing for the last couple of lectures? We're talking about linear regression. And I just want to remind you, this was the idea of I have some experimental data. Case of a spring where I put different weights on measure displacements. And regression was giving us a way of deducing a model to fit that data. And In some cases it was easy. We knew, for example, it was going to be a linear model. We found the best line that would fit that data. In some cases, we said we could use validation to actually let us explore to find the best model that would fit it, whether a linear, a quadratic, a cubic, some higher order thing. So we'll be using that to deduce something about a model. That's a nice segue into the topic for the next three lectures, the last big topic of the class, which is machine learning. And I'm going to argue, you can debate whether that's actually an example of learning. But it has many of the elements that we want to talk about when we talk about machine learning. So as always, there's a reading assignment. Chapter 22 of the book gives you a good start on this, and it will follow up with other pieces. And I want to start by basically outlining what we're going to do. And I'm going to begin by saying, as I'm sure you're aware, this is a huge topic. I've listed just five subjects in course six that all focus on machine learning. And that doesn't include other subjects where learning is a central part. So natural language processing, computational biology, computer vision robotics all rely today, heavily on machine learning. And you'll see those in those subjects as well. So we're not going to compress five subjects into three lectures. But what we are going to do is give you the introduction. We're going to start by talking about the basic concepts of machine learning. The idea of having examples, and how do you talk about features representing those examples, how do you measure distances between them, and use the notion of distance to try and group similar things together as a way of doing machine learning. And we're going to look, as a consequence, of two different standard ways of doing learning. One, we call classification methods. Example we're going to see, there is something called ""k nearest neighbor"" and the second class, called clustering methods. Classification works well when I have what we would call labeled data. I know labels on my examples, and I'm going to use that to try and define classes that I can learn, and clustering working well, when I don't have labeled data. And we'll see what that means in a couple of minutes. But we're going to give you an early view of this. Unless Professor Guttag changes his mind, we're probably not going to show you the current really sophisticated machine learning methods like convolutional neural nets or deep learning, things you'll read about in the news. But you're going to get a sense of what's behind those, by looking at what we do when we talk about learning algorithms. Before I do it, I want to point out to you just how prevalent this is. And I'm going to admit with my gray hair, I started working in AI in 1975 when machine learning was a pretty simple thing to do. And it's been fascinating to watch over 40 years, the change. And if you think about it, just think about where you see it. AlphaGo, machine learning based system from Google that beat a world-class level Go player. Chess has already been conquered by computers for a while. Go now belongs to computers. Best Go players in the world are computers. I'm sure many of you use Netflix. Any recommendation system, Netflix, Amazon, pick your favorite, uses a machine learning algorithm to suggest things for you. And in fact, you've probably seen it on Google, right? The ads that pop up on Google are coming from a machine learning algorithm that's looking at your preferences. Scary thought. Drug discovery, character recognition-- the post office does character recognition of handwritten characters using a machine learning algorithm and a computer vision system behind it. You probably don't know this company. It's actually an MIT spin-off called Two Sigma, it's a hedge fund in New York. They heavily use AI and machine learning techniques. And two years ago, their fund returned a 56% return. I wish I'd invested in the fund. I don't have the kinds of millions you need, but that's an impressive return. 56% return on your money in one year. Last year they didn't do quite as well, but they do extremely well using machine learning techniques. Siri. Another great MIT company called Mobileye that does computer vision systems with a heavy machine learning component that is used in assistive driving and will be used in completely autonomous driving. It will do things like kick in your brakes if you're closing too fast on the car in front of you, which is going to be really bad for me because I drive like a Bostonian. And it would be kicking in constantly. Face recognition. Facebook uses this, many other systems "
Introduction to Machine Learning,https://www.youtube.com/watch?v=KNAWp2S3w94,"♪ (music) ♪ You've probably heard a lot about AI machine learning over the last few months. And maybe you've been inspired by videos showing what's possible with AI machine learning. But what is it really? Once you go beyond the hype and get down to writing code, what does AI really look like? Well, that's what we're going to go through in this video series, where we'll teach you what it's like to write code for machine learning, and how it provides different, new, and exciting scenarios that will help you write applications that behave more like a human being, giving you artificial intelligence. I'm Laurence, and I'm going to be your guide. You don't need to know a lot to get started, and we'll be using the Python language. Don't worry if you've never used it, it's super simple to understand, and you'll be up and running in no time. So let's start with a very simple example. Consider you're creating a game of Rock, Paper, Scissors. When you play this with a human, it's very basic; every child can learn it in just a few minutes. Now, let's take a look at the most basic part of a game that the human brain is really good at, and that's recognizing what it's actually looking at. So consider these images. Most people can look at them and instantly recognize which ones are rock, which ones are paper, and which ones are scissors. But how would you program a computer to recognize them? Think about all of the diversity of hand types, skin color, and even people who do scissors like me, with their thumb sticking out, and people who do scissors with their thumb in. If you've ever written any kind of code, you'll instantly realize that this is a really, really difficult task. It might take you thousands or tens of thousands of lines of code, and that's just to play rock, paper, or scissors. So what if there was a different way to teach a computer to recognize what it sees? What if you could have a computer learn in the same way that a human does? That's the core of machine learning and the path to artificial intelligence. So traditional programming looks like this. You have data, for example, a feed from the webcam, and you have rules that act on this data. These rules are expressed in a programming language and are the bulk of any code that you write. Ultimately, these rules will act on the data and give you an answer. Maybe it sees a rock, maybe it sees a paper, and maybe it sees scissors. But what if you turn this diagram around, and instead of you as the programmer figuring out the rules, you instead give it answers with the data and have the computer figure out what the rules are. That's machine learning. So now, I can have lots of pictures of rocks and tell a computer that this is what a rock looks like, and this is what paper looks like, and this is what scissors looks like. And I can have a computer figure out the patterns that match them to each other. Then, my computer will have learned to recognize a rock, paper, and scissors. That's the core of building something that uses machine learning. You get a set of data that has patterns inherent in it, and you have a computer learn what those patterns are. Before we write a neural network that learns something as complex as rock, paper, and scissors, let's use a much simpler example. Take a look at these numbers-- there's a relationship between the X and Y values. Can you see it? It's actually Y = 2X - 1. So if you saw it, how did you get that? Maybe you noticed that the Y value increases by 2, while the X value only increases by 1. So it was Y = 2X plus a minus something. And then, you may have seen that when X was zero, Y was minus one, so you figured Y = 2X - 1 would be a good guess, and then you took a look at the other numbers and saw that it worked. That's exactly the principle that all machine learning works on. So let's take a look. This is the entire code that you can use to create a machine-learned model that figures out what matches these numbers to each other. Don't worry if some of it doesn't look very familiar right now, you'll be able to pick that up in no time. This first line defines the model itself. A model is a trained neural network, and here we have the simplest possible neural network, which, in this case, is a single layer indicated by the keras.layers.Dense code. And that layer has a single neuron in it, indicated by units = 1. We also feed a single value into the neural network, which is the X value, and we'll have the neural network predict what the Y would be for that X. So that's why we just say that input_shape is one value. When you compile the model, there are two functions: the loss and the optimizer. These are the key to machine learning. How machine learning works is that the model will make a guess about the relationship between the numbers. For example, it might guess that Y = 5X + 5. And when training, it will then calculate how good or how bad that guess is, using the loss function. And then, it will use the optimizer function to generate another guess. The logic is that the combination of these two functions will slowly get us closer and closer to the correct formula. And, in this case, it will go through that loop 500 times, making a guess, calculating how accurate that guess is, and then using the optimizer to enhance that guess, and so on. The data itself is set up as an array of Xs and Ys, "
Introduction to Machine Learning,https://www.youtube.com/watch?v=FIpbrnbVfJk,foreign tools like chat GPT exist and all of a sudden a lot of you want to dive into machine learning and artificial intelligence now I'm not here to validate that decision or to tell you that's the best possible path to go down but I am here to provide you a road map and make it as easy and stress-free as possible for you to dive into this world I myself learned quite a bit about machine learning and AI back when I was in university this was about three years ago I actually have a ton of videos on this channel related to machine learning and artificial intelligence and you can check those out for the link in the description I'll pop some of them up on the screen so I have a really good sense of what it takes to dive into this field what kind of algorithms you should be learning the level of math you need to know what programming language you should learn libraries and the next more advanced steps that you can take if you really are getting serious about that so with that said let's dive in and let me share with you how to learn machine learning and artificial intelligence in 2023 so let's begin here by walking through the steps that's involved in training a machine learning model or really developing some kind of artificial intelligence you're going to realize very quickly here that very little of what you're going to do as a machine learning engineer or as someone who's interested in artificial intelligence is actually building machine learning models or artificial intelligence a lot of this has to do with data pre-processing and also things like deploying testing and validating your model so keep that in mind this is not as glorious and as glamorous as a field as you might make it out to be and there's a lot of very frustrating time consuming and difficult work that goes into actually being successful in training machine learning models and creating artificial intelligence anyways let me walk you through the steps so step one in any machine learning application or project is to define the problem what is the problem what are you trying to solve what are your goals and objectives and how is machine learning going to help you do that now step two is to pick the data that you're going to be using for your model so data collection where are you getting this data from how are you storing it and how are you going to decide what data you actually need the next step is going to be data cleaning oftentimes when you collect data especially if you're getting it from the internet some kind of Open Source you're going to need to clean this data and get rid of a bunch of information that's unnecessary too much data can confuse a machine learning model too little obviously could be not enough to train it and you have to be careful how you're cleaning this data and putting it in a format such that it's usable for your model the next step is going to be data selection so now you should have this massive amount of data depending on the problem obviously they're going to be training a model on now you need to actually select the specific pieces of data that you want to use to train the model not all of your data is going to be relevant maybe there's some filters you need to apply there's a lot of stuff that could go on in this step next is going to be your model architecture or selecting a model oftentimes you don't need to reinvent the wheel you don't need to create your own custom model you simply need to pick an existing one and then train it using your data so once you pick your model now you're going to train the model so you're going to feed all of your cleaned and processed data into that model and then you're going to test and evaluate your model and make sure that it's actually working as you expect once you do that you're going to repeat this step countless times until you get a model that has an accuracy that you are happy with and then if you're going in a production environment you are going to deploy this model so the reason I wanted to share these steps with you is just so you have a better picture of what's actually involved and what you're going to be doing if you get into this field most of your work is not going to be training a model coming up with a model architecture and doing all of that fun stuff you might see in YouTube videos or hear about In Articles a lot of it is going to have to do with data collecting data processing data and then testing and evaluating your model and continuing to repeat that process until you hit a point that you are satisfied with so after listening to those steps some of you might be a little bit concerned that your computer may not be powerful enough to actually execute those steps and train kind of more advanced deep learning models now if that's the case you don't have to worry because I'm actually giving away an RTX 4080 this is courtesy of Nvidia which I've teamed up with for this video to anyone who enters the giveaway now to enter the giveaway you simply need to attend the GTC this is the global technology conference hosted by Nvidia this is from March 20th to 23rd you can register for free from the link in the description and if you attend one of the events at GTC and then you fill out the Google form that I have in the description I will enter you to win this RTX 4080 I will ship it 
Introduction to Machine Learning,https://www.youtube.com/watch?v=9gGnTQTYNaE,"Hey, what's up everyone? My name is Luv Aggarwal, and I’m a Data Platform Solution engineer for IBM. Machine Learning. There's no doubt that this is an incredibly hot topic with significant interest from both business professionals as well as technologists. So let's talk about what machine learning, or ML, is. So, before we get too far into the details, I want to take a minute to talk about some terms that are often used interchangeably but have certain differences. Terms like “artificial intelligence”, “machine learning”, and even “deep learning”. So, at the highest level, AI is defined as leveraging computers or machines to mimic the problem-solving and the decision-making capabilities of the human mind. And machine learning is a subset within AI that's more focused on the use of various self-learning algorithms that derive knowledge from data in order to predict outcomes. And then, finally, deep learning is a further subset within even machine learning, and deep learning is often thought of as scalable machine learning because it automates a lot of the feature extraction process away and eliminates the some of the human intervention involved to enable the use of some really, really big data sets. But for today we'll focus just on machine learning, so we'll get rid of the other two and dive one level deeper and talk about the different types of machine learning. Ok. So, the first type that we have is called “supervised learning”. And this is when we use labeled data sets to train algorithms to classify data or predict outcomes. And when I say labeled, I mean that the rows in the data set are labeled, tagged, or classified in some interesting way that tells us something about that data. So, it could be a yes or a no, or it could be a particular category of some, you know, different attribute. OK, so how do we apply supervised machine learning techniques? Well, this really depends on your particular use-case. We could be using a classification model which recognizes and groups ideas or objects into predefined categories. An example of this in the real world is with customer retention. So, if you're in the business of managing customers, one of your goals is typically minimizing and identifying customer churn, right, which are customers that no longer buy a particular product or service, and we want to avoid churn because it's almost always more costly to acquire a new customer than it is to retain an existing one, right? So, if we have historical data for the customer, like their activity - whether they churned or not, right - we can build a classification model using supervised machine learning, and our labeled data set that will help us identify customers that are about to churn, and then allow us to take action to retain them. OK, so the other type of supervised learning is regression. Now, this is when we build an equation using various input values with their specific weights determined by the overall value of their impact on the outcome. And we use these to generate an estimate for an output value. So, let me give you another example here. So, airlines rely heavily on machine learning, and they use regression techniques to accurately predict how much they should be charging for a particular flight, right? So, they use various input factors like, you know, days before departure, the day of the week, the departure, the destination to use these to predict an accurate dollar value for how much they should be charging for a specific flight that will maximize their revenue. OK, so now let's move on to the second type of machine learning which is “unsupervised learning”. OK, so this is when we use machine learning algorithms to analyze and cluster unlabeled data sets, and this method helps us discover hidden patterns or groupings without the need for human intervention, right? So, we're using unlabeled data here. So, again, let's talk about the different techniques for unsupervised learning. One method is “clustering”. And a real-world example of this is when organizations try to do customer segmentation. Right. So, when businesses try to do effective marketing it's really critical that they really understand who their customers are, right, so that they can connect with them in the most relevant way. And, oftentimes, it's not obvious or clear how certain customers are similar to or different from one another, right, and clustering algorithms can help take into account a variety of information on the customer like their purchase history, you know, their social media activity, or website activity, could be their geography, and much more, to group similar customers into buckets so that we can send them more relevant offers, provide them better customer service, and be more targeted with our marketing efforts. Ok. And the last point I want to touch on for unsupervised learning is called “dimensionality reduction”. So, we won't discuss this in detail in this video, but this refers to techniques that reduce the number of input variables in a data set so we don't let some redundant parameters over represent the impact on the on the outcome. Ok. So the last type of machine learning I want to talk about today is called “reinforcement learning”. Now, this is a form of semi-supervised learning where we typically have an agent or system take actions in an environment. Now the environment will then either reward the agent for correct moves, or punish it for incorrect moves. Right. And, through many iterations of this, we can teach a system a particular task. Now a great example of this method in the real world is with self-driving cars. So, autonomous driving has several factors, right? There's the speed limit, there are drivable zones, there are collisions, and so on. So, we can use forms of reinforcement learning to teach a system how to drive by avoiding collisions, following the speed limit, and so on. OK, so we covered many topics today, but you know, "
Introduction to Machine Learning,https://www.youtube.com/watch?v=Gv9_4yMHFhI,gonna start this tech quest with silly song but if you don't like silly songs that's okay stack quests hello I'm Josh stormer and welcome to stack quest today we're going to do a gentle introduction to machine learning note this stack quest was originally prepared for and presented at the Society for scientific advancements annual conference one of the things that Sosa does is promote science and technology in Jamaica let's start with a silly example do you like silly songs if you like silly songs are you interested in machine learning if you like silly songs and machine learning then you'll love stack quest if you like silly songs but not machine learning are you interested in statistics if you like silly songs and statistics but not machine learning then you'll still love stack quest otherwise you might not like stack quest won't Wang if you don't like silly songs are you interested in machine learning if you don't like silly songs but you like machine learning then you'll love stack quest if you don't like silly songs or machine learning are you interested in statistics if you don't like silly songs or machine learning but you're interested in statistics then you will love stack quest otherwise you might not like stack quest wah wah this is a silly example but it illustrates a decision tree a simple machine learning method the purpose of this particular decision tree is to predict whether or not someone will love stack quest alternatively we could say that this decision tree classifies a person as either someone who loves stack quest or someone who doesn't since decision trees are a type of machine learning then if you understand how we use this tree to predict or classify if someone would love stack quest you are well on your way to understanding machine learning BAM here's another silly example of machine learning imagine we measured how quickly someone could run 100 meters and how much yam they ate this is me I'm not very fast and I don't eat much yam these are some other people and this is Shane bolt hold is very fast Andy eats a lot of yam given this pretend data we see that the more yam someone eats the faster they run the 100-meter dash we can fit a black line to the data to show the trend but we can also use the black line to make predictions for example if someone told us they ate this much yam then we could use the black line to predict how fast that person might run this is the predicted speed the black line is a type of machine learning because we can use it to make predictions in general machine learning is all about making predictions and classifications BAM now that we can make predictions and classifications let's talk about some of the main ideas in machine learning first of all in machine learning lingo the original data is called training data so the black line is fit to training data alternatively we could have fit a green squiggle to the training data the green squiggle fits the training data better than the black line but remember the goal of machine learning is to make predictions so we need a way to decide if the green squiggle is better or worse than the black line at making predictions so we find a new person and measure how fast they run and how much ham they eat and then we find another and another and another altogether the blue dots represent testing data we use the testing data to compare the predictions made by the black line to the predictions made by the green squiggle let's start by seeing how well the black line predicts the speed of each person in the testing data here's the first person in the testing data they ate this much yam and they ran this fast however the black line predicts that someone who ate this much yam should run a little slower so let's measure the distance between the actual speed and the predicted speed and save the distance on the right while we focus on the other people in the testing data here's the second person in the testing data they ate this much yam and they ran this fast but the black line predicts that they will run a little faster so we measure the distance between the actual speed and the predicted speed and add it to the one we measured for the first person in the testing data then we measure the distance between the real and the predicted speed for the third person in the testing data and add it to our running total of distances between the real and predicted speeds for the black line then we do the same thing for the fourth person in the testing data and add that distance to our running total for the black line this is the sum of all the distances between the real and predicted speeds for the black line now let's calculate the distances between the real and predicted speeds using the green squiggle remember the green squiggle did a great job fitting the training data but when we are doing machine learning we are more interested in how well the green squiggle can make predictions with new data so just like before we determine this person's real speed and their predicted speed and measure the distance between them and just like we did for the black line we'll keep track of the distances for the green squiggle over here then we do the same thing for the second person in the testing data and the third person and the fourth person this is the sum of the distances between the real and predicted speeds for the green squiggle the sum of the distances is larger for the green squiggle than the black line in other words even 
Supervised Learning: Regression,https://www.youtube.com/watch?v=W01tIRP_Rqs,"Supervised and unsupervised learning are two core components in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them and then answer the question of which is best for you. Now, in supervised learning, the machine learning algorithm is trained on a labeled dataset. So this means that each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seen before. Now, using labeled inputs and outputs, the model can measure its accuracy and learn over time. Supervised learning can be actually divided into a couple of subcategories. Firstly, there is a category of classification. And classification talks about whether the output is a discrete class label such as ""spam"" and ""not spam"". Linear classifiers, support vector machines, or SPMs, decision trees, random forests - they're all common examples of classification algorithms. The other example is regression. The output here is a continuous value, such as price or probability. Linear regression and logistic regression are two common types of regression algorithms. Now, unsupervised learning is where the machine learning algorithm is not really given any labels at all. And these algorithms discover hidden patterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionality reduction. So let's take a look at each one of those, starting with clustering. Now clustering is where the algorithm groups similar experiences together. So a common application of clustering is customer segmentation, where businesses might group customers together based on similarities like, I don't know, age or location or spending habits, something like that. Then you have association. And association is where the algorithm looks for relationships between variables in the data. Now association rules are often used in market basket analysis, where businesses want to know which items are often bought together. You know, something along the lines of, ""customers who bought this item also bought "", that sort of thing. The final one to talk about is dimensional ... dimensional reduction. And this is where the algorithm reduces the number of variables in the data, while still preserving as much of the information as possible. Now, often this technique is used in the pre-processing data stage, such as when autoencoders remove noise from visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervised learning, the algorithm learns from training datasets by iteratively making predictions on the data and then adjusting for the correct answer. While supervised learning models tend to be more accurate than unsupervised learning models, they do require all of this up-front human intervention to label the data appropriately. For example, a supervised learning model can predict how long your commute will be on the time of day and thinking about the weather conditions and so forth. But first you'll have to train it to know things like rainy weather extends the driving time. By contrast, unsupervised learning models work on their own to discover the inherent structure of unlabeled data. These models don't need humans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were ahead of time. Now, an important distinction to make is that unsupervised learning models don't make predictions. They only group data together. So if you were to use an unsupervised learning model on that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long each commute would take. Okay, so which of these two options is right for you? In general, supervised learning is more commonly used than unsupervised learning, and that's really because it's more accurate and efficient. But that being said, unsupervised learning has its own advantages. There's two that I can think of. Firstly, unsupervised learning can be used on data that is not labeled, which is often the case in real world datasets. And then secondly, unsupervised learning can be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highly accurate and trustworthy. And in contrast, unsupervised learning can handle large volumes of data in real time. But there's a lack of transparency into how that data is clustered and a high risk given accurate results. But wait, it is not an ""either/or"" choice. May I present to you the middle ground known as semi-supervised learning. This is, well, a happy medium where you use a training data set with both labeled and unlabeled data. And it's particularly useful when it's difficult to extract relevant features from data when you have a high volume of data. So, for example, you could use a semi-supervised learning algorithm on a data set with millions of images where only a few thousand of those images are actually labeled. Semi-supervised learning is ideal for medical images, where a small amount of training data could lead to a significant improvement in accuracy. For example, a radiologist can look at and label some small subset of CT scans for tumors or diseases, and then the machine can more accurately predict which patients might require more medical attention without going through and labeling the entire set. Machine learning models are a powerful way to gain the data insights that improve our world. The right model for your data depends on the type of data that you have and what you want to do with it. "
Supervised Learning: Regression,https://www.youtube.com/watch?v=qxo8p8PtFeA,I have a confession to make I don't like math well at least if you're like me you might not like pure theoretical math whenever I would be sitting in my math classes like calculus or algebra or trigonometry I always wondered if I would ever have to use these Concepts however if you're like me you might love computer science I love computer science because it helps us explain math topics in a way that's easy for our computer to understand and that makes it a little easier for me to understand too that's important because when we're studying topics like machine learning and artificial intelligence it's important for us to be able to explain to the computer how to apply mathematical Concepts to large amounts of data so one important topic in machine learning is linear regression you may have heard of the term linear regression and you're not sure if it's a good fit for your data set well look no further I'm going to explain what linear regression is at a basic level and give you a short example of how it can be used in a data set with two variables linear regression is a form of supervised machine learning which means that it uses a labeled data set we're using linear regression to make predictions on continuous data which is numerical data that can have infinite values so continuous data can be something like height or age or even foot size this is in contrast with categorical data which have finite distinct values this can be things like color or the species of an animal like a cat or a dog these types of data would probably not be a good candidate for linear regression because there are finite types of um categories that they can be in however if your data looks like this let's keep going let's take a look at this graph let's say for example we want to build a machine learning model that will identify if there's a positive correlation between a person's height and their shoe size and we have a bunch of data points that we want to use to predict those future values so let's see if there's a positive correlation between height and shoe size right now height and inches is our independent variable X and shoe size is our dependent variable y we want to model the relationship by X and Y by plotting all of these points on the graph and then finding the best fit line between all of those points so for example let's say that we have a data point where a person who's 62 in has a shoe size of six a person who's 63 in has a shoe size of N9 64 in has a shoe size of seven 65 in has a shoe size of seven and 66 in has a shoe size of eight now keep in mind I actually am 63 in and and I have a shoe size of nine so now that we have this data let's try to find the best fit line between all of these data points just by looking at this we'll probably do something like this that would be the best fit line between all of our data points it allows us to see a trend in the data and make predictions and it also allows us to identify outliers such as myself overall linear regression is a great algorithm to start with if you're just getting started with machine learning models you don't have to love pure math to work with machine learning and AI as long as you're able to recognize relationships and patterns between data and you have the passion for it overall happy coding thanks for watching in the comments below please feel free to make your own predictions about how linear regression can help you and as always please remember to click like And subscribe 
Supervised Learning: Regression,https://www.youtube.com/watch?v=4b4MUYve_U8,"Morning and welcome back. So what we'll see today in class is the first in-depth discussion of a learning algorithm, linear regression, and in particular, over the next, what, hour and a bit you'll see linear regression, batch and stochastic gradient descent is an algorithm for fitting linear regression models, and then the normal equations, um, uh, as a way of- as a very efficient way to let you fit linear models. Um, and we're going to define notation, and a few concepts today that will lay the foundation for a lot of the work that we'll see the rest of this quarter. Um, so to- to motivate linear regression, it's gonna be, uh, maybe the- maybe the simplest, one of the simplest learning algorithms. Um, you remember the ALVINN video, the autonomous driving video that I had shown in class on Monday, um, for the self-driving car video, that was a supervised learning problem. And the term supervised learning [NOISE] meant that you were given Xs which was a picture of what's in front of the car, and the algorithm [NOISE] had to map that to an output Y which was the steering direction. And that was a regression problem, [NOISE] because the output Y that you want is a continuous value, right? As opposed to a classification problem where Y is the speed. And we'll talk about classification, um, next Monday, but supervised learning regression. So I think the simplest, maybe the simplest possible learning algorithm, a supervised learning regression problem, is linear regression. And to motivate that, rather than using a self-driving car example which is quite complicated, we'll- we'll build up a supervised learning algorithm using a simpler example. Um, so let's say you want to predict or estimate the prices of houses. So [NOISE] the way you build a learning algorithm is start by collecting a data-set of houses, and their prices. Um, so this is a data-set that we collected off Craigslist a little bit back. This is data from Portland, Oregon. [NOISE] But so there's the size of a house in square feet, [NOISE] um, and there's the price of a house in thousands of dollars, [NOISE] right? And so there's a house that is 2,104 square feet whose asking price was $400,000. Um, [NOISE] house with, uh, that size, with that price, [NOISE] and so on. Okay? Um, and maybe more conventionally if you plot this data, there's a size, there's a price. So you have some dataset like that. And what we'll end up doing today is fit a straight line to this data, right? [NOISE] And go through how to do that. So in supervised learning, um, the [NOISE] process of supervised learning is that you have a training set such as the data-set that I drew on the left, and you feed this to a learning algorithm, [NOISE] right? And the job of the learning algorithm is to output a function, uh, to make predictions about housing prices. And by convention, um, I'm gonna call this a function that it outputs a hypothesis, [NOISE] right? And the job of the hypothesis is, [NOISE] you know, it will- it can input the size of a new house, or the size of a different house that you haven't seen yet, [NOISE] and will output the estimated [NOISE] price. Okay? Um, so the job of the learning algorithm is to input a training set, and output a hypothesis. The job of the hypothesis is to take as input, any size of a house, and try to tell you what it thinks should be the price of that house. Now, when designing a learning algorithm, um, and- and, you know, even though linear regression, right? You may have seen it in a linear algebra class before, or some other class before, um, the way you go about structuring a machine learning algorithm is important. And design choices of, you know, what is the workflow? What is the data-set? What is the hypothesis? How does this represent the hypothesis? These are the key decisions you have to make in pretty much every supervised learning, every machine learning algorithm's design. So, uh, as we go through linear regression, I will try to describe the concepts clearly as well because they'll lay the foundation for the rest of the algorithms. Sometimes it's much more complicated with the algorithms you'll see later this quarter. So when designing a learning algorithm the first thing we need to ask is, um, [NOISE] how- how do you represent the hypothesis, H, right? And in linear regression, for the purpose of this lecture, [NOISE] we're going to say that, um, the hypothesis is going to be [NOISE] that. Right? That the input, uh, size X, and output a number as a- as a linear function, um, of the size X, okay? And then, the mathematicians in the room, you'll say technically this is an affine function. It was a linear function, there's no theta 0, technically, you know, but- but the machine learning sometimes just calls this a linear function, but technically it's an affine function. Doesn't- doesn't matter. Um, so more generally in- in this example we have just one input feature X. More generally, if you have multiple input features, so if you have more data, more information about these houses, such as number of bedrooms [NOISE] Excuse me, my handwriting is not big. That's the word bedrooms, [NOISE] right? Then, I guess- [NOISE] All right. Yeah. Cool. My- my- my father-in-law lives a little bit outside Portland, uh, and he's actually really into real estate. So this is actually a real data-set from Portland. [LAUGHTER] Um, so more generally, uh, if you know the size, as well as the number of bedrooms in these houses, then you may have two input features [NOISE] where X1 is the size, and X2 is the number of bedrooms. [NOISE] Um, I'm using the pound sign bedrooms to denote number of bedrooms, "
Supervised Learning: Regression,https://www.youtube.com/watch?v=VWCRDH1_rv0,regression is an instance of supervised learning which we covered in a previous video it is a reliable method of identifying which variables have an impact on a topic of interest it is used for estimating the relationships between a dependent variable often called the outcome variable and one or more independent variables often called predictors covariates or features the process of performing a regression allows you to confidently determine which factors matter most which factors can be ignored and how these factors influence each other let's see a simple example suppose attendees satisfaction with the event is our dependent variable the topics covered length of sessions food provided and the cost of a ticket are our independent variables in this case we'd want to measure the historical levels of satisfaction with the events from the past three years or so as well as any information possible in regards to the independent variables perhaps we are particularly curious about how the price of a ticket to the event has impacted levels of satisfaction to begin investigating whether or not there is a relationship between these two variables we would begin by plotting these data points on a chart with the satisfaction on the y-axis and price on the x-axis once your data is plotted you may begin to see correlations if the theoretical chart above did indeed represent the impact of ticket prices on events satisfaction then we'd be able to confidently say that the higher the ticket price the higher the levels of event satisfaction we can then trace a function representing our data as accurately as possible in this example the line would fit the data it is called linear regression the regression line represents the relationship between our independent variable the price of a ticket and our dependent variable the satisfaction in short regression is used to identify a tendency between variables and deduce an output from it there are multiple regression algorithms like linear regression polynomial regression elastic net regression and much or subscribe to not miss any of these terms clearly explained you 
Supervised Learning: Regression,https://www.youtube.com/watch?v=i_LwzRVP7bg,"Kylie Ying has worked at many interesting places such as MIT, CERN, and Free Code Camp. She's a physicist, engineer, and basically a genius. And now she's going to teach you about machine learning in a way that is accessible to absolute beginners. What's up you guys? So welcome to Machine Learning for Everyone. If you are someone who is interested in machine learning and you think you are considered as everyone, then this video is for you. In this video, we'll talk about supervised and unsupervised learning models, we'll go through maybe a little bit of the logic or math behind them, and then we'll also see how we can program it on Google CoLab. If there are certain things that I have done, and you know, you're somebody with more experience than me, please feel free to correct me in the comments and we can all as a community learn from this together. So with that, let's just dive right in. Without wasting any time, let's just dive straight into the code and I will be teaching you guys concepts as we go. So this here is the UCI machine learning repository. And basically, they just have a ton of data sets that we can access. And I found this really cool one called the magic gamma telescope data set. So in this data set, if you want to read all this information, to summarize what I what I think is going on, is there's this gamma telescope, and we have all these high energy particles hitting the telescope. Now there's a camera, there's a detector that actually records certain patterns of you know, how this light hits the camera. And we can use properties of those patterns in order to predict what type of particle caused that radiation. So whether it was a gamma particle, or some other head, like hadron. Down here, these are all of the attributes of those patterns that we collect in the camera. So you can see that there's, you know, some length, width, size, asymmetry, etc. Now we're going to use all these properties to help us discriminate the patterns and whether or not they came from a gamma particle or hadron. So in order to do this, we're going to come up here, go to the data folder. And you're going to click this magic zero for data, and we're going to download that. Now over here, I have a colab notebook open. So you go to colab dot research dot google.com, you start a new notebook. And I'm just going to call this the magic data set. So actually, I'm going to call this for code camp magic example. Okay. So with that, I'm going to first start with some imports. So I will import, you know, I always import NumPy, I always import pandas. And I always import matplotlib. And then we'll import other things as we go. So yeah, we run that in order to run the cell, you can either click this play button here, or you can on my computer, it's just shift enter and that that will run the cell. And here, I'm just going to order I'm just going to, you know, let you guys know, okay, this is where I found the data set. So I've copied and pasted this actually, but this is just where I found the data set. And in order to import that downloaded file that we we got from the computer, we're going to go over here to this folder thing. And I am literally just going to drag and drop that file into here. Okay. So in order to take a look at, you know, what does this file consist of, do we have the labels? Do we not? I mean, we could open it on our computer, but we can also just do pandas read CSV. And we can pass in the name of this file. And let's see what it returns. So it doesn't seem like we have the label. So let's go back to here. I'm just going to make the columns, the column labels, all of these attribute names over here. So I'm just going to take these values and make that the column names. All right, how do I do that? So basically, I will come back here, and I will create a list called calls. And I will type in all of those things. With f size, f conk. And we also have f conk one. We have f symmetry, f m three long, f m three trans, f alpha. Let's see, we have f dist and class. Okay, great. Now in order to label those as these columns down here in our data frame. So basically, this command here just reads some CSV file that you pass in CSV has come about comma separated values, and turns that into a pandas data frame object. So now if I pass in a names here, then it basically assigns these labels to the columns of this data set. So I'm going to set this data frame equal to DF. And then if we call the head is just like, give me the first five things, give me the first five things. Now you'll see that we have labels for all of these. Okay. All right, great. So one thing that you might notice is that over here, the class labels, we have G and H. So if I actually go down here, and I do data frame class unique, you'll see that I have either G's or H's, and these stand for gammas or hadrons. And our computer is not so good at understanding letters, right? Our computer is really good at understanding numbers. So what we're going to do is we're going to convert this to zero for G and one for H. So here, I'm going to set this equal to this, whether or not that equals G. And then "
Supervised Learning: Classification,https://www.youtube.com/watch?v=W01tIRP_Rqs,"Supervised and unsupervised learning are two core components in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them and then answer the question of which is best for you. Now, in supervised learning, the machine learning algorithm is trained on a labeled dataset. So this means that each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seen before. Now, using labeled inputs and outputs, the model can measure its accuracy and learn over time. Supervised learning can be actually divided into a couple of subcategories. Firstly, there is a category of classification. And classification talks about whether the output is a discrete class label such as ""spam"" and ""not spam"". Linear classifiers, support vector machines, or SPMs, decision trees, random forests - they're all common examples of classification algorithms. The other example is regression. The output here is a continuous value, such as price or probability. Linear regression and logistic regression are two common types of regression algorithms. Now, unsupervised learning is where the machine learning algorithm is not really given any labels at all. And these algorithms discover hidden patterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionality reduction. So let's take a look at each one of those, starting with clustering. Now clustering is where the algorithm groups similar experiences together. So a common application of clustering is customer segmentation, where businesses might group customers together based on similarities like, I don't know, age or location or spending habits, something like that. Then you have association. And association is where the algorithm looks for relationships between variables in the data. Now association rules are often used in market basket analysis, where businesses want to know which items are often bought together. You know, something along the lines of, ""customers who bought this item also bought "", that sort of thing. The final one to talk about is dimensional ... dimensional reduction. And this is where the algorithm reduces the number of variables in the data, while still preserving as much of the information as possible. Now, often this technique is used in the pre-processing data stage, such as when autoencoders remove noise from visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervised learning, the algorithm learns from training datasets by iteratively making predictions on the data and then adjusting for the correct answer. While supervised learning models tend to be more accurate than unsupervised learning models, they do require all of this up-front human intervention to label the data appropriately. For example, a supervised learning model can predict how long your commute will be on the time of day and thinking about the weather conditions and so forth. But first you'll have to train it to know things like rainy weather extends the driving time. By contrast, unsupervised learning models work on their own to discover the inherent structure of unlabeled data. These models don't need humans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were ahead of time. Now, an important distinction to make is that unsupervised learning models don't make predictions. They only group data together. So if you were to use an unsupervised learning model on that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long each commute would take. Okay, so which of these two options is right for you? In general, supervised learning is more commonly used than unsupervised learning, and that's really because it's more accurate and efficient. But that being said, unsupervised learning has its own advantages. There's two that I can think of. Firstly, unsupervised learning can be used on data that is not labeled, which is often the case in real world datasets. And then secondly, unsupervised learning can be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highly accurate and trustworthy. And in contrast, unsupervised learning can handle large volumes of data in real time. But there's a lack of transparency into how that data is clustered and a high risk given accurate results. But wait, it is not an ""either/or"" choice. May I present to you the middle ground known as semi-supervised learning. This is, well, a happy medium where you use a training data set with both labeled and unlabeled data. And it's particularly useful when it's difficult to extract relevant features from data when you have a high volume of data. So, for example, you could use a semi-supervised learning algorithm on a data set with millions of images where only a few thousand of those images are actually labeled. Semi-supervised learning is ideal for medical images, where a small amount of training data could lead to a significant improvement in accuracy. For example, a radiologist can look at and label some small subset of CT scans for tumors or diseases, and then the machine can more accurately predict which patients might require more medical attention without going through and labeling the entire set. Machine learning models are a powerful way to gain the data insights that improve our world. The right model for your data depends on the type of data that you have and what you want to do with it. "
Supervised Learning: Classification,https://www.youtube.com/watch?v=Lf2bCQIktTo,"Hello! In this video, we'll be covering Classification. The concept of categorizing data is based off of training with a set of data so that the machine can essentially learn boundaries that separate categories of data. Therefore, new data inputted into the model can be categorized based on where the point exists. Imagine that you were a machine learning model. Where would you put boundaries to classify this data? Please pause the video here and take a minute to observe the data points here. Did what you came up with look something like this? Depending on the data used to train the model, as well as the type of model used, there are virtually endless possibilities in the boundaries that could be created here, so don't worry if the ones you were thinking of weren't an exact match. So the point of this is if we had a new, out-of-sample data point, let's say right about here. And we wanted to classify this new point, the model can now do the classification for us! In this case, the point would most likely be predicted as a virginica. In K-Nearest Neighbor, data points are categorized and when determining the category of a new data point, the K nearest points are used in this process. This means that based on the value of K, the classification of the point can change. In this example here, we're trying to classify the black X labelled Xu, where the value of K is 5 or K=5. This is shown with the black arrows connecting the five nearest points. This can also be represented by a circle that encompasses these points. For the classification of this point in this case, we have four red points and one green point. Therefore, X_u, would be classified as a red point. Suppose we change the value of K to a larger number, such as K = 14 for example. We can see here that there are six red points and eight green points, therefore X_u would be classified as a green point. So now you should be able to understand and define what supervised learning is. "
Supervised Learning: Classification,https://www.youtube.com/watch?v=eg8DJYwdMyg,"The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation, or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. PROFESSOR: Hello, everybody. Before we start the material, a couple of announcements. As usual, there's some reading assignments, and you might be surprised to see something from Chapter 5 suddenly popping up. But this is my relentless attempt to introduce more Python. We'll see one new concept later today, list comprehension. Today we're going to look at classification. And you remember last, on Monday, we looked at unsupervised learning. Today we're looking at supervised learning. It can usually be divided into two categories. Regression, where you try and predict some real number associated with the feature vector, and this is something we've already done really, back when we looked at curve fitting, linear regression in particular. It was exactly building a model that, given some features, would predict a point. In this case, it was pretty simple. It was given x predict y. You can imagine generalizing that to multi dimensions. Today I'm going to talk about classification, which is very common, in many ways more common than regression for-- in the machine learning world. And here the goal is to predict a discrete value, often called a label, associated with some feature vector. So this is the sort of thing where you try and, for example, predict whether a person will have an adverse reaction to a drug. You're not looking for a real number, you're looking for will they get sick, will they not get sick. Maybe you're trying to predict the grade in a course A, B, C, D, and other grades we won't mention. Again, those are labels, so it doesn't have to be a binary label but it's a finite number of labels. So here's an example to start with. We won't linger on it too long. This is basically something you saw in an earlier lecture, where we had a bunch of animals and a bunch of properties, and a label identifying whether or not they were a reptile. So we start by building a distance matrix. How far apart they are, an in fact, in this case, I'm not using the representation you just saw. I'm going to use the binary representation, As Professor Grimson showed you, and for the reasons he showed you. If you're interested, I didn't produce this table by hand, I wrote some Python code to produce it, not only to compute the distances, but more delicately to produce the actual table. And you'll probably find it instructive at some point to at least remember that that code is there, in case you need to ever produce a table for some paper. In general, you probably noticed I spent relatively little time going over the actual vast amounts of codes we've been posting. That doesn't mean you shouldn't look at it. In part, a lot of it's there because I'm hoping at some point in the future it will be handy for you to have a model on how to do something. All right. So we have all these distances. And we can tell how far apart one animal is from another. Now how do we use those to classify animals? And the simplest approach to classification, and it's actually one that's used a fair amount in practice is called nearest neighbor. So the learning part is trivial. We don't actually learn anything other than we just remember. So we remember the training data. And when we want to predict the label of a new example, we find the nearest example in the training data, and just choose the label associated with that example. So here I've just drawing a cloud of red dots and black dots. I have a fuschia colored X. And if I want to classify X as black or red, I'd say well its nearest neighbor is red. So we'll call X red. Doesn't get much simpler than that. All right. Let's try and do it now for our animals. I've blocked out this lower right hand corner, because I want to classify these three animals that are in gray. So my training data, very small, are these animals. And these are my test set here. So let's first try and classify the zebra. We look at the zebra's nearest neighbor. Well it's either a guppy or a dart frog. Well, let's just choose one. Let's choose the guppy. And if we look at the guppy, it's not a reptile, so we say the zebra is not a reptile. So got one right. Look at the python, choose its nearest neighbor, say it's a cobra. The label associated with cobra is reptile, so we win again on the python. Alligator, it's nearest neighbor is clearly a chicken. And so we classify the alligator as not a reptile. Oh, dear. Clearly the wrong answer. All right. What might have gone wrong? Well, the problem with K nearest neighbors, we can illustrate it by looking at this example. So one of the things people do with classifiers these days is handwriting recognition. So I just copied from a website a bunch of numbers, then I wrote the number 40 in my own inimitable handwriting. So if we go and we look for, say, the nearest neighbor of four-- or sorry, of whatever that digit is. It is, I believe, this one. And sure enough that's the row of fours. We're OK on this. Now if we want to classify my zero, the actual nearest neighbor, in terms of the bitmaps if you will, turns out to be this guy. A very poorly written nine. I didn't make up this nine, it was it was already there. And the problem we see here when we use nearest neighbor is if something is noisy, if you have one noisy piece of data, "
Supervised Learning: Classification,https://www.youtube.com/watch?v=i_LwzRVP7bg,"Kylie Ying has worked at many interesting places such as MIT, CERN, and Free Code Camp. She's a physicist, engineer, and basically a genius. And now she's going to teach you about machine learning in a way that is accessible to absolute beginners. What's up you guys? So welcome to Machine Learning for Everyone. If you are someone who is interested in machine learning and you think you are considered as everyone, then this video is for you. In this video, we'll talk about supervised and unsupervised learning models, we'll go through maybe a little bit of the logic or math behind them, and then we'll also see how we can program it on Google CoLab. If there are certain things that I have done, and you know, you're somebody with more experience than me, please feel free to correct me in the comments and we can all as a community learn from this together. So with that, let's just dive right in. Without wasting any time, let's just dive straight into the code and I will be teaching you guys concepts as we go. So this here is the UCI machine learning repository. And basically, they just have a ton of data sets that we can access. And I found this really cool one called the magic gamma telescope data set. So in this data set, if you want to read all this information, to summarize what I what I think is going on, is there's this gamma telescope, and we have all these high energy particles hitting the telescope. Now there's a camera, there's a detector that actually records certain patterns of you know, how this light hits the camera. And we can use properties of those patterns in order to predict what type of particle caused that radiation. So whether it was a gamma particle, or some other head, like hadron. Down here, these are all of the attributes of those patterns that we collect in the camera. So you can see that there's, you know, some length, width, size, asymmetry, etc. Now we're going to use all these properties to help us discriminate the patterns and whether or not they came from a gamma particle or hadron. So in order to do this, we're going to come up here, go to the data folder. And you're going to click this magic zero for data, and we're going to download that. Now over here, I have a colab notebook open. So you go to colab dot research dot google.com, you start a new notebook. And I'm just going to call this the magic data set. So actually, I'm going to call this for code camp magic example. Okay. So with that, I'm going to first start with some imports. So I will import, you know, I always import NumPy, I always import pandas. And I always import matplotlib. And then we'll import other things as we go. So yeah, we run that in order to run the cell, you can either click this play button here, or you can on my computer, it's just shift enter and that that will run the cell. And here, I'm just going to order I'm just going to, you know, let you guys know, okay, this is where I found the data set. So I've copied and pasted this actually, but this is just where I found the data set. And in order to import that downloaded file that we we got from the computer, we're going to go over here to this folder thing. And I am literally just going to drag and drop that file into here. Okay. So in order to take a look at, you know, what does this file consist of, do we have the labels? Do we not? I mean, we could open it on our computer, but we can also just do pandas read CSV. And we can pass in the name of this file. And let's see what it returns. So it doesn't seem like we have the label. So let's go back to here. I'm just going to make the columns, the column labels, all of these attribute names over here. So I'm just going to take these values and make that the column names. All right, how do I do that? So basically, I will come back here, and I will create a list called calls. And I will type in all of those things. With f size, f conk. And we also have f conk one. We have f symmetry, f m three long, f m three trans, f alpha. Let's see, we have f dist and class. Okay, great. Now in order to label those as these columns down here in our data frame. So basically, this command here just reads some CSV file that you pass in CSV has come about comma separated values, and turns that into a pandas data frame object. So now if I pass in a names here, then it basically assigns these labels to the columns of this data set. So I'm going to set this data frame equal to DF. And then if we call the head is just like, give me the first five things, give me the first five things. Now you'll see that we have labels for all of these. Okay. All right, great. So one thing that you might notice is that over here, the class labels, we have G and H. So if I actually go down here, and I do data frame class unique, you'll see that I have either G's or H's, and these stand for gammas or hadrons. And our computer is not so good at understanding letters, right? Our computer is really good at understanding numbers. So what we're going to do is we're going to convert this to zero for G and one for H. So here, I'm going to set this equal to this, whether or not that equals G. And then "
Supervised Learning: Classification,https://www.youtube.com/watch?v=UadzJLHJB50,"Okay, let's now talk about the three broad categories of machine learning. So the three broad categories of machine learning are supervised learning, unsupervised learning, and reinforcement learning. So what is supervised learning? supervised learning is, I would say the most common form of machine learning or deep learning. So supervised learning is the biggest subcategory. And it involves labeled data. So if you recall the spam classification example, that would be one example of a supervised problem, where we want to predict whether an email is spam or not spam. And the labeled data are then examples of spam and non spam email. And it also involves direct feedback. So we can, yeah, we can say whether our prediction was correct or not, basically. So you, you have an example of an email that spam or not spam, the model does a prediction. And if you, let's say during evaluation, know the correct label, you have direct feedback here, during the training. We will talk about this a little bit more. So let me just briefly go over the three categories. And then I will talk more about supervised learning, and the others two. So unsupervised learning is, in contrast to supervised learning, not involving any labeled data. So here, we don't have labels or targets here, we also then don't have feedback. So we assign, for example, a group to give an example. For example, in clustering, we have cluster memberships, but we don't have any feedback here. So it's more really about finding a hidden structure in the data. reinforcement learning is the third category of machine learning. And here, this is more about learning a decision process. So think about the example of the self driving car. So the process of driving the car, it's like making multiple decisions. It's like the goal here might be to arrive at a destination, but to get there, you have to make a lot of decisions like stopping at a red street light on turning around the corner, and so forth. So here, what we do is we employ in a reward system that rewards the agent for making correct choices. And it's not like predicting a single outcome or something. It's more about learning a series of actions like steering the car through a difficult environment. And there are some examples where you can actually use supervised learning for self driving cars to, for example, recognizing the streetlight itself, right, could be involving convolution network, for example. But yeah, the process here is more like learning a series of actions. We won't be talking about reinforcement learning in this class, though, it's kind of out of the scope of this class. Nowadays, you can also use deep learning for reinforcement learning that is usually known as deep Q learning. But yeah, this is really not in the scope of this class. So here, we will be focusing more on supervised and unsupervised learning. And as you have seen from the introductory slides, where I showed you the topics, we already have plenty of things to talk about in this class. So yeah, a little bit more about supervised learning. So supervised learning, like I said, is the largest subcategory of machine learning, which is focused on label to data. So you already from other statistics classes have seen an example of supervised learning, that would be, for example, regression. So here is just a sketch of a linear regression model. And here, how this works is it's just a example with one observational input, or we call that in machine learning feature. So our x variable, the explanatory variable, we call that the feature. And the target here, the dependent variable that would be our y axis, or the data on the y axis here. And here, we are fitting a linear model. So we, for example, can then predict, if we have a new observation, what its target value might be given this model. So these x's here, the training data, think of it as the training data. And now think of it as, let's say I have a new data point that is, let's say here, sorry, if I go up here, so this is my input, I only know the x value. So I know this x value to predict the y value, I would go up here, and then look up the y value here. So using this linear model, I can make predictions for new data points for this one here, which is not in my data set, it would be one example of a supervised learning problem. Another example of supervised learning is classification. And actually, classification is much more common than regression. If you look at literature and deep learning. So most systems are focused on classification, for example, recognizing objects and images like is a cat or a dog or something like that. So we will see a lot of examples of classification also in this class. Or if you think back of the ATM machine that I showed you earlier, where it was recognizing these handwritten digits, it was recognizing or classifying these digit digits. So whether it's a two or three or four, and so forth. Here's an example of a binary classification problem, where we only have two possible labels, one possible label is the minus sign and one is the plus sign. So the labels here, either the minus sign or the plus sign here, my two possible labels is just for the sake of simplicity. And here I have two features, x one and x two. And the goal is to predict for a new data point, whether it's a plus or minus. So if I have, for example, new data point that is located here, because it's on the left side of the decision boundary. So this is a linear decision boundary learned by this machine learning system, it would predict a minus sign and everything on the right hand side would be predicted as "
Unsupervised Learning: Clustering,https://www.youtube.com/watch?v=wk2ylI1qgU0,"We will now take a closer look at one of the techniques used in unsupervised learning, which is referred to as clustering. So, what exactly is clustering? This is a technique where your data is divided into a number of logical groups. This grouping is done based on the characteristics of the data, and all of the data within a particular cluster will have certain similarities. To understand this, consider that you have a huge dataset where you have millions of data points, and there are no labels for you to work with. Before you use the data for anything, you want to see if there are any patterns which can help you identify groups in your data. For that, you can make use of a clustering algorithm, and these will examine all of the attributes in your dataset. And will be able to determine that there are several data points which share similar attributes or characteristics. Based on the similarities, a number of different groups may be formed in your dataset. And each of these groups will form a cluster of data points. So the task of a clustering algorithm is to examine the data which is presented to it, and then break it up into a number of logical clusters. All the data points within a cluster will have far more in common than the points outside the cluster. So why exactly would you want to employ the technique of clustering? Well, consider that entities in the real world are in fact very complex and may not be very easy to categorize. For example, you may have a lot of data about the products which are sold on an e-commerce site, but how many different categories should you divide them into? And what exactly should those categories be? You're faced with similar questions when categorizing the users who are on a social media platform, or even in the case of readers of an online newspaper. Do you classify people by their age, their gender, their geographical location. And if you do opt for geographical location, then what exactly is the definition of a single location? Do you divide people by state, by city or by neighborhood? So there is clearly no way to objectively divide your data into groups. This is where clustering comes into the picture. But before you can apply it you will need to ensure that the defining characteristics of each of your data points are represented using numbers. For example, in the case of products, one of the attributes of the data can be a rating which is given to the product by the users. It can also be an overall review sentiment. So for all of the reviews made for the product, you may come up with an aggregate sentiment score which could range somewhere between 0 and 1. The attributes in fact need not be entirely numeric, but could be categorical with numeric labels representing each of the categories. For example, the number 1 could represent electronic products, and number 2 for some fashion accessory. Some other attributes for a product could include the dimensions such as the size, the weight and so on. You may even have a color for the product, and this again, because it's a categorical value, will need to be represented by some numeric label. So for fields such as the product category and the color, these are some of the predefined categories you come up with. But your clustering algorithm may help you identify certain other categories. For example, highly rated electronic items which are available in blue. Moving along now to the defining characteristics of users, whether on a social media platform or the readers of some online newspaper. You may have some kind of rating for the post made by the user as well as the comments, likes and shares. You may even have some kind of categorical label for each of their posts and this could be by the topic of the post itself. You may rate each user or reader by their activity on the platform. So 100 can represent a highly active user and 0 if they're not active at all. In the case of social media, you may rate each user by the number of connections which they have. And you may also give a score for each user depending on how complete their profile is. So sticking with the example of users on a social media platform, based on the factors such as connections, activity, and profile completeness. You can represent each user by a point on a three-dimensional plot, such as this one. Rather than just considering three attributes, you can extend this to N attributes where you'll have an N-dimensional hyperplane on which you will represent your users as points. Once each user is represented as a point on this hyperplane, then there will be number of points which happen to be close to each other in terms of overall distance. So there may be one group of users who are very highly active, but also have very few connections. As we can imagine, there is likely to be a cluster where the profile completeness is close to zero, as is the activity and probably the connections as well. It is these kind of clusters which a clustering algorithm will help you find. Moving to another example, consider that you have plotted on an N-dimensional plane all the readers of a newspaper such as the The New York Times. Well, if you implement some kind of clustering algorithm on these points. You may discover that one cluster represent users who spend a lot of time in the technology section of the newspaper. So that is one common attribute for all the members in one cluster. On the other hand there will be different clusters and this may represent readers in different sections of the paper, such as current affairs or sports. So when you examine the users within a particular cluster, "
Unsupervised Learning: Clustering,https://www.youtube.com/watch?v=JnnaDNNb380,"Thanks to Wix for supporting PBS Digital Studios. Hey, I’m Jabril and welcome to Crash Course AI! So far in this series, we’ve focused on artificial intelligence that uses Supervised Learning. These programs need a teacher to use labeled data to tell them “right” from “wrong.” And we humans have places where supervised learning happens, like classrooms with teachers, but that’s not the only way we learn. We can also learn lots of things on our own by finding patterns in the world. We can look at dogs and elephants and know they’re different animals without anyone telling us. Or we can even figure out the rules of a sport just by watching people play. This kind of learning without a teacher is called Unsupervised Learning and, in some cases, computers can do it too. INTRO The key difference between supervised and unsupervised learning is what we’re trying to predict. In supervised learning, we’re trying to build a model to predict an answer or label provided by a teacher. In unsupervised learning, instead of a teacher, the world around us is basically providing training labels. For example, if I freeze this video of a tennis ball RIGHT NOW, can you draw what could be the next frame? Unsupervised learning is about modeling the world by guessing like this, and it’s useful because we don’t need labels provided by a teacher. Babies do a lot of unsupervised learning by watching and imitating people, and we’d like computers to be able to learn like this as well. This lets us utilize lots of freely available data in the world or on the internet. In many cases, one of the easiest ways to understand how AI can use unsupervised learning is by doing it ourselves, so let’s look at a few photos of flowers with no labels. The most basic way to model the world is to assume that it’s made up of distinct groups of objects that share properties. So, for example, how many types of flowers are here? We could say there are two because there are two colors, purple and yellow. Or we could look at the petal shapes, and divide them into round petals and tall vertical ones. Or maybe we have some more experience with flowers and realize that two of these are tulips, one is a sunflower, and one is a daisy, so there are three categories. Immediately recognizing different properties like this and creating categories is called unsupervised clustering. We don’t have labels provided by a teacher, but we do have a key assumption about the world that we’re modeling: certain objects are more similar to each other than others. We can program computers to perform clustering too. But to do that, we need to choose a few properties of flowers we’re interested in looking at, like how we picked color or shape just now. For a more realistic example, let’s say I bought a packet of iris seeds to plant in my garden. After the flowers bloom though, it looks like there were several species of irises mixed up in that one packet. Now I’m no expert gardener, but I can use some AI to help me analyze my garden. To construct a model, we have to answer two key questions. First, what observations can we measure? All of these flowers are purple, so that’s probably not the best way to tell them apart. But different irises seem to have different petal lengths and widths, which we can measure and place on this graph with petal length on the Y axis and width on the X axis. And second, how do we want to represent the world? We’re going to stick to a very simple assumption here: there are clusters in our data. Specifically, we’re going to say there are some number of groups called K clusters, but we don’t know where they are. To help us, we’re going to use the K-means clustering algorithm. K-means clustering is a simple algorithm. All it needs is a way to compare observations, a way to guess how many clusters exist in the data, and a way to calculate averages for each cluster it predicts. In particular, we want to calculate the mean by adding up all data points in a cluster and dividing by the total number of points. Remember, unsupervised learning is about modeling the world, so our algorithm will have two steps: First, our AI will predict. What does the model expect the world to look like? In other words, which flowers should be clustered together because they’re the same species? Second, our AI will correct or learn. The model will update its beliefs to agree with its observation of the world. To start the process, we have to specify how many clusters the model should look for. I’m guessing there are three clusters in the data, so that becomes the model’s initial understanding of the world, and we’re looking for K=3 averages, or three types of irises. But to start, our model doesn’t really know anything, so the averages are random and so are its predictions. Each datapoint (which is a flower) is given a label as type1, type2, or type3, based on the algorithm’s beliefs. Next, our model tries to correct itself. The average of each cluster of datapoints should be in the middle, so the model corrects itself by calculating new averages. We can see those averages here, marked with Xs, which gives our updated model of the three (or so we guessed) types of irises. The graph is still pretty noisy. For example, it’s a little weird that there are type2 flowers so close to the average for type3. But we did start with a random model, so we can’t expect too much accuracy. Logically, we know that irises of the same species tend to have similar petals, so those datapoints should be clustered together. Since we just did a correction or learning step, we can repeat the process, starting with a new prediction step. "
Unsupervised Learning: Clustering,https://www.youtube.com/watch?v=W01tIRP_Rqs,"Supervised and unsupervised learning are two core components in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them and then answer the question of which is best for you. Now, in supervised learning, the machine learning algorithm is trained on a labeled dataset. So this means that each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seen before. Now, using labeled inputs and outputs, the model can measure its accuracy and learn over time. Supervised learning can be actually divided into a couple of subcategories. Firstly, there is a category of classification. And classification talks about whether the output is a discrete class label such as ""spam"" and ""not spam"". Linear classifiers, support vector machines, or SPMs, decision trees, random forests - they're all common examples of classification algorithms. The other example is regression. The output here is a continuous value, such as price or probability. Linear regression and logistic regression are two common types of regression algorithms. Now, unsupervised learning is where the machine learning algorithm is not really given any labels at all. And these algorithms discover hidden patterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionality reduction. So let's take a look at each one of those, starting with clustering. Now clustering is where the algorithm groups similar experiences together. So a common application of clustering is customer segmentation, where businesses might group customers together based on similarities like, I don't know, age or location or spending habits, something like that. Then you have association. And association is where the algorithm looks for relationships between variables in the data. Now association rules are often used in market basket analysis, where businesses want to know which items are often bought together. You know, something along the lines of, ""customers who bought this item also bought "", that sort of thing. The final one to talk about is dimensional ... dimensional reduction. And this is where the algorithm reduces the number of variables in the data, while still preserving as much of the information as possible. Now, often this technique is used in the pre-processing data stage, such as when autoencoders remove noise from visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervised learning, the algorithm learns from training datasets by iteratively making predictions on the data and then adjusting for the correct answer. While supervised learning models tend to be more accurate than unsupervised learning models, they do require all of this up-front human intervention to label the data appropriately. For example, a supervised learning model can predict how long your commute will be on the time of day and thinking about the weather conditions and so forth. But first you'll have to train it to know things like rainy weather extends the driving time. By contrast, unsupervised learning models work on their own to discover the inherent structure of unlabeled data. These models don't need humans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were ahead of time. Now, an important distinction to make is that unsupervised learning models don't make predictions. They only group data together. So if you were to use an unsupervised learning model on that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long each commute would take. Okay, so which of these two options is right for you? In general, supervised learning is more commonly used than unsupervised learning, and that's really because it's more accurate and efficient. But that being said, unsupervised learning has its own advantages. There's two that I can think of. Firstly, unsupervised learning can be used on data that is not labeled, which is often the case in real world datasets. And then secondly, unsupervised learning can be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highly accurate and trustworthy. And in contrast, unsupervised learning can handle large volumes of data in real time. But there's a lack of transparency into how that data is clustered and a high risk given accurate results. But wait, it is not an ""either/or"" choice. May I present to you the middle ground known as semi-supervised learning. This is, well, a happy medium where you use a training data set with both labeled and unlabeled data. And it's particularly useful when it's difficult to extract relevant features from data when you have a high volume of data. So, for example, you could use a semi-supervised learning algorithm on a data set with millions of images where only a few thousand of those images are actually labeled. Semi-supervised learning is ideal for medical images, where a small amount of training data could lead to a significant improvement in accuracy. For example, a radiologist can look at and label some small subset of CT scans for tumors or diseases, and then the machine can more accurately predict which patients might require more medical attention without going through and labeling the entire set. Machine learning models are a powerful way to gain the data insights that improve our world. The right model for your data depends on the type of data that you have and what you want to do with it. "
Unsupervised Learning: Clustering,https://www.youtube.com/watch?v=esmzYhuFnds,"The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high-quality educational resources for free. To make a donation, or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. JOHN GUTTAG: I'm a little reluctant to say good afternoon, given the weather, but I'll say it anyway. I guess now we all do know that we live in Boston. And I should say, I hope none of you were affected too much by the fire yesterday in Cambridge, but that seems to have been a pretty disastrous event for some. Anyway, here's the reading. This is a chapter in the book on clustering, a topic that Professor Grimson introduced last week. And I'm going to try and finish up with respect to this course today, though not with respect to everything there is to know about clustering. Quickly just reviewing where we were. We're in the unit of a course on machine learning, and we always follow the same paradigm. We observe some set of examples, which we call the training data. We try and infer something about the process that created those examples. And then we use inference techniques, different kinds of techniques, to make predictions about previously unseen data. We call that the test data. As Professor Grimson said, you can think of two broad classes. Supervised, where we have a set of examples and some label associated with the example-- Democrat, Republican, smart, dumb, whatever you want to associate with them-- and then we try and infer the labels. Or unsupervised, where we're given a set of feature vectors without labels, and then we attempt to group them into natural clusters. That's going to be today's topic, clustering. So clustering is an optimization problem. As we'll see later, supervised machine learning is also an optimization problem. Clustering's a rather simple one. We're going to start first with the notion of variability. So this little c is a single cluster, and we're going to talk about the variability in that cluster of the sum of the distance between the mean of the cluster and each example in the cluster. And then we square it. OK? Pretty straightforward. For the moment, we can just assume that we're using Euclidean distance as our distance metric. Minkowski with p equals two. So variability should look pretty similar to something we've seen before, right? It's not quite variance, right, but it's very close. In a minute, we'll look at why it's different. And then we can look at the dissimilarity of a set of clusters, a group of clusters, which I'm writing as capital C, and that's just the sum of all the variabilities. Now, if I had divided variability by the size of the cluster, what would I have? Something we've seen before. What would that be? Somebody? Isn't that just the variance? So the question is, why am I not doing that? If up til now, we always wanted to talk about variance, why suddenly am I not doing it? Why do I define this notion of variability instead of good old variance? Any thoughts? What am I accomplishing by not dividing by the size of the cluster? Or what would happen if I did divide by the size of the cluster? Yes. AUDIENCE: You normalize it? JOHN GUTTAG: Absolutely. I'd normalize it. That's exactly what it would be doing. And what might be good or bad about normalizing it? What does it essentially mean to normalize? It means that the penalty for a big cluster with a lot of variance in it is no higher than the penalty of a tiny little cluster with a lot of variance in it. By not normalizing, what I'm saying is I want to penalize big, highly-diverse clusters more than small, highly-diverse clusters. OK? And if you think about it, that probably makes sense. Big and bad is worse than small and bad. All right, so now we define the objective function. And can we say that the optimization problem we want to solve by clustering is simply finding a capital C that minimizes dissimilarity? Is that a reasonable definition? Well, hint-- no. What foolish thing could we do that would optimize that objective function? Yeah. AUDIENCE: You could have the same number of clusters as points? JOHN GUTTAG: Yeah. I can have the same number of clusters as points, assign each point to its own cluster, whoops. Ooh, almost a relay. The dissimilarity of each cluster would be 0. The variability would be 0, so the dissimilarity would be 0, and I just solved the problem. Well, that's clearly not a very useful thing to do. So, well, what do you think we do to get around that? Yeah. AUDIENCE: We apply a constraint? JOHN GUTTAG: We apply a constraint. Exactly. And so we have to pick some constraint. What would be a suitable constraint, for example? Well, maybe we'd say, OK, the clusters have to have some minimum distance between them. Or-- and this is the constraint we'll be using today-- we could constrain the number of clusters. Say, all right, I only want to have at most five clusters. Do the best you can to minimize dissimilarity, but you're not allowed to use more than five clusters. That's the most common constraint that gets placed in the problem. All right, we're going to look at two algorithms. Maybe I should say two methods, because there are multiple implementations of these methods. The first is called hierarchical clustering, and the second is called k-means. There should be an S on the word mean there. Sorry about that. All right, let's look at hierarchical clustering first. It's a strange algorithm. We start by assigning each item, each example, to its own cluster. So this is the trivial solution we talked about before. So if you have N items, you now have N clusters, each containing just one item. "
Unsupervised Learning: Clustering,https://www.youtube.com/watch?v=WV7Slf_lDgE,"While in supervised learning the goal is to learn a mapping between a set of inputs and outputs provided by a supervisor, in unsupervised learning we have only input values and the goal is to unveil the underlying hidden structure in the data. Unsupervised learning is a set of techniques that allows learning better representations for data, which is critical to improve the performance of downstream tasks. Most of the learning that occurs in our brain can be considered unsupervised. In the first year of their life, children are provided with very little “labeled data” with respect to the amount of learning they perform. A parent or teacher doesn’t need to show children every breed of dog to teach them to recognize dogs. They can learn from a few examples, without a lot of explanation, and generalize on their own. Of course, they can make mistakes in doing this, but developing good representations of what they observe allows them to correct themselves quickly. Learning for the sake of learning, without a particular task in mind, allows the agent to uncover patterns that can be surprisingly useful for developing autonomous intelligence. Given an unlabeled dataset, an unsupervised learning algorithm might try to group data into clusters, or try to reduce the dimensionality of the data by compressing the information with a new set of features, or try to discover rules or patterns inside the data. One of the main tasks in unsupervised learning is clustering, that is the task of grouping examples so that the examples in the same cluster are more similar to each other than to those in other clusters. The similarity is measured according to some metrics Since the notion of cluster cannot be precisely defined, there are several cluster models: connectivity models, centroid models, distribution models, density models, and many others. For each of them, many algorithms have been proposed. In order to fix ideas, let’s consider one of the most popular clustering algorithms: K-means. K-means is an iterative algorithm that uses a centroid model, meaning that each cluster is represented by its center which corresponds to the mean of the points assigned to the cluster. Initially, we must select the desired number K of clusters and we initialize the centroid of each cluster by picking a point at random in our dataset. Then, we take each example in our dataset and determine which cluster it belongs to by computing the distances to all the centroids and taking the closest. The next step is to adjust the centroids to the mean of the examples assigned to each cluster. These two steps are repeated until the assignments no longer change. Clustering is used in many applications. News aggregators everyday look at tens of thousands of new stories on the web and automatically group them according to their topic, without human intervention. Let’s consider, for example, genomics and in particular DNA microarray data analysis, where we can group individuals into different categories according to how much certain genes are expressed. This is unsupervised learning because we are not telling the algorithm which persons are of type 1 or type 2, but the groups are inferred by observing similarities in the data. These kinds of algorithms are also employed in social network analysis, market segmentation, astronomical data analysis, and many others. Moreover, the output of a clustering algorithm can be used for classification, for anomaly detection, for customer segmentation, as well as even improving supervised learning models. A common use case to start is the classification for data that is not labeled. Even if your data does not have a column that specifies the classes, clustering algorithms will try to find heterogeneous groupings within your dataset. Examples of this include finding groupings other than normal e-mail messages to identify spam. Another common use case for clustering is anomaly detection. Imagine that we are working with credit card transactions and we have a certain user, and we see that there is a small cluster compared to the rest of those users’ transactions that has high volume of attempts or perhaps small amounts or at new merchants. This would create its own new cluster and that would present an anomaly within the dataset and, perhaps, that would indicate to the credit card company that there is fraudulent transactions happening. Customer segmentation is also a common use case. For example, think about searching groupings that help you find out how many types of customers your business has based on the recency, the frequency, and the average amount of visits over the past three months. It requires a combination of each one of those different features and comes up with different segments. Or another common segmentation is by demographics at that engagement level. For example, you can create groups for single customers, new parents, empty nesters, etc, and determine the preferred marketing channel, and use these insights to drive your future marketing campaigns. Finally, another common use case is to help improve supervised learning. For example, we can check a good model, for instance, a good linear regression model, that we trained on our entire dataset, and see how well that performs compared to models trained for subsegments of our data that we found through clustering. Perhaps we'll be able to improve our performance if we look at each one of these different classes and come up with different predictions for each one of these different groupings. There is no guarantee that this will always work, but it is a common practice to segment the data, to find these heterogeneous groups, and then train a model for each group to help improve that classification or regression. In the next lesson, we will overview how unsupervised learning can be used to find a compressed representation of the data by means of dimensionality reduction techniques. "
Unsupervised Learning: Dimensionality Reduction,https://www.youtube.com/watch?v=3uxOyk-SczU,"Let's talk about Dimensionality Reduction. So, here, I've got a bunch of points, okay? And these points are in 2D, all right? It's called kind of green and red, but that's sort of a joke. We're not going to worry too much about why it was done that. What you should notice is that in the middle here are these orangeish yellowish points. Do you see why it's R and G? Red plus green makes sort of yellowish, or so, so along that diagonal it'll be kind of yellow. I don't know who did this originally, but it's kind of cool. All right, if you think about these orange points, let's think about how they're distributed, okay? So, they have a have a mean sit somewhere, and they've got a. Set of eigenvectors, right? They've got a co-variants matrix that describes them. They have a axis of least inertia and then the next axis. So here you're seeing the points, the orange points are x, x bar here is supposed to be their, their mean. And then we've got the big eigenvector v1 and then the smaller one v2. And the idea is. We can represent those orange points by only their v1 coordinates, plus the mean. And what that would mean is haha. Essentially that we're going to think of all the orange points as just being on that line. And all I'm going to tell you is where on the line they are. And we're going to essentially ignore the amount that they're off that line. So we've just reduced the dimensions from how many? Two. To how many? One. Okay, that doesn't sound like that big a deal, nor is it. But in higher dimensions, this could be a huge deal. Imagine you've got something in 10,000-dimensional space, and yes, in just a minute, we're going to do a 10,000-dimensional space. If you said, well, I'm going to represent them by one number, or even 30, that would be a huge reduction. So if I say, well, what direction does it vary the most in? And I just give you that value. If that's good enough for what we want to do, you've reduced the description from being a lot of numbers to being a much smaller number. In fact, we can sort of express that here algebraically in terms of just, thinking about it, whatever dimension x happens to be in. So if I've got a whole bunch of data points in some N-dimensional space, what I want to know is the direction of projection. And we'll just say it's v. All right, that if I projected those points, after subtracting out the mean, that I'd have the greatest amount. Right, the greatest variation. And that's what that says here, right? So take x, subtract out the mean, dotted with the v, summed over all the x's, take the norm. Take the square. Well that can be written as an expression like this of just v transpose Av, where A is just this outer product. Okay, that's the co-variants matrix that we're, we're familiar with before. And as we said before. The eigenvector with the largest eigenvalue lambda is going to be the one that captures that greatest variation. In a minute I'll give you a little argument about why it's the largest eigenvector with the largest eigenvalue. Or you can just take my word for it. And, in fact, the smallest eigenvalue would be the least amount of dimension, so basically, what we're going to have to do at some point is take the eigenvectors of this co-variance matrix. "
Unsupervised Learning: Dimensionality Reduction,https://www.youtube.com/watch?v=iSdXdJBLNyc,"Learning in problems with high-dimensional input spaces requires to consider complex models which need many samples to be trained. This is called the curse of dimensionality. Dimensionality reduction aims at representing our dataset in lower dimensions, while maintaining most of the information that is contained in our dataset. Working on a compressed representation requires less samples, but compressing too much can be harmful as we risk losing relevant information. We need to manage this tradeoff in relation to the number of examples available in our dataset. Let’s consider for instance a dataset of face images: each example is a face, represented by an image containing millions of pixels. Working in such a high dimensional space consumes a large amount of memory and computational power and, most of all, it is useless. What is the probability of generating the image of a face by picking an image at random? Almost zero! This is a symptom that the representation that we are using is not effective for representing the information contained in our dataset. We can leverage the structure in our data to build a new and more compact representation on which it will be easier to solve new learning tasks. One of the most popular dimensionality reduction techniques is Principal Component Analysis or PCA. Starting from the original input features, PCA creates new features which are linear combinations of the original ones. Let’s look at this image that shows points in a two-dimensional space. As we can see, these two features look very correlated one with the other as the points lie very close to a line. If we wanted to reduce the input features from two to one, which choice would we make? If we consider that line and we project all the points onto that line, this will entail a linear transformation of our data. This new feature allows us to replace the two original features by preserving the original information as much as possible. As we will see, the goal of PCA is to produce a new representation that retains the original variance as much as possible. So, how does PCA find out these lines onto which projecting the data? This is obtained by computing the covariance matrix of our dataset. Then, to know which is the most important direction, PCA considers the eigenvector associated with the largest eigenvalue. This is the first principal component, that is the direction along which there is the largest variance. The values in the eigenvector specify the coefficients of the linear combination of the original input features. The second principal component will be the eigenvector associated with the second largest eigenvalue, and so on for all the other components. Given the properties of eigenvectors, all the new features are orthogonal to each other. Now, how can we choose how many and which principal components to select? First, we must sort the eigenvectors in descending order of eigenvalue. Then, given a threshold on the percentage of the original variance that we want to retain with the new representation, starting from the first principal component, we keep adding new features until the ratio between the sum of the eigenvalues of the selected components and the sum of the eigenvalues of all the components crosses the chosen threshold. In the picture, we see an application of PCA to a dataset of face images. In the dataset there are 128 images of faces of size 256 x 128 pixels. In this picture we see a visual representation of the first fifteen principal components, in this case called eigenfaces, and at the top left corner the reconstruction of a face in this fifteen-dimensional space. In this way, instead of using more than thirty thousand values to represent the intensity of each pixel, we can represent a face with only fifteen values. Of course, as shown in this image, the accuracy of the reconstruction grows with the number of eigenfaces considered. Unfortunately, linear combinations of the original features are not always able to capture the inherent structure in our data. If we have a dataset with examples distributed as in the picture, we will not be able to identify a principal direction, even if the data clearly occupy a submanifold of the feature space. In this case, we need to resort to dimensionality reduction approaches able to create new features that are nonlinear combinations of the original ones, such as locally-linear embedding, self-organizing maps, kernel PCA, Isomap, and many others. Recently, also deep learning methods have been proposed to perform nonlinear dimensionality reduction. A popular approach is called autoencoders. The idea of autoencoders is to train a deep neural network to learn the identity function. We want the network to be able to reproduce the input correctly. Dimensionality reduction is achieved by using a specific neural network architecture, composed of an encoder, a decoder, and, in the middle, a bottleneck. The encoder compresses the data to their lower-dimensional representation. The bottleneck, also called latent space, stores the lower-dimensional representation. The decoder reconstructs our low dimensional data back to their initial dimension. If such a neural network can accurately reconstruct the input, it means that the features stored in the bottleneck contain the relevant information about our dataset. After training, the decoder is discarded and the output from the bottleneck is used directly as the reduced dimensionality of the input. Beyond dimensionality reduction, autoencoders are also used for image denoising. In the next lesson, we will see another set of unsupervised learning techniques used to discover relevant patterns in a dataset: association rules. "
Unsupervised Learning: Dimensionality Reduction,https://www.youtube.com/watch?v=HMOI_lkzW08,that quest is the best if you don't think so then we have different opinions hello I'm Josh stommer and welcome to stat quest today we're gonna be talking about the main ideas behind principle component analysis and we're going to cover those concepts in five minutes if you want more details than you get here be sure to check out my other PCA video let's say we had some normal cells if you're not a biologist imagine that these could be people or cars or cities or etc they could be anything even though they look the same we suspect that there are differences these might be one type of cell or one type of person or car or city etc these might be another type of cell and lastly these might be a third type of cell unfortunately we can't observe differences from the outside so we sequence the messenger RNA in each cell to identify which genes are active this tells us what the cell is doing if they were people we could measure their weight blood pressure reading level etc okay here's the data each column shows how much each gene is transcribed in each cell for now let's imagine there are only two cells if we just have two cells then we can plot the measurements for each gene this gene gene one is highly transcribed in cell one and lowly transcribed in cell two and this gene gene 9 is lowly transcribed in cell 1 and highly transcribed in cell 2 in general cell one and cell to have an inverse correlation this means that they are probably two different types of cells since they are using different genes now let's imagine there are three cells we've already seen how we can plot the first two cells to see how closely they are related now we can also compare cell one to sell three cell one and cell three are positively correlated suggesting they are doing similar things lastly we can also compare cell two to cell three the negative correlation suggests that cell two is doing something different from cell 3 alternatively we could try to plot all three cells at once on a three dimensional graph cell one could be the vertical axis cell two could be the horizontal axis and sell three could be depth we could then rotate this graph around to see how the cells are related to each other but what do we do when we have four or more cells draw tons and tons of to sell plots and try to make sense of them all or draw some crazy graph that has an axis for each cell and makes our brain explode no both of those options are just plain silly instead we draw a principal component analysis or PCA plot a PCA plot converts the correlations or lack thereof among the cells into a 2d graph cells that are highly correlated cluster together this cluster of cells are highly correlated with each other so are these and so are these to make the clusters easier to see we can color-code them once we've identified the clusters in the PCA plot we can go back to the original cells and see that they represent three different types of cells doing three different types of things with their genes BAM here's one last main idea about how to interpret PCA plots the axes are ranked in order of importance differences among the first principal component access PC one are more important than differences along the second principal component access PC two if the plot looked like this where the distance between these two clusters is about the same as the distance between these two clusters then these two clusters are more different from each other than these two clusters before we go you should know that PCA is just one way to make sense of this type of data there are lots of other methods that are variations on this theme of dimension reduction these methods include heat maps tea Snee plots and multiple dimension scaling plots the good news is that I've got stat quests for all of these so you can check those out if you want to learn more note if the concept of dimension reduction is freaking you out check out the original stat quest on PCA I take it nice and slow so it's clearly explained hooray we've made it to the end of another exciting stat quest if you like the stat quest and want to see more of them please subscribe and if you have any ideas for additional stat quests well put them in the comments below until next time quest on 
Unsupervised Learning: Dimensionality Reduction,https://www.youtube.com/watch?v=AU_hBML2H1c,"Hello! Welcome to Dimensionality Reduction using feature extraction and feature selection Dimensionality Reduction is the process of reducing the number of variables/features in review. Dimensionality Reduction can be divided into two subcategories called Feature Selection which includes Wrappers, Filters, and Embedded. And Feature Extraction which includes Principle Components Analysis. So how exactly does Dimensionality Reduction Improve Performance? It does so by reducing the number of features that are to be considered. To see how this works, think of a simple algebraic equation. a + b + c + d = e. If you can equate ab = a + b, making a representation of two variables into one, you're using Feature Extraction to reduce the number of variables. Now, consider if c was equal to 0 or an arbitrarily small number, it wouldn't really be relevant, therefore it could be taken out of the equation. By doing so, you'd be using Feature Selection because you'd be selecting only the relevant variables and leaving out the irrelevant ones. Feature Selection is the process of selecting a subset of relevant features or variables. There are 3 main subset types: * Wrappers, * Filters, and * Embedded. To help you visualize how feature selection works, imagine a set of variables let's use a series of shapes, for example -- with each shape representing different dimensions or features. By ignoring the irrelevant variables, or selecting the ones that improve accuracy, we reduce the amount of strain on the system and produce better results. Wrappers use a predictive model that scores feature subsets based on the error-rate of the model. While they're computationally intensive, they usually produce the best selection of features. A popular technique is called stepwise regression. It's an algorithm that adds the best feature, or deletes the worst feature at each iteration. Filters use a proxy measure which is less computationally intensive but slightly less accurate. So it might have a good prediction, but it still may not be the best Filters do capture the practicality of the dataset but, in comparison to error measurement, the feature set that's selected will be more general than if a Wrapper was used. An interesting fact about filters is that they produce a feature set that don't contain assumptions based on the predictive model, making it a useful tool for exposing relationships between features, such as which variables are 'Bad' together and, as a result, drop the accuracy or 'Good' together and therefore raise the accuracy. Embedded algorithms learn about which features best contribute to an accurate model during the model building process. The most common type of is called a regularization model. In our shape example, it would be similar to picking the shapes or good features in each step of the model building process. It might be Picking the Triangle feature in Step One, Picking the Cross feature in Step Two. Or Picking the Lightning feature in Step Three to obtain our accurate model. Feature Extraction is the process of transforming or projecting a space composing of many dimensions into a space of fewer dimensions. This is similar to representing data in multiple dimensions to ones that are less. This is useful for when you need to keep your information but want to reduce the resources that it may consume during processing. The main linear technique is called Principle Components Analysis. There are other linear and non-linear techniques but reviewing them here is out of scope for this course. Principle Components Analysis is the reduction of higher vector spaces to lower orders through projection. It can be used to visualize the dataset through compact representation and compression of dimensions. An easy representation of this would be the projection from a 3-dimensional plane to a 2-dimensional one. A plane is first found which captures most (if not all) of the information. Then the data is projected onto new axes and a reduction in dimensions occur. When the projection of components happens, new axes are created to describe the relationship. This is called the principle axes, and the new data is called principle components. This becomes a more compact visualization for the data and thus, is easier to work with. Thanks for watching! "
Unsupervised Learning: Dimensionality Reduction,https://www.youtube.com/watch?v=uZTHYRM2SDc,"Welcome to this deeper dive into unsupervised learning. Unsupervised learning is a machine learning task in which you take unlabeled data and try to learn the underlying structure of the data. If you're not already familiar with the core concepts of machine learning, I strongly recommend that you go watch my short introduction to that first. In this presentation, I'll start by covering clustering, then go on to talk about dimensionality reduction and autoencoders, and then finally talk a bit about how we can evaluate the results and the big picture of how the many different methods relate to each other. But let's first have a look at the input data. When you're doing unsupervised learning, you start from a set of endpoints that live in some high-dimensional space. You can then take this and for example calculate pairwise distances, you could use Euclidean distance for that, and that way get an in-by-in distance matrix. Alternatively, you could calculate all pairwise similarities using for example cosine similarity and get an N-by-N similarity matrix instead. And if you apply a cutoff to this one, you would obtain a network within nodes. These are all different views of the same input data and different algorithms start from different views. The first type of unsupervised learning is clustering. The goal of clustering is to discover groups in the data, and there are two main approaches to that. One is hierarchical clustering, in which we're trying to build a tree or dendrogram like this that shows which input points are most closely related and then gradually build up bigger and bigger groups. The other is partitional clustering, in which we're trying to explicitly take the points and divide them into a number of clusters. There are several algorithms for this. The best known is probably the K-means algorithm, in which we work in the original input coordinates and try to define centroids that correspond to the clusters. Another popular clustering algorithm is Markov clustering, also called MCL, which takes the network view in the data and tries to do community detection. Another class of unsupervised learning is dimensionality reduction. The goal here is very different, namely to take the high-dimensional input data and compress it to produce a lower-dimensional space that captures the information. This lower-dimensional space is also called a latent representation of the input data. There are several methods, some are linear, the best known being of course principal component analysis, and most of them are non-linear, including t-SNE and UMAP, both of which are commonly used for visualizing single-cell data, and also multi-dimensional scaling and force-directed layouts. All of these algorithms do similar things, but they have different objective functions that are being optimized and different transformations that are allowed. So for example, in PCA, we try to maximize the variance captured, in t-SNE, we are focusing mainly on preserving the local structure, and UMAP tries to preserve both the local and the global structure. Another approach to dimensionality reduction is autoencoders. Autoencoders are in a way supervised learning, but instead of trying to learn to predict the output from the input, it tries to predict the input from the input. You might of course object, ""That's trivial"" and that's true. But what we do is we introduce a bottleneck in the neural network architecture. So we work with an architecture that looks like this, where you have an input layer, then possibly a hidden layer with lower dimensionality, the small code layer in the middle, and then mirroring the architecture in the other half to again produce an output layer with the same size as the input layer. The trick here is that when we train the model, it has to learn in the code layer a low-dimensional representation of the data that allows the five-dimensional version to be reproduced faithfully. After having trained it, we can throw away the lower half of the network and now have our autoencoder that takes the data in the input layer and converts it into a low-dimensional representation in the code layer, in other words, the latent representation. There are many variants of autoencoders. Autoencoders can be linear, but most autoencoders are nonlinear. In addition to normal autoencoders, you have denoising autoencoders that take a noisy version of the input data and tries to reproduce the clean version of the input data and you have variational autoencoders that have become very popular recently that instead of learning a single vector representation in the middle, rather learns a patient representation of the data. So let's say we've done some unsupervised learning. How can we evaluate the quality of the results? The short answer is, it's difficult. And the reason is that we don't know the ground truth. That's why we're doing unsupervised learning. If you have some labels for some of your data points, it's very wise to check for consistency. In other words, whether the structure you found in the data is consistent with what you know about your data points. Alternatively, if you're doing clustering, you can use metrics like intra-cluster cohesion and inter-cluster separation, in other words, looking at whether the points within clusters aren't much closer to each other than the points in different clusters. If you're doing dimensionality reduction, for example, PCA, you will want to look at how much of the variance do you manage to capture in the first few dimensions. And if you're doing autoencoders, which are a form of self-supervised learning, you can of course steal the tricks from supervised learning, leave out some of the data, and thereby have an independent test set to see if your autoencoder indeed works also on new data. But in the end, it typically comes down to expert judgment. That is, having somebody who knows about the data, look at your unsupervised learning and see if it makes sense. Alternatively, something that I like to do is what I call downstream benchmarking. Typically, when you're doing unsupervised learning, "
Reinforcement Learning,https://www.youtube.com/watch?v=nIgIv4IfJ6s,"Hey, I’m Jabril and welcome to Crash Course AI. Say I want to get a cookie from a jar that’s on a tall shelf. There isn’t one “right way” to get the cookies. Maybe I find a ladder, use a lasso, or build a complicated system of pulleys. These could all be brilliant or terrible ideas, but if something works, I get the sweet taste of victory... and I learn that doing that same thing could get me another cookie in the future. We learn lots of things by trial-and-error, and this kind of “learning by doing” to achieve complicated goals is called Reinforcement Learning. INTRO So far, we’ve talked about two types of learning in Crash Course AI: Supervised Learning, where a teacher gives an AI answers to learn from, and Unsupervised Learning, where an AI tries to find patterns in the world. Reinforcement Learning is particularly useful for situations where we want to train AIs to have certain skills we don’t fully understand ourselves. For example, I’m pretty good at walking, but trying to explain the process of walking is kind of difficult. What angle should your femur be relative to your foot? And should you move it with an average angular velocity of… yeah, never mind… its really difficult. With reinforcement learning, we can train AIs to perform complicated tasks. But unlike other techniques, we only have to tell them at the very end of the task if they succeeded, and then ask them to tell us how they did it. (We’re going to focus on this general case, but sometimes this feedback could come earlier. So if we want an AI to learn to walk, we give them a reward if they’re both standing up and moving forward, and then figure out what steps they took to get to that point. The longer the AI stands up and moves forward, the longer it’s walking, and the more reward it gets. So you can kind of see how the key to reinforcement learning is just trial-and-error, again and again. For humans, a reward might be a cookie or the joy of winning a board game. But for an AI system, a reward is just a small positive signal that basically tells it “good job” and “do that again”! Google Deepmind got some pretty impressive results when they used reinforcement learning to teach virtual AI systems to walk, jump, and even duck under obstacles. It looks kinda silly, but works pretty well! Other researchers have even helped real life robots learn to walk. So seeing the end result is pretty fun and can help us understand the goals of reinforcement learning. But to really understand how reinforcement learning works, we have to learn new language to talk about these AI and what they’re doing. Similar to previous episodes, we have an AI (or Agent) as our loyal subject that’s going to learn. An agent makes predictions or performs Actions, like moving a tiny bit forward, or picking the next best move in a game. And it performs actions based on its current inputs, which we call the State. In supervised learning, after /each/ action, we would have a training label that tells our AI whether it did the right thing or not. We can’t do that here with reinforcement learning, because we don’t know what the “right thing” actually is until it’s completely done with the task. This difference actually highlights one of the hardest parts of reinforcement learning called credit assignment. It’s hard to know which actions helped us get to the reward (and should get credit) and which actions slowed down our AI when we don’t pause to think after every action. So the agent ends up interacting with its Environment for a while, whether that’s a game board, a virtual maze, or real life kitchen. And the agent takes many actions until it gets a Reward, which we give out when it wins a game or gets that cookie jar from that really tall shelf. Then, every time the agent wins (or succeeds at its task), we can look back on the actions it took and slowly figure out which game states were helpful and which weren’t. During this reflection, we’re assigning Value to those different game states and deciding on a Policy for which actions work best. We need Values and Policies to get anything done in reinforcement learning. Let’s say I see some food in the kitchen: a box, a small bag, and a plate with a donut. So my brain can assign each of these a value, a numerical yummy-ness value. The box probably has 6 donuts in it, the bag probably has 2, and the plate just has 1… so the values I assign are 6, 2, and 1. Now that I’ve assigned each of them a value, I can decide on a policy to plan what action to take! The simplest policy is to go to the highest value (that box of possibly 6 donuts). But I can’t see inside of it, and that could be a box of bagels, so it’s high reward but high risk. Another policy could be low reward but low risk, going with the plate with 1 guaranteed delicious donut. Personally, I’d pick a middle-ground policy, and go for the bag because I have a better chance of guessing that there are donuts inside than the box, and a value of 1 donut isn’t enough. That’s a lot of vocab, so let’s see these concepts in action to help us remember everything. Our example is going to focus on a mathematical framework that could be used with different underlying machine learning techniques. Let’s say John-Green-bot wants to go to the charging station to recharge his batteries. In this example, John-Green-bot is a brand new Agent, and the room is the Environment he needs to learn about. From where he is now in the room, he has four possible Actions: moving up, down, left, or right. "
Reinforcement Learning,https://www.youtube.com/watch?v=2xATEwcRpy8,"Hey Charles, this is when we get to talk about reinforcement learning. >> Hi Michael. This is when I get to hear about reinforcement learning. >> Wow. I'm glad we're on the same page. So- >> Are we on the same page? Is this all of reinforcement learning or is it just the reinforcement learning basics? >> We're going to start with the basics. >> Oh, okay. I can't wait to hear what that is. >> So the first concept to try to understand when you're doing reinforcement learning is that a lot of it takes place as a conversation between an agent and an environment. >> Okay, so like right now, you're the agent and I'm the environment. >> Actually I think I'm going to have you be the agent. >> Okay. >> And we'll just imagine some kind of, I don't know, like a video game environment. >> That sounds reasonable. By the way did you notice I've lost weight? >> [LAUGH] Oh, good job, how did you do that? >> Well, I got drawn as a stick figure. >> [LAUGH] That's fair. So here we are, the agent and the environment, and the conversation basically talks about what is going back and forth between the agent and the environment. So, the environment is going to reveal itself to you, to the agent, in the form of states, S. You then get to have some influence on the environment by taking actions, a, and then you also receive back, before the next state, some kind of reward for the most recent state action combination. >> Okay. Fair enough. >> So this is the same kind of elements that we have in an MDP. But the important thing is that instead of just being given an MDP as some kind of a graph structure and then we get to compute on it. Really the computation's happening inside the head of the agent and the information about the environment is really only available through the course of this interaction. So does that make some sense? >> It does make some sense but I guess how is that any different from the MDP? Well, it's the same story as how a policy interacts with an MDP. Right, where this is playing the role of the MDP, and this is playing the role of the policy, pi. >> Mm-hm. >> But now, again, the computational aspect of the system here, the agent doesn't know the environment, it's not living inside the agent's head. Instead the agent is just experiencing the environment by interacting with it. It can then you know if it so chose build some kind of a model of the environment in its head and then think about that. But what's in the agent's head and what's in the environment are two different things in this set up. >> Okay fair enough. I get that. >> So maybe I can make this a little bit clearer. So let's actually put you in this environment. What do you say? >> Okay, metaphorically? >> No, let's just do it. >> Sure. "
Reinforcement Learning,https://www.youtube.com/watch?v=i7q8bISGwMQ,"welcome back so i've started this video lecture series on reinforcement learning and the last three videos were at a very high level kind of what is reinforcement learning how does it work uh what are some of the applications but we really didn't dig into too many details on the actual algorithms of how you implement reinforcement learning in practice and so that's what i'm actually going to do today and in this next part of this series is something i hope is going to be really really useful kind of for all of you which is the The first thing is i'm going to kind of organize the different approaches of reinforcement learning This is a massive field that's about 100 years old This merges neuroscience, behavioral science like Pavlov's dog, optimization theory, optimal control think Bellman's equation and the Hamilton Jacobi Bellman equation all the way to modern day deep reinforcement learning which is kind of how to use powerful machine learning techniques to solve these optimization problems and you'll remember that in my view of reinforcement learning this is really at the intersection of machine learning and control theory so we're essentially machine learning good effective control strategies to interact with in a an environment So in this first lecture what i'm gonna do and i think i'm hoping that this is actually super useful for some of you is i'm going to talk through the organization of these different decisions you have to make and kind of how you can think about the landscape of reinforcement learning. Before going on I want to mention this is actually a chapter in the new second edition of our book data driven science and engineering with myself and Nathan Kutz and reinforcement learning was one of the new chapters I decided to write so this is a great excuse for me to get to learn more about reinforcement learning and it's also a nice opportunity for me to kind of get to communicate more details to you so if you want to download this chapter the link is here, and I'll also put it in the comments below and i'll have a link to the second edition of the book uh up soon as well probably in the comments good so um a new chapter you can follow along with all of the videos and each video kind of um you know follows follows the chapter good so before i get into that organizational chart of how you know all of these different types of reinforcement learning can be thought of i want to just do a really really quick recap of what is the reinforcement learning problem so in reinforcement learning you have an agent that gets to interact with the world or the environment through a set of actions sometimes these are discrete actions sometimes these are continuous actions if i have a robot i might have a continuous action space whereas if i'm playing a game if i'm the you know the white pieces on a chess board then i have a discrete set of actions even though it might be kind of high dimensional and i observe the state of the system at each time step i get to observe the state of the system and use that information uh to change my actions to try to maximize my current or future rewards uh through through playing and i'll mention that in lots of applications for example in chess the reward structure might be quite sparse i might not get any feedback on whether or not i'm making good moves until the very end when i either win or lose tic-tac-toe backgammon checkers go are all kind of the same way and that delayed reward structure is one of the things that makes this reinforcement learning problem really really challenging it's what makes uh you know learning in animal systems also challenging if you want to teach your dog a trick you know they have to know kind of step by step what you want them to do and so you actually sometimes have to give them rewards at intermediate steps to train a behavior and so the agent their control strategy or their their policy is typically called pi and it basically is a probability of taking action a given a state s a current state s so this could be a deterministic policy it could be a probabilistic policy but essentially it's a set of rules that determines what actions i as the agent take given uh what i sense uh in the environment to maximize my future rewards so that's the policy and again usually this is written in a probabilistic framework because typically the environment is written as a probabilistic model and there is something called a value function so given um you know some policy pi that i take then i can associate a value with being in each of the states of this system essentially by what is my expected future reward add up all of my future rewards what's the expectation of that and we'll put in this little discount factor because future rewards might be less uh advantageous to me than than current rewards this is just something that people do in economic theory it's kind of like a you know utility function and so for every policy pi there is a value associated with being in each of the given states s now again i'll point out for for even reasonably sophisticated problems you can do this for tic-tac-toe you can enumerate all possible states and all possible actions and you can compute this value function kind of through brute force but even for moderately complicated games even like checkers let alone back game in her chess or go this state space the the the space of all possible states you could observe your system in is astronomically large i think it's estimated that there's 10 to the 80 plus um "
Reinforcement Learning,https://www.youtube.com/watch?v=L_4BPjLBF4E,"This is Albert. He's an artificial intelligence we taught him to crawl to targets, but can he learn to walk? Albert can control each of his limbs, and he's rewarded for getting closer to the target, so he'll learn how to use his limbs to walk. It looks like you learned how to do the worm... that's not how you're supposed to walk... I see how it is, Albert. Let's see the worm work in these next rooms. From now on you'll be punished for hitting the ground, but rewarded when your feet hit the ground. No, Albert, the worm won't work here. You'll need to get a lot better with your legs to walk... You're actually balancing! Albert, that was your first step! It wasn't very graceful, but it's a start. Now we're getting somewhere, You're learning to skip! You're getting pretty good at it, Albert! Skipping is better than the worm, I guess. But you're supposed to walk, not skip. Skipping isn't going to work for long, Albert. Nevertheless, you're almost there! Oh... you don't know how to turn... You hit the button, but there's a lot more to learn. In this room, you'll be forced to learn to turn. On top of the other rewards, you'll be rewarded for keeping your chest up. To make sure you don't cheat, if your chest isn't high enough, You can't hit buttons. Yes, walls exist now, Albert. It looks like you're still skipping... But at least you've hit the first button! Again?! *sigh* That's better than jumping off the edge, I suppose. That's two buttons! You're almost there, Albert! You can't walk through walls, Albert. Go around it. Nice work, Albert! But this is more of a shuffle than a walk... Regardless, Well done! Now it's time to learn to take real steps. For this room, you need to learn to deal with cubes. On top of the other rewards and punishments, you'll also be rewarded for alternating feet. With this new reward, your shuffle isn't as good. Yes! You're finally starting to take proper steps! but you still suck... Come on, Albert, you got this! No, Albert. That's the wrong way... It looks like you've learned to manage the cubes! Yes! That was good, Albert, but you'll need to be a lot better to beat this final challenge. Excellent work, Albert! Now that you can walk, there's a whole new world of things to learn :) "
Reinforcement Learning,https://www.youtube.com/watch?v=0MNVhXEX9to,"Welcome back. So I'm really excited to do this lecture on reinforcement learning. I've been wanting to do this for a long time. Those of you who know me know that I love control theory and machine learning and reinforcement learning is kind of at this sweet spot between these two super important fields. Okay, so reinforcement learning is essentially a branch of machine learning that deals with how to learn control strategies to interact with a complex environment. And one of the ways I think about this, the way I'm going to define this, is that reinforcement learning is a framework for learning how to interact with the environment from experience. This is a very biologically inspired idea... this is what animals do. So through trial and error, through experience, through positive and negative rewards and feedback, they learn how to interact with their environment. OK good. So before I jump in I want to show some motivating videos. I really like this one where reinforcement learning is used to learn how to walk in this artificial environment. And there's a lot of papers like this where people use reinforcement learning as kind of an optimization framework to learn how to control a complex system, in this case a bipedal walker, often in a simulated environment. And this just looks really cool and it's a difficult control problem. This is a really hard non-linear control problem. Now the goal would be to take what you learn here and start to port that over into the real world to make better robots and better actual physical agents that can interact with the world alongside us, to learn how to learn like humans and animals do. So another video I love... this is my dog Mordecai and my wife has trained him... this is a treat on his nose... to hold the treat on his nose until she says ok, after which he can then grab the treat and eat it. This is not an easy trick to learn and this again this goes to show you anytime you, anybody who's trained an animal, a dog or any other animal, has done some type of reinforcement learning or reinforcement training. OK and so that's actually where the word reinforcements comes from is that in in animal systems in human systems you in you reinforce good behavior with rewards like treats okay and so that's kind of the whole name of the game here is learning a good control strategy or a good set of actions through positive reinforcement good so that's what we're gonna talk about today I'm gonna walk you through the framework so I want to disentangle there is a reinforcement learning framework kind of the framework for how you learn to interact with the environment and then there's a hard optimization problem for how you actually optimize the agents actions or policies given that framework and those are kind of two pieces that we're going to talk about today and then in a future video I'm going to talk about kind of deep reinforcement learning or reinforcement learning with modern techniques and deep neural networks and some of the incredible applications and and performance that you can get out of those systems good so also I'll point out you can follow updates on these videos at eggin steve on twitter please like please subscribe hit the bell so you get notifications and comment below tell me what you want to see more of tell me what you like or don't like oftentimes people in the comments provide a lot of really important useful information that I might have left out of these videos so I think it's also a big service to other people watching these all right so we're gonna jump in and we're gonna build this reinforcement learning framework from the ground up from scratch and so at the heart of it you start with an agent and an environment and the agent I actually like the name agent because it implies some agency the agent gets to take actions to interact with the environment so in the first example and I'm gonna have a few examples we're going to talk about a mouse in a maze so the agent is a mouse the environment is a maze the mouse gets to measure its current state in the environment so it measures that state s notice that it doesn't measure the full state the mouse does not have a top-down view of the whole maze it just knows where it is right now and where it was in the past and then the add Mouse gets to take some action a it gets to make some decision about what to do next okay so it could turn left it could turn right or it could go forward in this case and only until the very end of the maze does the mouse actually get a reward are so these rewards are very sparse few and far between in this case if it goes to the very end of the maze it might get a piece of cheese actually my wife tells me that when they do training experiments for rats they really like fruit loops and it looks adorable because a fruit loop is gigantic to a mouse or to a rat but the moral of the story here is that this agent gets to make some decisions it has control over its actions so it has agency and the environment it gets to measure where it is in the environment and occasionally it gets rewards very occasionally it gets rewards and so part of the the goal of this system is to learn what actions actually caused it to get a reward or not okay and this is in some sense in the machine learning lingo this is called semi-supervised learning so if the mouse got a reward at every single stage of the maze if at every correct turn it got a piece of cheese "
Machine Learning Project: Predicting House Prices,https://www.youtube.com/watch?v=MJD-_QsH0yc,
Machine Learning Project: Predicting House Prices,https://www.youtube.com/watch?v=vlBZ50P_UpE,hello everyone welcome to mcprotech today in this section we learn about how to develop a machine learning project so based on the what and all the different python concepts and machine learning libraries we have learned now i am going to show you how we can develop a machine learning project so in this section i'm going to show you a house price prediction project so how we how we are going to develop this machine learning project in order to predict the house price i'm going to show you so the i'm going to show you execute all the programming part required for this project in jupyter notebook so first go to home page of your system or laptop uh in the search bar you write anaconda navigator so it should be previously installed in your system so open the new search here unaccounted navigator it will show so open anaconda navigator and when you open as you can see here this is the home page of anaconda navigator here you should open the jupyter notebook as you can see here there is a jupiter notebook here there is a launch button available here so just click on this launch on so the jupiter notebook will open in the default browser of your system or laptop so this is how a jupiter notebook will open so now i am going to explain you this house house price prediction project so what exactly we are going to do in this project so here this is what the program required for this household like house price prediction project so in this project we are going to take on a data set which contains the house price details required for prediction of a house price so based on the data like different data available in the data set we are going to train the machine learning model so whatever the train model we are going to use that in order to predict the uh house price so what and all the data steps required in order to develop this machine learning project i am going to explain you in this section so first in the different uh libraries or packages we are using in this project is numpy pandas matplotlib in order to plot the different graphs and a c bond so first we have to import all the required so required libraries are packages we have to import so if you in order to import first you should be installed in these packages should be installed in your system so you can install them in the command prompt python command prompt where you can directly install paper install numpy prep install uh pandas pip install matplotlib pip install sibo like that a command you give and enter the packages will be installed so now first we will import all the required packages so import numpy as np import this number numpy is mainly required in order to work with the data set and also do not to do some function it is required in the project and import pandas as pd so pandas library mainly required in order to do and do some manipulation and work on the data set and import matplotlib dot pi plot as plt so this matplotlib and c bond library import c bonuses these two are required in order to plot a different uh different kind of plots required for the projects so after importing all the required packages so now we'll load our data set so in order to load the data set we should make use of pandas libraries so pandas library have allies does the name pd so pd dot read underscore csv so this read underscore csv mainly used in order to load the data set which are in csv format that is comma separated values so pd dot read underscore csv of inside the braces you should give give the name of the data set so the name the data set name i have given which is stored in my system is house house dataset.csv so one more important thing is when you want to use a particular data set in your if you want to use the particular data set required for your project you should upload the data set in the jupyter notebook how you can upload that means when you open the jupyter notebook the default home page open will look like this whenever it open as you can see in the video here in this section so whatever it is showing here there is a upload button you can see here this you should click on this upload button so it will go to your file manager and you open the file manager here wherever the data set particular data set have downloaded that you should select and open so when you open that the data set will be come here at that moment when i now i will open my house data set when i click on here you can see here now i have opened that particular data set here so it is showing upload so when it shows upload what you should do is just click on that upload here when you click on that the data set will be added to your jupyter notebook here this is already file name as you can see here it is showing how that because i have already added uploaded to my system here whatever the data set i'm using so just click on cancel if you are not added means it will show that a data set when you add that will be updated here here i will show you what is that here as you can see house house data date data set dot csv format so in this way you can add whatever the file or data set required for your particular project you can add here whatever all the or like uh 
Machine Learning Project: Predicting House Prices,https://www.youtube.com/watch?v=y73wuRvX8Aw,hi guys now i am explaining about linear regression using our studio that is in first we use general muscle learning for python and generally ask today also so i'm using this my data is house price or spread.tsv is my data first of all we are saying the data to dt run this and store it in df just run it and we have to view the data see this is my data with 19 columns and final six observations so you can see here df dt has the same okay now summary summary means let's see what is this summary see here for every variable there is minimum maximum first quarter second quarter third quartile and median mean all values are there so by using this we can find how much variable is affecting a this is prices dependent variable and these all independent variable we have to calculate the price based on these r values see here buster here only s value only when value is affecting that is not affecting it is usually setting and here somewhat values are missing indian means so we have to fill that these are concluding from this let's see this we have to plot some graph like this first of all from this you can see this is for price prime rate rainfall and nano orbitals here we see some like this these are linear relationship but increasing or decreasing but these are some note layers we have to solve that because we want quality so we have to solve that and this is not having any linear relationship somewhat like this so it's look like a logarithm value so just look locking the values to solve that and make a clean relationship with prime return rise and we use in our bedrooms to solve out what that is outlays so now we run these questions that is yes here only one variable so we know we don't need that variable so indeed not affecting because i destroyed because if we run airport here at least there is two values so this is affecting somewhat that leg and we run this summary of you know rooms that is see this maximum is very long large and these are at the same range we have to change that so we're using in somewhat quartile there is a thin quarter we use that to make three times portal value is by obtaining the data we can make quarter value and i send it to value to this maximum above maximum values so i'm running this see here maximum value 46 here or not one so these are similar and we run the rainfall also your rainfall summary is 60 that is imagine 60 min 3 this is not comparable so we change the lower value that is this lower value using 0.3 run this we can see that 3 can change to 6 that is we want and we solve this and we have some missing values in a as you can see that announcement see here we are in here that is nothing is there here so we have to assume somewhat values there also so we assign a mean value for that mean of that column data see first of all without any means the mean value is this and these are the equations where no value is there missing value so we are saying mean value to that and the total mean value is changing somewhat that is we make a linear relationship between crime rate and rainfall i suffered first that is crime rate employees are not having a linear relationship siren this these are linear relationship i change it to log the value before it is like this now it is having some linear relationship so and i am using creating a new variable average distance distance foundation solution three distinct foreign distances from the house which has to be sold the employment opportunities distances that have employment opportunities from the house were to be sold so these are affecting same so i'm doing an average for these values then creating new variable and i am removing that four values from that data that is okay now i am storing that in df2 you can see df2 there is no distance from listening to distance three only have resistance okay now we have some values like this c like this airport yes no no no under river lake none like this these values our values are integers but for data crossing we have to make these values also in one zeros orienting type so i'm making that using this package we use here packages like see these packages here i already installed dummies just installed packages of dummies so that are installed so click on this this is running now then libras library dummies is provided so now we can use this to send dummy values to that that is this and this let's see what happens see here here airport no zero airport is two columns are there and for water water bodies these are split up into four values and given an integer value yes that's what we want and we don't need any here two values we don't need only one value we can make so we can delete this and we can delete any one value from this that is we will give it water body none so we'll leave it first column nine that is this around this like you see there is no airport no value and i will run this again then there is water body none will not be c17 variables now we have correlation matrix that is we have to see which value is effecting the most the price value so we are saying this run this first of all there is nothing just core of df2 let's see which value is affecting yes let's see you can see that these all values are one the correlation 
Machine Learning Project: Predicting House Prices,https://www.youtube.com/watch?v=HSxFWWbt9S0,"In today’s video I am going to Show how to train a model for predicting house prices in a python application using tensorflow and keras Prepare the files that we need in order to be able to use the same model in a javascript app using tensorflow.js and convert the model The final result of these two parts video, is a vue.js app which expects the size of the apartment, number of bedrooms, bathrooms, parking spaces and the neighborhood so that it can return the price for the house. For the complete application we will have two projects, one written in python for training the model using keras and tensorflow as the backend and another one written using javascript for the web app using vue.js and tensorflowjs So let’s start!!! Inside the python project we have several python files, one for training the model, one for predicting, another one for converting some of the data for later use inside the javascript app and finally a set of common_somethings with stuff to be reused inside the other python files Also, we have a folder for the dataset which only has a csv file with a row per house price, in each row we have, the price, the size in meters, number of rooms, baths, parking spaces and finally the neighborhood in which the house is located. The dataset was cleaned a little bit but it could use some more cleaning. A folder for storing the different trained models using different parameters, like if the model was trained using feature scaling or categorical encoding. And finally one with all the files needed on the web app for making predictions, like the converted model, mean and variance for the scaled features and the list of neighborhoods Let’s start with the Common.py modules in which we define first the default folder for storing the files related to the model and then also create the folder if it doesn’t exist already, the filename for the final version of the model and also for the checkpoints And the column names that we are using for the input and output of the model On the common_visualization.py I have a function for plotting the different metrics for the model so that it is easier to analyze its performance later. The common_scaler.py with the functions needed for creating and loading the scaler and one for normalizing the current data The common_categorical.py with similar functions for also creating and loading an encoder and one for encoding a categorical feature Inside the common_pre_post_processing.py A function for preprocessing input columns and another one for post processing output values using the scaler and the categorical encoders. and finally inside the common_file.py a function for creating json files mostly for sharing stuff with the tensorflow.js app Inside the train_model.py we define different cells as with the previous video In the first one I import several libraries that we need for training the model, like pandas for importing and working with the dataset Scikit learn for splitting the dataset Tensorflow for checking the version and what type of device we are using given that we are using it as the backend for keras Several keras modules for creating and training the model And our common_somethings modules In the next cell I load the dataset in csv format, divide the dataframe into the inputs and outputs that we need for the model and print the first few items In the next cells I create the categorical encoder for the neighborhood and a scaler for the numeric features, I will explain more about scaling and one hot encoding in future videos. Then I use scikit-learn for splitting the dataset into a train and a test sets and then print the size and a few items for each one I transform the inputs and outputs into the format that the network understands using the onehot encoder and the scaler for the training and validation datasets I, then, define a simple model with only one dense layer with 100 units, relu as the activation and dropout and then another dense layer with the size of the outputs which in this case will be the price for the house, The model is compiled using ADAM for the optimizer, mean squared error as the loss function and mean absolute error for the metrics Finally we print the summary describing the model that we built Then I define some hyperparameters for the model like how many epochs to iterate and the batch size and some callbacks for keras for creating checkpoints for the model and also one for stopping the training if we haven’t improved for 20 epochs In the next cell I train the model and store the history for later analysis, then I save the model to a file so it can be used later and then evaluate and print the results Finally I plot the results of the training session so that we can visualize better the performance of the model And finally For the Predict.py file we import the load model module from keras, And also our common modules I Load the trained model using the file name that we defined before I define a little sample of data to predict, using the size in meters, number of rooms, baths, parking spaces and the neighborhood in which the property is located I transform the inputs into the format and ranges expected by the network, call the predict function and transform the outputs into a more understandable format and finally print the results. This is a similar process that we are going to follow for the web app, first we will load the model, load the encoders and scalers and once we input some values we need to transform them into the form that the network understands, call predict, post process the outputs and then print them Inside the share_preprocessing_tfjs.py file I include the common modules as I have been doing for the other files I load the categorical encoder, get the list "
Machine Learning Project: Predicting House Prices,https://www.youtube.com/watch?v=zpD4omYRleA,welcome to introduction to machine learning this lecture is a an example and the idea is that we will go through a very simple example using all the topics that we've discussed so far and take a little bit of a look at the sort of results one can expect and also the code that's necessary to get it to work so this is a a data set that comes from kaggle kaggle is a google owned company that organizes machine learning competitions you can go to kaggle and download data for a range of different uh domains in this case this is uh house prices kaggle also keeps data sets in escrow so that one can compare the results of your own machine learning algorithm against a validation set which you've not seen before and other people haven't seen before and it also keeps track of other people's performance on particular data sets so it's a worthwhile place to go to get experience with trying machine learning a variety of domains this is a data set that consists of prices and features for 1456 homes in ames iowa and those were homes that were sold between 2006 and 2010 and here our goal is going to be to try to use the features of the houses in order to predict the price and we're going to focus on predicting the log of the price because the prices relative price is uh much more important than absolute price and house prices typically vary over a significant range so our performance metric is going to be the rms error on the test set of the log of the house price now in particular if you have a an rms error of say 0.1 then it means that you can predict house prices within a factor of e to the point one which is about ten and a half percent here's the sort of thing one sees in this data here we have a plot of the target variable which is the log of the price against one of the independent variables the living area of the house in this case and you can see well first of all there's uh quite a lot of variation just knowing the living area doesn't narrow down the price very much here we've got house prices varying between e to the 10 and a half and e to the 13 e to the 14. which is must be less than 100 000 to more than 500 000 and uh so we're seeing quite a quite a variation here now the data set actually contains some 80 features we're going to use maybe the first 20 features and uh focus on uh on those so for our embedding uh for embedding the target variable we're going to let v be the price and y be the log of v it's the log of the price and then for the independent variables x we're going to embed them as follows we have some of those fields of a house record are numerical and we those we can just embed unchanged so in particular we have the year the house was built the area of the living space the area of the first floor the area of the second floor the area of the garage the area of the wooden deck the area of the basement the year of the last remodel and the area of the lot so all of the areas are in square feet they're just numbers and the years are simply years they're just integers and we will just embed those as numbers as they are there are also ordinal fields in our features and we will embed those as integers so we have a number of bedrooms number of kitchens number of fireplaces number of half bathrooms number of rooms condition condition is a number that's scored between 1 and 10 that's assigned by an expert presumably an appraiser or a realtor we have the quality of the materials and the finish again assigned by an expert but it's a score of between one and ten and the number of cars that the garage can hold so these are all small integers typically between zero and ten and we just embed them as as they are the kitchen quality is a field which is stored on a leica scale the kitchen is rated excellent good typical fair or poor although i don't think there are actually any entries in the data set that we see for poor rating and this is encoded as an integer between one and five after we embed it the building type is a categorical field this is embedded one hot and so it's embedded as a five dimensional vector one of the canonical unit vectors with a one in one position and zero and all the other positions the five different categories are single family townhouse end unit two family conversion townhouse inside unit and duplex there's also a neighborhood field there are 25 different neighborhoods so it's a categorical data field and this is also one hot embedded as a result we have looking back at our fields here we have 17 numerical fields and we have the kitchen quality which is 18 and then we have 30 components which are one hot so the total dimension of our x data variable is 48 and we're going to add one more which will be the constant now when we do the standardization and the data splitting we do this in a particular way so we split the data randomly 80 20 80 for training and 20 for test and that gives us an x zero training set and a corresponding y train and an x zero test set and a corresponding y-test now the way we do standardization is we use the training set to compute the means and the standard deviations of each of the features 
