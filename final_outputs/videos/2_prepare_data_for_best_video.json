{"Introduction to Deep Learning": [{"https://www.youtube.com/watch?v=aircAruvnKk": "This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math. My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around. But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests neural networks are inspired by the brain, but let's break that down. What are the neurons, and in what sense are they linked together? Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1. It's really not more than that. For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels. This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled. In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen. In practice there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. "}, {"https://www.youtube.com/watch?v=q6kJ71tEYqM": "look fair warning if you're feeling a little hungry right now you might want to pause this video and grab a snack before continuing because i'm going to explain the difference between machine learning and deep learning by talking about pizza delicious tasty pizza now before we get to that let's let's address the fundamental question here what is the difference between these two terms well put simply deep learning is a subset of machine learning actually the the hierarchy goes like this at the top we have a i or artificial intelligence now a subfield of a i is ml or machine learning beneath that then we have n n or neural networks and they make up the backbone of deep learning algorithms dl and here on the ibm technology channel we have a whole bunch of videos on these topics you might want to consider subscribing now machine learning algorithms leverage structured labeled data to make predictions so let's build one a model to determine whether we should order pizza for dinner there are three main factors that influence that decision so let's map those out as inputs the first of those inputs we'll call x1 and x1 asks will it save time by ordering out we can say yes with a one or no with a zero yes it will so x that equals one now x two that input says will i lose weight by ordering pizza that's a zero i'm i'm ordering all the toppings and x3 will it save me money actually i have a coupon for a free pizza today so that's a one now look these binary responses ones and zeros i'm using them for simplicity but neurons in a network can represent values from well everything to everything negative infinity to positive infinity with our inputs defined we can assign weights to determine importance larger weights make a single inputs contribution to the output more significant compared to other inputs now my threshold here is five so let's weight each one of these w1 well i'm going to give this a full five because i value my time and w2 this was the will i lose weight 1 i'm going to rate this a 3 because i have some interest in keeping in shape and for w3 i'm going to give this a 2 because like either way this isn't going to break the bank to order dinner now we plug these weights into our model and using an activation function we can calculate the output which in this case is the decision to order pizza or not so to calculate that we're going to calculate the y hat and we're going to use these weights and these inputs so here we've got 1 times 5 we've got 0 times 3 and we've got 1 times 2. and we need to consider as well our threshold which was 5. so that gives us if we just add these up 1 times 5 that's 5 plus 0 times 3 that's 0 plus 1 times 2 that's 2 minus 5. well that gives us a total of positive 2. and because the output is a positive number this correlates to pizza night okay so that's machine learning but what differentiates deep learning well the answer to that is more than three as in a neural network is considered a deep neural network if it consists of more than three layers and that includes the input and the output layer so we've got our input and output we have multiple layers in the middle and this would be considered a deep learning network classical machine learning is more dependent on human intervention to learn human experts well they determine a hierarchy of features to understand the differences between data inputs so if i showed you a series of images of different types of fast food like pizza burger and taco you could label these in a data set for processing by the neural network a human expert here has determined the characteristics which distinguish each picture as the specific fast food type so for example it might be the bread of each food type might be a distinguishing feature across each picture now this is known as supervised learning because the process incorporates human intervention or human supervision deep machine learning doesn't necessarily require a labeled data set it can ingest unstructured data in its raw form like text and images and it can automatically determine the set of features which distinguish pizza burger and taco from one another by observing patterns in the data a deep learning model can cluster inputs appropriately these algorithms discover hidden patterns of data groupings without the need for human intervention and they're known as unsupervised learning most deep neural networks are feed forward that means that they go in one direction from the input to the output however you can also train your model through something called a back propagation that is it moves in the opposite direction from output to input back propagation allows us to calculate and attribute the error associated with each neuron and allows us to adjust and fit the algorithm appropriately so when we talk about machine learning and deep learning we're essentially talking about the same field of study neural networks they're the foundation of both types of learning and both are considered subfields of a i the main distinction between the two are that number of layers in a neural network more than three and whether or not human intervention is required to label data pizza burgers tacos yeah that's uh that's enough for today it's time for lunch oh oh and before i go if you did enjoy this video here are some others you might also like if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching "}, {"https://www.youtube.com/watch?v=O5xeyoRL95U": "Welcome everyone to 2019. It's really good to see everybody here make it in the cold. This is 6.S094 Deep Learning for Self-Driving Cars. It is part of a series of courses on deep learning that we're running throughout this month. The website that you can get all the content of videos, the lectures and the code is deeplearning.mit.edu. The videos and slides will be made available there along with a github repository that's accompanying the course. Assignments for registered students will be emailed later on in the week. And you can always contact us with questions, concerns, comments at hcai, human centered AI, at mit.edu. So let's start through the basics, the fundamentals. To summarize in one slide, what is deep learning? It is a way to extract useful patterns from data in an automated way with as little human effort involved as possible hence to automate it. How? The fundamental aspect that we'll talk about a lot is the optimization of neural networks. The practical nature that we'll provide the code and so on is that there's libraries that make it accessible and easy to do some of the most powerful things in deep learning using Python, TensorFlow & friends. The hard part always with machine learning artificial intelligence in general is asking good questions and getting good data. A lot of times the exciting aspects of what's the news covers and a lot of the exciting aspects of what is published and that the prestigious conferences in an archive, in a blog post is the methodology. The hard part is applying the methodology to solve real world problems, to solve fascinating interesting problems. And that requires data, that requires asking the right questions of that data, organizing that data and labeling selecting aspects of that data that can reveal the answers to the questions you ask. So why has this breakthrough over the past decade of the application of neural networks, the ideas in neural networks? What has happened? What has changed? They've been around since the 1940s. And ideas were percolating even before. The digitization of information, data. The ability to access data easily in a distributed fashion across the world. All kinds of problems have now a digital form. They can be accessed by learning algorithms. Hardware; compute, both the Moore's Law of CPU and GPU and ASICs, Google's TPU systems, hardware that enables the efficient effective large-scale execution of these algorithms. Community; people here, people all over the world are being able to work together, to talk to each other, to feed the fire of excitement behind machine learning. github and beyond. The tooling; we'll talk about TensorFlow PyTorch and everything in between that enables a person with an idea to reach a solution in less and less and less time. Higher and higher levels of abstraction empower people to solve problems in less and less time with less and less knowledge, where the idea and the data become the central point, not the effort, that takes you from an idea to the solution. And there's been a lot of exciting progress. Some of which we'll talk about from face recognition to the general problem of scene understanding, image classification, the speech, text, natural language processing, transcription, translation in medical applications and medical diagnosis. And cars being able to solve many aspects of perception in autonomous vehicles with drivable area, lane detection, object detection, digital assistance, ones on your phone and beyond the ones in your home. Ads, recommender systems from Netflix to search to social, Facebook. And of course deep reinforcement learning successes in the playing of games, from board games to StarCraft and Dota. Let's take a step back. Deep learning is more than a set of tools to solve practical problems. Pamela McCorduck said in 79 \"AI began with the ancient wish to forge the gods.\" Throughout our history, throughout our civilization, human civilization we've dreamed about creating echoes of whatever is in this mind of ours in the machine. And creating living organisms from the popular culture in the 1800s with Frankenstein to Ex Machina this vision is dream of understanding intelligence and creating intelligence has captivated all of us. And deep learning is at the core of that. Because there's aspects of, the learning aspects that captivate our imagination about what is possible. Given data and methodology what learning learning to learn and beyond how far that can take us. And here visualized is just 3% of the neurons and one millionth of the synapses in our own brain. This incredible structure that's in our mind and there's only echoes of it. Small shadows of it in our artificial neural networks that we're able to create. But nevertheless those echoes are inspiring to us. The history of neural networks on this pale blue dot of ours started quite a while ago with summers and winters, with excitements and periods of pessimism. Starting in the 40s with neural networks and the implementation of those neural networks is a perceptron in the 50s; with ideas of backpropagation, restricted Boltzmann machine, recurrent neural networks in the 70s and 80s; with convolutional neural networks and the MNIST data set with data sets beginning to percolate LSTM, bi-directional RNNs in the 90s; and the rebranding and the rebirth of neural networks under the flag of Deep Learning and Deep Belief Nets in 2006; the birth of ImageNet, the data set that on which the possibilities of a deep learning can bring to the world has been first illustrated in the recent years in 2009. And AlexNet the network that an ImageNet performed exactly that with a few ideas like dropout and improved neural networks over time every year by year improving the performance of neural networks. In 2014 the idea of GANs, the Yann LeCun called the most exciting idea of the last 20 years, the Generative Adversarial Networks, the ability to with very little supervision generate data, to generate ideas after forming representation of those. "}, {"https://www.youtube.com/watch?v=QDX-1M5Nj7s": "Good afternoon everyone! Thank you all for joining today. My name is Alexander Amini and I'll be one of your course organizers this year along with Ava -- and together we're super excited to introduce you all to Introduction to Deep Learning. Now MIT Intro to Deep Learning is a really really fun exciting and fast-paced program here at MIT and let me start by just first of all giving you a little bit of background into what we do and what you're going to learn about this year. So this week of Intro to Deep Learning we're going to cover a ton of material in just one week. You'll learn the foundations of this really really fascinating and exciting field of deep learning and artificial intelligence and more importantly you're going to get hands-on experience actually reinforcing what you learn in the lectures as part of hands-oOn software labs. Now over the past decade AI and deep learning have really had a huge resurgence and many incredible successes and a lot of problems that even just a decade ago we thought were not really even solvable in the near future now we're solving with deep learning with Incredible ease. Now this past year in particular of 2022 has been an incredible year for a deep learning progress and I like to say that actually this past year in particular has been the year of generative deep learning using deep learning to generate brand new types of data that I've never been seen before and never existed in reality in fact I want to start this class by actually showing you how we started this class several years ago which was by playing this video that I'll play in a second now this video actually was an introductory video for the class it kind of exemplifies this idea that I'm talking about. So let me just stop there and play this video first of all Hi everybody and welcome to MIT 6.S191 -- the official introductory course on deep learning taught here at MIT. Deep Learning is revolutionizing so many fields: from robotics to medicine and everything in between. You'll learn the fundamentals of this field and how you can build some of these incredible algorithms. In fact, this entire speech and video are not real and were created using deep learning and artificial intelligence. And in this class you'll learn how. It has been an honor to speak with you today and I hope you enjoy the course. so in case you couldn't tell this video and its entire audio was actually not real it was synthetically generated by a deep learning algorithm and when we introduced this class A few years ago this video was created several years ago right but even several years ago when we introduced this and put it on YouTube it went somewhat viral right people really loved this video they were intrigued by how real the video and audio felt and looked uh entirely generated by an algorithm by a computer and people were shocked with the power and the realism of these types of approaches and this was a few years ago now fast forward to today and the state of deep learning today we have have seen deep learning accelerating at a rate faster than we've ever seen before in fact we can use deep learning now to generate not just images of faces but generate full synthetic environments where we can train autonomous vehicles entirely in simulation and deploy them on full-scale vehicles in the real world seamlessly the videos here you see are actually from a data driven simulator from neural networks generated called Vista that we actually built here at MIT and have open sourced to the public so all of you can actually train and build the future of autonomy and self-driving cars and of course it goes far beyond this as well deep learning can be used to generate content directly from how we speak and the language that we convey to it from prompts that we say deep learning can reason about the prompts in natural language and English for example and then guide and control what is generated according to what we specify we've seen examples of where we can generate for example things that again have never existed in reality we can ask a neural network to generate a photo of a astronaut riding a horse and it actually can imagine hallucinate what this might look like even though of course this photo not only this photo has never occurred before but I don't think any photo of an astronaut riding a horse has ever occurred before so there's not really even training data that you could go off in this case and my personal favorite is actually how we can not only build software that can generate images and videos but build software that can generate software as well we can also have algorithms that can take language prompts for example a prompt like this write code and tensorflow to generate or to train a neural network and not only will it write the code and create that neural network but it will have the ability to reason about the code that it's generated and walk you through step by step explaining the process and procedure all the way from the ground up to you so that you can actually learn how to do this process as well now I think some of these examples really just highlight how far deep learning and these methods have come in the past six years since we started this course and you saw that example just a few years ago from that introductory video but now we're seeing such incredible advances and the most amazing part of this course in my opinion is actually that within this one week we're going to take you through from the ground up starting from today all of the foundational building "}, {"https://www.youtube.com/watch?v=0qm_OL_VHGE": "ANNA BROWN: The way we interact with computers is changing. Its becoming a lot more, human. But no approach has made a more significant impact than deep learning. Deep learning is a type of machine learning that uses deep neural networks that imitate how our brains think. WAYNE THOMPSON: Lets say that I had fruit. Just like you know, a third grader is going to look at it and understand the difference between an apple, a peach, and an orange. You have to have features like color, shape and texture. And you give deep learning algorithm these features or data. And then through deep learning and progressive learning its actually able to determine what kind of fruit it is. ANNA BROWN: Deep learning sets up basic parameters about the data and trains the computer to learn on its own. It recognizes patterns using many layers of processing. ATHINA KANIOURA: Deep learning is focused on deep neural networks because it has the ability through graph analysis and computer vision to understand and define relationships amongst data that are not easily recognizable through traditional machine learning techniques. XIANGXIANG MENG: Well, deep learning is popular. I think it save people a lot of time. Because if you apply the traditional machine learning models to this unstructured data such as images and text you need to manually extract a lot of features from those data. ANNA BROWN: If you think that sounds like a lot work, it is. Until GPUs, or graphical processing units, came on the scene neural nets were declared impractical. A single GPU can have thousands of cores while a typical CPU has no more than 16. So it seems like deep learning thrives with lots and lots of data. Which is cool. But, it also has its quirks. A growing challenge is interpretability. WAYNE THOMPSON: We want to understand why that patient has cancer. We want to understand why that person was denied credit. OK? And thats only fair. So we want these models to have some level of interpretability. Now the problem is that these models have gazillions of these parameters and combinations of the data. ATHINA KANIOURA: You need to have a deep industry content plus also a deep computer science background to be able to make any type of actionable insights out of the analysis. WAYNE THOMPSON: Im bullish that we can, through deep learning, solve some of these very difficult problems like our diseases. I think that area is going to really ramp up and where well see a lot of benefit, tons of benefit. OLIVER SCHABENBERGER: Deep neural networks, they are the heart of this artificial intelligence revolution we are going through right now. Can be applied in many many many spaces. Natural language generation. Natural language interaction. Computer vision. Speech to text. They can do this exceedingly well and that is really revolutionary. "}], "Neural Networks: Foundations": [{"https://www.youtube.com/watch?v=aircAruvnKk": "This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math. My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around. But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests neural networks are inspired by the brain, but let's break that down. What are the neurons, and in what sense are they linked together? Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1. It's really not more than that. For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels. This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled. In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen. In practice there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. "}, {"https://www.youtube.com/watch?v=jmmW0F0biz0": "Here are five things to know about neural networks in under five minutes. Number one: neural networks are composed of node layers. There is an input node layer, there is a hidden layer, and there is an output layer. And these neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI and deep learning. In fact, we should be describing this as an artificial neural network, or an ANN, to distinguish it from the very un-artificial neural network that's operating in our heads. Now, think of each node, or artificial neuron, as its own linear regression model. That's number two. Linear regression is a mathematical model that's used to predict future events. The weights of the connections between the nodes determines how much influence each input has on the output. So each node is composed of input data, weights, a bias, or a threshold, and then an output. Now data is passed from one layer in the neural network to the next in what is known as a feed forward network -- number three. To illustrate this, let's consider what a single node in our neural network might look like to decide -- should we go surfing. The decision to go or not is our predicted outcome or known as our yhat. Let's assume there are three factors influencing our decision. Are the wave's good, 1 for yes or 0 for no. The waves are pumping, so x1 equals 1, 1 for yes. Is the lineup empty, well unfortunately not, so that gets a 0. And then let's consider is it shark-free out there, that's x3 and yes, no shark attacks have been reported. Now to each decision we assign a weight based on its importance on a scale of 0 to 5. So let's say that the waves, we're going to score that one, eh, so this is important, let's give it a 5. And for the crowds, that's w2. Eh, not so important, we'll give that a 2. And sharks, well, we'll give that a score of a 4. Now we can plug in these values into the formula to get the desired output. So yhat equals (1 * 5) + (0 * 2) + (1 * 4), then minus 3, that's our threshold, and that gives us a value of 6. Six is greater than 0, so the output of this node is 1 -- we're going surfing. And if we adjust the weights or the threshold, we can achieve different outcomes. Number four. Well, yes, but but but number four, neural networks rely on training data to learn and improve their accuracy over time. We leverage supervised learning on labeled datasets to train the algorithm. As we train the model, we want to evaluate its accuracy using something called a cost function. Ultimately, the goal is to minimize our cost function to ensure the correctness of fit for any given observation, and that happens as the model adjusts its weights and biases to fit the training data set, through what's known as gradient descent, allowing the model to determine the direction to take to reduce errors, or more specifically, minimize the cost function. And then finally, number five: there are multiple types of neural networks beyond the feed forward neural network that we've described here. For example, there are convolutional neural networks, known as CNNs, which have a unique architecture that's well suited for identifying patterns like image recognition. And there are recurrent neural networks, or RNNs, which are identified by their feedback loops and RNNs are primarily leveraged using time series data to make predictions about future events like sales forecasting. So, five things in five minutes. To learn more about neural networks, check out these videos. Thanks for watching. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please Like and Subscribe. "}, {"https://www.youtube.com/watch?v=CqOfi41LfDw": "Neural networks... seem so complicated, but they're not! StatQuest! Hello! I'm Josh Starmer and welcome to StatQuest! Today, we're going to talk about neural networks, part one: inside the black box! Neural networks, one of the most popular algorithms in machine learning, cover a broad range of concepts and techniques. however, people call them a black box because it can be hard to understand what they're doing. the goal of this series is to take a peek into the black box by breaking down each concept and technique into its components and walking through how they fit together, step by step. in this first part, we will learn about what neural networks do, and how they do it. in part two, we'll talk about how neural networks are fit to data with backpropagation. then, we will talk about variations on the simple neural network presented in this part, including deep learning. note: crazy awesome news! i have a new way to think about neural networks that will help beginners and seasoned experts alike gain a deep insight into what neural networks do. for example, most tutorials use cool looking, but hard to understand graphs, and fancy mathematical notation to represent neural networks. in contrast, i'm going to label every little thing on the neural network to make it easy to keep track of the details. and the math will be as simple as possible, while still being true to the algorithm. these differences will help you develop a deep understanding of what neural networks actually do. so, with that said, let's imagine we tested a drug that was designed to treat an illness and we gave the drug to three different groups of people, with three different dosages: low, medium, and high. the low dosages were not effective so we set them to zero on this graph. in contrast, the medium dosages were effective so we set them to one. and the high dosages were not effective, so those are set to zero. now that we have this data, we would like to use it to predict whether or not a future dosage will be effective. however we can't just fit a straight line to the data to make predictions, because no matter how we rotate the straight line, it can only accurately predict two of the three dosages. the good news is that a neural network can fit a squiggle to the data. the green squiggle is close to zero for low dosages, close to one for medium dosages, and close to zero for high dosages. and even if we have a really complicated dataset like this, a neural network can fit a squiggle to it. in this StatQuest we're going to use this super simple dataset and show how this neural network creates this green squiggle. but first, let's just talk about what a neural network is. a neural network consists of nodes and connections between the nodes. note: the numbers along each connection represent parameter values that were estimated when this neural network was fit to the data. for now, just know that these parameter estimates are analogous to the slope and intercept values that we solve for when we fit a straight line to data. likewise, a neural network starts out with unknown parameter values that are estimated when we fit the neural network to a dataset using a method called backpropagation. and we will talk about how backpropagation estimates these parameters in part 2 in this series. but, for now, just assume that we've already fit this neural network to this specific dataset, and that means we have already estimated these parameters. also, you may have noticed that some of the nodes have curved lines inside of them. these bent or curved lines are the building blocks for fitting a squiggle to data. the goal of this StatQuest is to show you how these identical curves can be reshaped by the parameter values and then added together to get a green squiggle that fits the data. note: there are many common bent or curved lines that we can choose for a neural network. this specific curved line is called soft plus, which sounds like a brand of toilet paper. alternatively, we could use this bent line, called ReLU, which is short for rectified linear unit, and sounds like a robot. or, we could use a sigmoid shape, or any other bent or curved line. oh no! it's the dreaded terminology alert! the curved or bent lines are called activation functions. when you build a neural network you have to decide which activation function, or functions, you want to use. when most people teach neural networks they use the sigmoid activation function. however, in practice, it is much more common to use the ReLU activation function, or the soft plus activation function. so we'll use the soft plus activation function in this StatQuest. anyway, we'll talk more about how you choose activation functions later in this series. note: this specific neural network is about as simple as they get. it only has one input node, where we plug in the dosage, only one output node to tell us the predicted effectiveness, and only two nodes between the input and output nodes. however, in practice, neural networks are usually much fancier and have more than one input node, more than one output node, different layers of nodes between the input and output nodes, and a spider web of connections between each layer of nodes. oh no! it's another terminology alert! these layers of nodes between the input and output nodes are called hidden layers. when you build a neural network one of the first things you do is decide how many hidden layers you want and how many nodes go into each hidden layer. although there are rules of thumb for making decisions about the hidden layers, you essentially make a guess and see how well the neural network performs, adding more layers and nodes if needed. "}, {"https://www.youtube.com/watch?v=rEDzUT3ymw4": "Imagine a Neural Network as a kind of black box, which takes one or multiple inputs like the sensors of a self-driving car, processing them into one or multiple outputs like the controls of that car. The neural network itself consists of many small units called \"Neurons\". These Neurons are grouped into several layers. Units of one layer interact with the Neurons of the next layer through \"weighted connections\" which really are just connections with a real-valued number attached to them. A Neuron takes the value of a connected Neuron and multiplies it with their connection's weight. The sum of all connected Neurons and the Neuron's bias value is then put into a so-called \"activation function\", which simply mathematically transforms the value before it finally can be passed on to the next Neuron. This way the inputs are propagated through the whole network. That's pretty much all the network does but the real deal behind neural networks is to find the right weights in order to get the right results. This can be done through a wide range of techniques such as machine learning however that's a topic for another minute. "}, {"https://www.youtube.com/watch?v=GXhBEj1ZtE8": "What if someone told you that a machine has discovered an antibiotic that can treat previously untreatable strains of bacteria? Yes, you heard that right! The antibiotic is a chemical named Halicin, and it was discovered by a pioneering machine-learning approach called Graph Neural Networks. The fairly straightforward approach can also be used to find new drugs for other diseases like cancer, improve transportation, solve problems related to social networks, and many others. But what is a Graph Neural Network, and how is it different from the neural networks that have been around for more than 5 years? Let's find out! Imagine that you're Cupid, and... Oh! Hi Andy! Ok, so you've been tasked to find a suitable match for Andy to fall in love with. How would you approach this? First, we know Andy's age, gender, occupation, that he reads fiction, loves cycling, and he can't dance.. Well that's a bit harsh, how about 'learning to dance'. Lets place all these features about Andy into a list as such. Next, lets gather information about his social interactions - who does he hang out with most often and who are some friends of those friends. Alright, we can now begin to analyze this web of information to find Andy a suitable match. This web of data can actually be viewed as what we call a Graph in computer science. But what is a Graph, exactly? A graph is a structured way to represent data. It consists of nodes to represent entities like people, and edges to represent the relationships between these entities. Like in Andy\u2019s network, each node might also have a set of features which describe it. For edges, they could either be undirected - meaning two way, or directed - one way Extending from undirected to directed graphs is not difficult once we get the basic mechanisms. So lets stick with undirected edges for now. But, why Graph Neural Networks? If you\u2019re watching this, I\u2019m sure you\u2019ve heard of the expansive success that Neural Networks has had on many problems They are able to learn a variety of desired outputs from several types of inputs such as numbers like stock prices, words that we use everyday or even images like this low resolution '4' over here. So, why don\u2019t we just feed Andy\u2019s network through the all-powerful neural network and let it tell us his best match? Well\u2026 It turns out that neural networks have mostly been successful on more structured data types. For example, numbers belong in a series, words that we use everyday are found in sentences, and images? Well they actually come as a grid of pixels. But neural networks still struggled with more complex data such as graphs, which do not have any fixed structure or ordering. So, in order to reasons with graphs, researchers went back to the drawing board to study what made neural networks so successful. The first layer in a common Convolutional Neural Network takes each pixel and extracts information about its region by aggregating the pixel within the context of its neighborhood. The next layer then extracts information about that region within the context of its neighboring regions. Doing this over enough layers, the network will be able to reason over different parts of the whole image. And then with some linear computations, it can finally identify the relevant object. Take a second to think how might you apply this mechanism to graphs? If we look closer at an image, we can actually view it as a special type of graph - a grid graph that is structured, and every node is a pixel that is connected to its 8 neighboring pixels. Seeing it like this, you might think to yourself - why don\u2019t we just use the same aggregation mechanism that we have seen before? And you're right! We can actually use that same intuition on graphs! Relaxing the properties of fixed structure and ordering, we can similarly gain a better understanding of each node by aggregating the messages received from all neighboring nodes. Now, hold on to your new message, Andy, as your friends complete their first round of message passing. Observe how the color of each message changes to represent the newly aggregated message. And, with each new round of message passing, more and more information gets passed around in the graph. Over sufficient rounds of message passing, we will eventually obtain a final representation of each node in the graph that well describe it given the larger context. We can also view the rounds of message passing as a series of layers just like in our convolutional neural network. Here we go from the input, to round one of message passing, and then round two of message passing. And this produces our final understanding of nodes. So\u2026 We now know how to understand each node better, but how does this allow us to help Andy again? Well, as humans, we might take a look at Andy's graph, and mentally or visually, we will process all the information such that people who are more compatible with Andy will have a stronger association with him. In other words, in our mind, maybe people who have similar characteristics to Andy will be placed closer to him - and they are therefore stronger candidates. Similarly, in the process of reasoning over a graph, the graph neural network seeks to summarize all that information of each node into a numerical representation, where nodes which are more \u2018similar\u2019 will be closer to each other. We call this representation of nodes - node embeddings, and we call the space of all possible representations - you guessed it - the embedding space. But how does the network know how to come up with these embeddings, and where to place them in the embedding space? Neural networks need an appropriate measurable objective to guide the network into learning something useful for our task. In Andy's case, one way is to tell the network that embeddings of people who Andy spends "}], "Deep Learning Architectures": [{"https://www.youtube.com/watch?v=SZorAJ4I-sA": "[MUSIC PLAYING] DALE MARKOWITZ: The neat thing about working in machine learning is that every few years, somebody invents something crazy that makes you totally reconsider what's possible, like models that can play Go or generate hyper-realistic faces. And today, the mind-blowing discovery that's rocking everyone's world is a type of neural network called a transformer. Transformers are models that can translate text, write poems and op-eds, and even generate computer code. They could be used in biology to solve the protein folding problem. Transformers are like this magical machine learning hammer that seems to make every problem into a nail. If you've heard of the trendy new ML models BERT, or GPT-3, or T5, all of these models are based on transformers. So if you want to stay hip in machine learning and especially in natural language processing, you have to know about the transformer. So in this video, I'm going to tell you about what transformers are, how they work, and why they've been so impactful. Let's get to it. So what is a transformer? It's a type of neural network architecture. To recap, neural networks are a very effective type of model for analyzing complicated data types, like images, videos, audio, and text. But there are different types of neural networks optimized for different types of data. Like if you're analyzing images, you would typically use a convolutional neural network, which is designed to vaguely mimic the way that the human brain processes vision. And since around 2012, neural networks have been really good at solving vision tasks, like identifying objects in photos. But for a long time, we didn't have anything comparably good for analyzing language, whether for translation, or text summarization, or text generation. And this is a problem, because language is the primary way that humans communicate. You see, until transformers came around, the way we used deep learning to understand text was with a type of model called a Recurrent Neural Network, or an RNN, that looked something like this. Let's say you wanted to translate a sentence from English to French. An RNN would take as input an English sentence and process the words one at a time, and then sequentially spit out their French counterparts. The keyword here is sequential. In language, the order of words matters, and you can't just shuffle them around. For example, the sentence Jane went looking for trouble means something very different than the sentence Trouble went looking for Jane. So any model that's going to deal with language has to capture word order, and recurrent neural networks do this by looking at one word at a time sequentially. But RNNs had a lot of problems. First, they never really did well at handling large sequences of text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph, they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train. Because they process words sequentially, they couldn't paralellize well, which means that you couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow to train, you can't train it on all that much data. This is where the transformer changed everything. They're a model developed in 2017 by researchers at Google and the University of Toronto, and they were initially designed to do translation. But unlike recurrent neural networks, you could really efficiently paralellize transformers. And that meant that with the right hardware, you could train some really big models. How big? Really big. Remember GPT-3, that model that writes poetry and code, and has conversations? That was trained on almost 45 terabytes of text data, including almost the entire public web. [WHISTLES] So if you remember anything about transformers, let it be this. Combine a model that scales really well with a huge data set and the results will probably blow your mind. So how do these things actually work? From the diagram in the paper, it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are three main innovations that make this model work so well. Positional encodings and attention, and specifically, a type of attention called self-attention. Let's start by talking about the first one, positional encodings. Let's say we're trying to translate text from English to French. Positional encodings is the idea that instead of looking at words sequentially, you take each word in your sentence, and before you feed it into the neural network, you slap a number on it-- 1, 2, 3, depending on what number the word is in the sentence. In other words, you store information about word order in the data itself, rather than in the structure of the network. Then as you train the network on lots of text data, it learns how to interpret those positional encodings. In this way, the neural network learns the importance of word order from the data. This is a high level way to understand positional encodings, but it's an innovation that really helped make transformers easier to train than RNNs. The next innovation in this paper is a concept called attention, which you'll see used everywhere in machine learning these days. In fact, the title of the original transformer paper is \"Attention Is All You Need.\" So the agreement on the European economic area was signed in August 1992. Did you know that? That's the example sentence given in the original paper. And remember, the original transformer was designed for translation. Now imagine trying to translate that sentence to French. One bad way to translate text is to try to translate each word one for one. But in French, some words are flipped, like in the French translation, European comes before economic. Plus, French is a language that has gendered agreement between words. So the word [FRENCH] needs to be in the feminine form to match with [FRENCH]. The attention mechanism is a neural network structure "}, {"https://www.youtube.com/watch?v=aircAruvnKk": "This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math. My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around. But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests neural networks are inspired by the brain, but let's break that down. What are the neurons, and in what sense are they linked together? Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1. It's really not more than that. For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels. This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled. In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen. In practice there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. "}, {"https://www.youtube.com/watch?v=zbdong_h-x4": "BENOIT DHERIN: Hello, everybody. My name is Benoit Dherin. I'm a machine learning engineer at Google's Advanced Solutions Lab. If you want to know more about the Advanced Solutions Lab, please follow the link in the description box below. There is a lot of excitement currently around generative AI and new advancements including new Vertex AI features, such as Gen AI Studio, Model Garden, Gen AI API. Our objective in these short courses is to give you a solid footing on some of the underlying concepts that make all the gen AI magic possible. Today I'm going to talk about the encoder-decoder architecture, which is at the core of large language models. We will start with a brief overview of the architecture. Then I'll go over how we train these models. And at last, we will see how to produce text from a trained model at serving time. To begin with, the encoder-decoder architecture is a sequence-to-sequence architecture. This means that it takes, for example, a sequence of words as input, like the sentence in English, \"the cat ate the mouse,\" and it outputs the translation in French, \"le chat a mang\u00e9 la souris.\" The encoder-decoder architecture is a machine that consumes sequences and spits out sequences. Another input example is the sequence of words forming the prompt sent to a large language model. Then the output is the response of the large language model to this prompt. Now we know what an encoder-decoder architecture does. But how does it do it? Typically the encoder-decoder architecture has two stages, first an encoder stage that produces a vector representation of the input sentence. Then this encoder stage is followed by a decoder stage that creates the sequence output. Both the encoder and the decoder can be implemented with different internal architectures. The internal mechanism can be a recurrent neural network, as shown in this slide, or a more complex transformer block, as in the case of the super powerful language models we see nowadays. A recurrent neural network encoder takes each token in the input sequence one at a time and produces a state representing this token as well as all the previously ingested tokens. Then the state is used in the next encoding step as input along with the next token to produce the next state. Once you are done ingesting all the input tokens into the RNN, you output a vector that essentially represents the full input sentence. That's it for the encoder. What about the decoder part? The decoder takes the vector representation of the input sentence and produces an output sentence from that representation. In the case of an RNN decoder, it does it in steps, decoding the output one token at a time using the current state and what has been decoded so far. OK, now that we have a high-level understanding of the encoded-decoder architecture, how do we train it? That's the training phase. To train a model, you need a data set that is a collection of input/output pairs that you want your model to imitate. You can then feed that data set to the model, which will correct its own weights during training on the basis of the error it produces on a given input in the data set. This error is essentially the difference between what the neural networks generates given an input sequence and the true output sequence you have in the data set. But then how do you produce this data set? In the case of the encoder-decoder architecture this is a bit more complicated than for typical predictive models. First you need a collection of input and output texts. In the case of translation, that would be sentence pairs where one sentence is in the source language while the other is the translation. You'd feed the source language sentence to the encoder, and then compute the error between what the decoder generates and the actual translation. However, there is a catch. The decoder also needs its own input at training time. You need to give the decoder the correct previous translated token as input to generate the next token, rather than what the decoder has generated so far. This method of training is called teacher forcing because you force the decoder to generate the next token from the correct previous token. This means that in your code you have to prepare two input sentences-- the original one fed to the encoder and also the original one shifted to the left that you'll feed to the decoder. Another subtle point is that the decoder generates at each step only the probability that each token in your vocabulary is the next one. Using these probabilities you have to select a word, and there are several approaches for that. The simplest one called Grid Search is to generate the token that has the highest probability. A better approach that produces better results is called Beam Search. In that case, you use the probabilities generated by the decoder to evaluate the probability of sentence chunks, rather than individual words. And you keep at each step the most likely generated chunk. That's how training is done. Now let's move on to serving. After training at serving time, when you want to, say, generate a new translation or a new response to a prompt, you'll start by feeding the encoder representation of the prompt to the decoder along with a special token like go. This will prompt the decoder to generate the first word. Let's see in more details what happens during the generation stage. First of all, the start token needs to be represented by a vector using an embedding layer. Then the recurrent layer will update the previous state produced by the encoder into a new state. This state will be passed to a dense softmax layer to produce the word probabilities. Finally, the word is generating by taking the highest probability word with Greddy Search or the highest probability chunk with Beam Search. "}, {"https://www.youtube.com/watch?v=aR9orp7QLBA": "neural networks have gained immense popularity in recent years and are being used in a variety of applications, ranging from image and speech recognition to natural language processing and game playing with the growing number of neural network architectures available, it can be challenging to choose the right neural network for a particular task. that's why, in this video, we will discuss some tips that can help you choose a neural network that fits your needs 1. determine the type of data you are working with the type of data you are working with plays a significant role in determining the neural network architecture you should choose. for example, if you are working with image data, a convolutional neural network (cnn) is an appropriate choice. cnns are specifically designed to process image data and are excellent at identifying patterns in images. on the other hand, if you are working with text data, a recurrent neural network (rnn) or a transformer network might be a better choice. as rnns are specifically designed for sequential data and can process text in a way that preserves the order of words. and on the other hand, transformer networks are great at processing large amounts of text data and have been used in natural language processing applications. 2. consider the complexity of the task the complexity of the task at hand is another critical factor to consider when choosing a neural network. if the task is relatively simple, a shallow neural network with only a few layers might be sufficient. however, for more complex tasks, a deeper neural network with more layers might be required. for example, in image recognition tasks, deeper cnns with multiple layers are often used. here, these networks can identify increasingly complex features in an image as they move through the layers, ultimately leading to more accurate predictions. 3. determine the availability of labeled data the availability of labeled data is another important factor to consider when choosing a neural network. supervised learning, which requires labeled data, is the most common type of machine learning used in neural networks. however, if labeled data is not available, unsupervised learning methods can be used instead. for example, if you are working with image data but do not have access to labeled data, a generative adversarial network (gan) might be a good choice. gans use unsupervised learning to generate new data that is similar to the training data. 4. consider the amount of training data the amount of training data you have available also plays a role in choosing a neural network. generally, larger datasets require more complex neural networks with more parameters. however, if you have a relatively small amount of training data, you might want to use a simpler neural network to avoid overfitting. for example, if we want to classify images of cats and dogs, a large dataset with diverse images is needed for effective training. but if we have a small dataset, then we may have to use transfer learning or data augmentation techniques. also, on the contrary, by using large datasets we can build complex neural network architectures with more deep layers for better performance, like resnet or inception. 5. think about the need for transfer learning transfer learning is the process of using a pre-trained neural network to solve a new task. pre-trained models can be used to speed up the training process and improve the accuracy of the model. for example, if you are working on an image classification task, you can use a pre-trained cnn like resnet or vgg to extract features from the images. you can then use these features as input to a new neural network that is specifically designed for your task. 6. evaluate the importance of sequential data sequential data is data that has a temporal or sequential order, such as audio, video, or text. if you are working with sequential data, you will need to choose a neural network architecture that can handle this type of data. two common types of neural networks that can handle sequential data are recurrent neural networks (rnns) and convolutional neural networks (cnns). rnns are specifically designed for processing sequential data and have the ability to maintain a memory of previous inputs. this makes them a good choice for tasks like language modeling, speech recognition, and natural language processing. cnns, on the other hand, are typically used for image data, but they can also be adapted to handle sequential data by using 1d convolutional filters. for example, in speech recognition tasks, rnns are often used because they can process the audio data sequentially, maintaining a memory of previous inputs as they go. in natural language processing tasks, like sentiment analysis or machine translation, rnns are also commonly used because they can process text data in a way that preserves the order of the words. 7. consider the importance of layers the number and types of layers in a neural network architecture can significantly affect its performance. generally, deeper neural networks with more layers tend to perform better on more complex tasks, but they also require more training data and longer training times. shallow neural networks with fewer layers are often faster to train, but they may not perform as well on more complex tasks. it's also important to consider the types of layers you include in your neural network architecture. for example, in image recognition tasks, convolutional layers are commonly used to extract features from the image data. in natural language processing tasks, embedding layers are used to transform text data into a numerical format that can be processed by the neural network. 8. look at existing models and benchmarks when choosing a neural network architecture, it's important to look at existing models and benchmarks for the task you are working on. this can give you a good idea of what neural network architectures are commonly used for similar tasks and how they perform. for example, if you are working on an image classification task, you can look at existing models like resnet or vgg and see how they perform on "}, {"https://www.youtube.com/watch?v=ZXiruGOCn9s": "no it's it it's not those transformers but but they can do some pretty cool things let me show you so why did the banana cross the road because it was sick of being mashed yeah i'm not sure that i quite get that one and that's because it was created by a computer i literally asked it to tell me a joke and this is what it came up with specifically i used a gpt-3 or a generative pre-trained transformer model the three here means that this is the third generation gpt-3 is an auto-regressive language model that produces text that looks like it was written by a human gpt3 can write poetry craft emails and evidently come up with its own jokes off you go now while our banana joke isn't exactly funny it does fit the typical pattern of a joke with a setup and a punch line and sort of kind of makes sense i mean who wouldn't cross the road to avoid getting mashed but look gpt3 is just one example of a transformer something that transforms from one sequence into another and language translation is just a great example perhaps we want to take a sentence of why did the banana cross the road and we want to take that english phrase and translate it into french well transformers consist of two parts there is an encoder and there is a decoder the encoder works on the input sequence and the decoder operates on the target output sequence now on the face of it translation seems like little more than just like a basic lookup task so convert the y here of our english sentence to the french equivalent of porcua but of course language translation doesn't really work that way things like word order in terms of phrase often mix things up and the way transformers work is through sequence to sequence learning where the transformer takes a sequence of tokens in this case words in a sentence and predicts the next word in the output sequence it does this through iterating through encoder layers so the encoder generates encodings that define which part of the input sequence are relevant to each other and then passes these encodings to the next encoder layer the decoder takes all of these encodings and uses their derived context to generate the output sequence now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean that they are pre-trained in an unsupervised manner with a large unlabeled data set and then they're fine-tuned through supervised training to get them to perform better now in previous videos i've talked about other machine learning algorithms that handle sequential input like natural language for example there are recurrent neural networks or rnns what makes transformers a little bit different is they do not necessarily process data in order transformers use something called an attention mechanism and this provides context around items in the input sequence so rather than starting our translation with the word why because it's at the start of the sentence the transformer attempts to identify the context that bring meaning in each word in the sequence and it's this attention mechanism that gives transformers a huge leg up over algorithms like rnn that must run in sequence transformers run multiple sequences in parallel and this vastly speeds up training times so beyond translations what can transformers be applied to well document summaries they're another great example you can like feed in a whole article as the input sequence and then generate an output sequence that's going to really just be a couple of sentences that summarize the main points transformers can create whole new documents of their own for example like write a whole blog post and beyond just language transformers have done things like learn to play chess and perform image processing that even rivals the capabilities of convolutional neural networks look transformers are a powerful deep learning model and thanks to how the attention mechanism can be paralyzed are getting better all the time and who knows pretty soon maybe they'll even be able to pull off banana jokes that are actually funny if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching "}], "Training Deep Learning Models": [{"https://www.youtube.com/watch?v=0JEb7knenps": "[Music] hi I'm Nicola Ravi I'm an engineer on Facebook's AI team working on computer vision and 3d understanding my team and I built an open source library called pi torch 3d to make 3d d planning easier with pi torch it contains reusable components that can be useful for a wide variety of 3d tasks and that are optimized thoroughly tested and well documented one of the major limitations for advancing 3d understanding has been the lack of sufficient engineering tools to support the complexities of combining deep learning or 3d data unlike 2d images that can be represented as a single tensor 3d models require us to support operations on batches of complex 3d data like triangle meshes which are composed of both vertex XYZ coordinates as well as space connection patterns this can present both memory and compute challenges this includes operators and loss functions for 3d data such as Tantalus and meshes a data structure with methods for manipulating batches of variable size measures for example calculating the chamfer loss between two meshes requires two intensive operations first we need to differentially sample excessive points from the surface of each mesh and second for each point we need to find the closest neighboring points in the other point cloud this can be a memory intensive computation in pi torch as we need to calculate a pairwise distance matrix and then find the minimum value in comparison a fused CUDA kernel enables significant speed up and reduced memory usage using graphics operators such as rendering where deep learning is an exciting research area we created a modular differentiable renderer implementation which can be easily incorporated into a deep learning training it has composable units to enable researchers to access a wide variety of intermediate variables and extend it to support custom lighting or shading effects [Music] accurately predicting and reconstructing the 3d shapes of unconstrained scenes in the real world is an important step towards unlocking and enhancing new experiences in several domains in the future we could enable AR and VR applications such as virtual Tryon for clothing or visualize furniture and enhance robotic navigation by improving understanding of a 3d scene and objects within it our goal with Pytor 3d is to help accelerate research at the intersection of 3d and deep learning and reduce the startup time needed to work on 3d research and by open sourcing the code we hope that the wider AI community will leverage Pytor 3d to help press 3d research forward we welcome contributions from the community to build this resource father feel free to try it yourself a pilot or 3G org or on github you "}, {"https://www.youtube.com/watch?v=aircAruvnKk": "This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math. My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around. But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests neural networks are inspired by the brain, but let's break that down. What are the neurons, and in what sense are they linked together? Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1. It's really not more than that. For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels. This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled. In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen. In practice there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. "}, {"https://www.youtube.com/watch?v=IHZwWFHWa-w": "Last video I laid out the structure of a neural network. I'll give a quick recap here so that it's fresh in our minds, and then I have two main goals for this video. The first is to introduce the idea of gradient descent, which underlies not only how neural networks learn, but how a lot of other machine learning works as well. Then after that we'll dig in a little more into how this particular network performs, and what those hidden layers of neurons end up looking for. As a reminder, our goal here is the classic example of handwritten digit recognition, the hello world of neural networks. These digits are rendered on a 28x28 pixel grid, each pixel with some grayscale value between 0 and 1. Those are what determine the activations of 784 neurons in the input layer of the network. And then the activation for each neuron in the following layers is based on a weighted sum of all the activations in the previous layer, plus some special number called a bias. Then you compose that sum with some other function, like the sigmoid squishification, or a relu, the way I walked through last video. In total, given the somewhat arbitrary choice of two hidden layers here with 16 neurons each, the network has about 13,000 weights and biases that we can adjust, and it's these values that determine what exactly the network actually does. Then what we mean when we say that this network classifies a given digit is that the brightest of those 10 neurons in the final layer corresponds to that digit. And remember, the motivation we had in mind here for the layered structure was that maybe the second layer could pick up on the edges, and the third layer might pick up on patterns like loops and lines, and the last one could just piece together those patterns to recognize digits. So here, we learn how the network learns. What we want is an algorithm where you can show this network a whole bunch of training data, which comes in the form of a bunch of different images of handwritten digits, along with labels for what they're supposed to be, and it'll adjust those 13,000 weights and biases so as to improve its performance on the training data. Hopefully, this layered structure will mean that what it learns generalizes to images beyond that training data. The way we test that is that after you train the network, you show it more labeled data that it's never seen before, and you see how accurately it classifies those new images. Fortunately for us, and what makes this such a common example to start with, is that the good people behind the MNIST database have put together a collection of tens of thousands of handwritten digit images, each one labeled with the numbers they're supposed to be. And as provocative as it is to describe a machine as learning, once you see how it works, it feels a lot less like some crazy sci-fi premise, and a lot more like a calculus exercise. I mean, basically it comes down to finding the minimum of a certain function. Remember, conceptually, we're thinking of each neuron as being connected to all the neurons in the previous layer, and the weights in the weighted sum defining its activation are kind of like the strengths of those connections, and the bias is some indication of whether that neuron tends to be active or inactive. And to start things off, we're just going to initialize all of those weights and biases totally randomly. Needless to say, this network is going to perform pretty horribly on a given training example, since it's just doing something random. For example, you feed in this image of a 3, and the output layer just looks like a mess. So what you do is define a cost function, a way of telling the computer, no, bad computer, that output should have activations which are 0 for most neurons, but 1 for this neuron, what you gave me is utter trash. To say that a little more mathematically, you add up the squares of the differences between each of those trash output activations and the value you want them to have, and this is what we'll call the cost of a single training example. Notice this sum is small when the network confidently classifies the image correctly, but it's large when the network seems like it doesn't know what it's doing. So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal. This average cost is our measure for how lousy the network is, and how bad the computer should feel. And that's a complicated thing. Remember how the network itself was basically a function, one that takes in 784 numbers as inputs, the pixel values, and spits out 10 numbers as its output, and in a sense it's parameterized by all these weights and biases? Well the cost function is a layer of complexity on top of that. It takes as its input those 13,000 or so weights and biases, and spits out a single number describing how bad those weights and biases are, and the way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data. That's a lot to think about. But just telling the computer what a crappy job it's doing isn't very helpful. You want to tell it how to change those weights and biases so that it gets better. To make it easier, rather than struggling to imagine a function with 13,000 inputs, just imagine a simple function that has one number as an input and one number as an output. How do you find an input that minimizes the value of this function? Calculus students will know that you can sometimes figure out that minimum explicitly, but that's not always feasible for really complicated functions, "}, {"https://www.youtube.com/watch?v=ZXiruGOCn9s": "no it's it it's not those transformers but but they can do some pretty cool things let me show you so why did the banana cross the road because it was sick of being mashed yeah i'm not sure that i quite get that one and that's because it was created by a computer i literally asked it to tell me a joke and this is what it came up with specifically i used a gpt-3 or a generative pre-trained transformer model the three here means that this is the third generation gpt-3 is an auto-regressive language model that produces text that looks like it was written by a human gpt3 can write poetry craft emails and evidently come up with its own jokes off you go now while our banana joke isn't exactly funny it does fit the typical pattern of a joke with a setup and a punch line and sort of kind of makes sense i mean who wouldn't cross the road to avoid getting mashed but look gpt3 is just one example of a transformer something that transforms from one sequence into another and language translation is just a great example perhaps we want to take a sentence of why did the banana cross the road and we want to take that english phrase and translate it into french well transformers consist of two parts there is an encoder and there is a decoder the encoder works on the input sequence and the decoder operates on the target output sequence now on the face of it translation seems like little more than just like a basic lookup task so convert the y here of our english sentence to the french equivalent of porcua but of course language translation doesn't really work that way things like word order in terms of phrase often mix things up and the way transformers work is through sequence to sequence learning where the transformer takes a sequence of tokens in this case words in a sentence and predicts the next word in the output sequence it does this through iterating through encoder layers so the encoder generates encodings that define which part of the input sequence are relevant to each other and then passes these encodings to the next encoder layer the decoder takes all of these encodings and uses their derived context to generate the output sequence now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean that they are pre-trained in an unsupervised manner with a large unlabeled data set and then they're fine-tuned through supervised training to get them to perform better now in previous videos i've talked about other machine learning algorithms that handle sequential input like natural language for example there are recurrent neural networks or rnns what makes transformers a little bit different is they do not necessarily process data in order transformers use something called an attention mechanism and this provides context around items in the input sequence so rather than starting our translation with the word why because it's at the start of the sentence the transformer attempts to identify the context that bring meaning in each word in the sequence and it's this attention mechanism that gives transformers a huge leg up over algorithms like rnn that must run in sequence transformers run multiple sequences in parallel and this vastly speeds up training times so beyond translations what can transformers be applied to well document summaries they're another great example you can like feed in a whole article as the input sequence and then generate an output sequence that's going to really just be a couple of sentences that summarize the main points transformers can create whole new documents of their own for example like write a whole blog post and beyond just language transformers have done things like learn to play chess and perform image processing that even rivals the capabilities of convolutional neural networks look transformers are a powerful deep learning model and thanks to how the attention mechanism can be paralyzed are getting better all the time and who knows pretty soon maybe they'll even be able to pull off banana jokes that are actually funny if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching "}, {"https://www.youtube.com/watch?v=IzcX9bTJLj8": "machine learning models are what make ai effective in the enterprise but there's a science to building the right model [Music] many organizations are using ai for a wide range of business applications but ai is not a one size fits all technology every ai project is customized to solve a specific business problem with machine learning models these models which rely on data and algorithms are what address the project's needs for many organizations machine learning model development is a new and daunting activity but some established methodologies help ensure success in this video we break down the process of building machine learning model into seven steps after watching which step is most intimidating which do you still have questions about let us know in the comments and hit that like button too understand and identify the business problem and define success get a firm grasp of the business project's objectives and requirements and set specific quantifiable goals include machine learning metrics and business kpis in calculating your estimated roi from the project understand and identify data a machine learning model is built by learning from training data and then applying that knowledge to make accurate predictions from new data so identifying data needs the quantity quality and type is critical knowing how your model will operate on real world data offline or in real time for example will also play into the kind of data you need collect and prepare data once you've identified the data you need you must shape it to train the model data preparation tasks include data collection cleansing aggregation labeling and transformation among others this step can take up to 80 percent of the entire project but the time spent on prepping and cleansing is well worth it to get an effective model read more about the data preparation process by clicking the link above or in the description below determine the model's features in train the model this phase requires model technique selection training hyper parameter setting validation development and testing algorithm selection and model optimization evaluate model performance model evaluation is the qa of machine learning adequately evaluating model performance against metrics and requirements determines how the model will work in the real world evaluation includes model metric evaluation confusion matrix calculations kpis model performance metrics model quality measurements and a final determination of whether the model can meet the established business goals experiment and adjust the model in operation seeing how the model works in the real world or operationalizing the model might include scenarios in a cloud environment at the edge in an on-premise or closed environment or within a controlled group and finally repeat the process and make improvements always iterate and reflect on what has worked in your model what needs work and what's a work in progress [Music] you "}], "Deep Learning Project: Image Classification": [{"https://www.youtube.com/watch?v=ZTCoMo_gDYo": ""}, {"https://www.youtube.com/watch?v=il8dMDlXrIE": "hey my name is Felipe and welcome to my channel in this video we are going to make an image classifier. Image classification involves classifying an image into different categories and is one of the most important fields in computer vision I'm going to show you how to make an image classifier which is very simple, very robust and it works 100% in Python, I'm going to walk you step by step through the entire process how to prepare the data how to train the classifier and how to test its performance so following the steps of this tutorial you will be able to build a very robust image classifier in only a few minutes so let's get started so this is the process in which we are going to be working today in today's tutorial you can see that this is a four steps process so training this image classifier will take only four steps this will be a very easy and a very straightforward process now let me show you the requirements we are going to use in this project we are going to use scikit learn, scikit image and numpy and the image classifier we're going to use comes from the library scikit learn this is the library which is very popular to solve machine learning related problems and if you want to work with computer vision or with machine learning you will definitely need to be familiar with scikit learn so this will be a very good example this will be a very good opportunity in order to get more familiar with scikit learn now let's start with this project let's start with this tutorial and the first step is preparing the data we are going to use in order to train this image classifier and now let me show you the data we are going to use in today's tutorial this is the data we are going to use you can see we have two different categories the categories are empty or not empty this sounds very strange but let me show you exactly how these two categories look like or how the data in each one of these categories look like and you can see from the not empty category we have something that looks like this so basically we have cars these are images from cars and if I show you... if I show you many many different pictures you can see that in all of them we can see pretty much the same situation it's like a car that's basically the not empty category and then if I show you the empty category this is pretty much an empty something this is pretty much all the images containing empty...ness containing a completely empty... these are completely empty images in some of them there are very small objects but you can see that all of the images are mostly empty and these are... this is how these two categories look like this is how these two categories look like and this data comes from one of my previous videos where I showed you how to create a parking slot detector and counter using python and computer vision in this other video I showed you how to take a video exactly like this... a video from a huge parking lot like this and to make it look like this let me execute the code I showed you how to build in this tutorial and the idea was to go from here to here the idea was to build something like this where we detect absolutely all the parking slots and for each one of the parking slots we classify if the parking slot is empty or not and if it's empty we plot it in green and if it's not empty we plot it in red so this is exactly where the data comes from the data we are going to use in today's tutorial you can see that for the not empty category this is how the data looks like these are images from parking slots containing cars and from the empty category we have images from parking slots which are completely and absolutely empty so this is the data we are going to use today and the image classifier I'm going to show you how to build in today's tutorial it's a very robust classifier in situations like this in situations where the data we have in situations where the classification we want to make on our data it's very easy it's very simple right where the different categories we want to tell apart the different categories we want to classify they are visually super super super distinct they are super different right in this case we have images from cars and in this other category we have absolutely empty images right, the image classifier I'm going to show you today it's very robust and it works super super well in situations like this and the reason I'm saying this is because this may not be a state-of-the-art image classifier if you look for papers or if you look for the most recent techniques obviously you are not going to find the classifier we are going to make today we're going to build today... and it's definitely not the most robust classifier in the machine learning industry but if you want to solve like a very very simple problem like the one we are going to solve today this image specifier will be more than enough and I'm going to show you exactly what's the performance we achieve with this classifier but that's later on in this tutorial now let's get started I already showed you the data we are going to use and the first thing I'm going to do is to import os which is a library which we definitely need in order to work with data and to read data and to load data from our computer so I'm going to define a directory "}, {"https://www.youtube.com/watch?v=taC5pMCm70U": "In computer vision, image classification is not the only problem that we try to solve. There are two other problems that we solve using computer vision, which is object detection and image segmentation. In this short video, I will explain you the difference between these three using a simple example. For image classification you try to classify entire image as one of the classes, for example is this a dog or not. Image classification with localization means you not only classify the image as one of the class but you find the location of that object within the image. So this is called image classification with localization. If you have multiple objects in your image such as cat and dog, you might want to find their location and you can try drawing these bounding boxes or rectangles specifying the location of these classes. This is called object detection. In the image, you're trying to detect an object, which in this case is cat and dog and you're drawing these bounding boxes or rectangles, so this is object detection. You can go one more level up and classify each pixel as one of the classes. For example, here all the red pixels belongs to dog and yellow pixels belongs to cat. This is more fine grained classification where you're exactly specifying each pixel belonging to each class. So just to summarize when you classify the entire image as one of the classes is called Image Classification. When you detect the objects within an image with rectangular bounding boxes, it's called Object Detection; and when you classify each of the pixels as one of the classes it is called Image Segmentation. Image Segmentation is little more involved, I just gave a very simple definition of it. The the image segmentation which, I showed you is called instance segmentation. There is semantic segmentation as well but for this video just to keep things simple you can leave this, with this understanding, and I hope you like this video and I hope you are enjoying this deep learning series. In the furthest further videos we'll go more into object detection image segmentation and we'll try to write python code using tensorflow. Thank you for watching! "}, {"https://www.youtube.com/watch?v=kwcillcWOg0": "[WOODEN TRAIN WHISTLE] Hello, and welcome to a very special episode of the Coding Train. Today, I am so excited to demonstrate something to you. I am going to show you a project from Google Creative Lab called Teachable Machine. Now, you might say, Ha-ha! I have heard of this Teachable Machine before, in fact you referenced it in some other videos you've made, that are floating over here right now. And in fact, I have, and in fact, I've talked a lot about the kind of thing that Teachable Machine does called Transfer Learning. But this video, I'm going to show you something that just launched today! Which is Teachable Machine 2.0. And Teachable Machine 2.0 allows you to train a machine learning model in the browser, you can train it on images, you can trade it on sounds, you can trade it on poses! And more to come in the future. And then you can save the model you trained and use it in your own project. So I'm going to make a few examples, images and sounds, train a model, demonstrate that, and then bring the train model into my P5 JS sketch with the ML5 JS library, and have some fun making a goofy project and giving you some starter code for you to make your own project. Let's get started by training an image classification model. And what do I need to create an image classification model? Props! I've brought a lot of props with me. I think this will make it fun. I should point out that a bunch of these things have green in them, and you can't see the color green, because I'm standing in front of a green screen. But if I show you the green screen, you can see the color green. To get started from Select a Project, I'm going to choose Image Project, because I want to start with images. Here, there are three steps, and I'm going to go through all of them, but the first step here is the data collection step. So what do I want? I want to collect a whole bunch of images of something, and provide a label for those images. Teachable Machine is going to assume you're at least going to have two kinds of images, otherwise why are you classifying images if you don't at least have two different kinds to distinguish between? And it's going to create automatic labels, like Class 1, or Classification 1. But it's much more fun to pick our own names, so I'm going to click here and switch this to unicorn. And we'll give it some images of this unicorn. So I'm gonna click Add Samples, Webcam, and get my unicorn placed nicely in front of the camera. So there we go, I've given it 413 example images of a unicorn. One thing I would like to mention-- in my excitement and enthusiasm for demonstrating this project, I collected a lot of images-- 400 plus for some of the categories-- and that's really quite unnecessary. It makes the training time take quite a bit longer, and with this transfer learning process that I've described, I could probably get the same exact performance and accuracy with many fewer images. So if you're following along, collect fewer images. The training will happen faster, it will be easier, then you could try it again with more images, with less images, just to experiment and see what works and what doesn't. But probably for getting started, I might start with, say, 25 to 50 images per category. Next of course is a train whistle. We'll call Class 2 \"Train\". Add samples. Now I definitely want to move on and show the ukulele and the rainbow as well, but let's at least see-- we've done two, let's see if it works. The next step is for me to click Train Model and actually train the model. So what happens when I press train the model? What is a model? What's the training process? What's going on? There is so much to say here. And let me give you a brief high level overview. Teachable Machine is using a technique called transfer learning. I've actually made several videos all about transfer learning with ML5 JS, and if you want, you could go back and look at those for a bit more of a deeper dive in how transfer learning works. But the short explanation is, there is a pre-trained model. Somebody else already trained a model on many, many images, and this model is called MobileNet. And there's a paper written about it that you could read. And the training data for this model is called ImageNet, and there's a paper about that that you could read. And you could do a lot of research into what is this base-- what is this foundation model upon which we're going to add our images? MobileNet is a model that runs fast, works in the browser, and knows about 1,000 different kinds of things. But it doesn't know about unicorns or train whistles or rainbows, necessarily. Maybe it's in there, maybe it's not. But what we could do is take the fact that it's learned how to boil the essence of images down into a bunch of different numbers, to then retrain it with our own images. And this allows us to get something that works with just 400 images of unicorns and 400 images of train whistles. Because if you were training an image classification model from scratch without this base model, you probably would need a much larger data set. So now that I've given you that brief explanation of how it works, we can actually press Train Model and watch it go. So one thing to note is, it's telling me \"don't switch tabs\". Don't switch tabs! The reason why it tells you not to switch tabs-- and this is very important-- is it's training the model in here, in the browser. "}, {"https://www.youtube.com/watch?v=zjlYu7Ztxfs": "hi I'm Kobayashi from Sony. In this video, I'll explain how to easily create an image classification project using the Neural Network Console for Windows, an integrated development environment for Deep Learning. If you'd like to learn what Deep Learning is, or if you'd like to learn how to use the cloud version, please refer to the links in the description box of this video. So let's create a project for image classification. In the Neural Network Console, when you create a new Neural Network, the first thing you do is create a new project. When you create a new program in most integrated development environments, you will probably create a new project. In the Neural Network Console as well, each time you create a Neural Network, you create a new project. In the Windows version, you can create a new project by clicking this \"New Project\" button. If you click here, a blank project will be created like this. Once you have created a new project, the first thing to do is to specify the dataset. Click on the \"DATASET\" tab here to specify the data to be used for training. To load a new dataset, click on the \"Open dataset\" button in this URI. Here you can see the datasets that are available as four samples. This time, we will run the training using this sample dataset. We'll show you how to prepare your own dataset in a future video. This time, we're going to load the \"small_mnist_4or9_training\" data for training. click here to load it like this. As you can see, the x:image has images of 4 or 9 loaded like this. And Y has numbers of 0 or 1 loaded into it. The dataset loaded here is the one introduced in the video that runs the sample project. If you can't load the dataset here, you can run the sample project in that video and you can load the dataset like this. This sample dataset is for identifying whether this input image is a 4 or a 9. It is labeled 0 for a 4 and 1 for a 9, so it is a dataset to create a classifier that can tell if it is a 9 or not. What has been loaded is a dataset for training. 1,500 images are included in this dataset. Using these 1,500 samples of data, we will build an image classifier to identify between images of 4 and 9. When training the Neural Network, in addition to the dataset used for training, we specify the Validation Dataset to check the accuracy of the trained Neural Network. Here we have two datasets, \"Training\" and \"Validation\". If you click on \"Validation\", you will see the \"Validation\" dataset. Earlier, we loaded 1,500 samples of data for training, but in the same way, we are going to load 500 samples of data for evaluation. This time, we will load the dataset \"small_mnist_4or9_test\". Now, \"small_mnist_4or9_test\" has been loaded. I think it's hard to tell them apart because they're similar to the dataset we loaded for training. However, if you compare it with Training, you will see that it loads other images of 4 and 9 that are not in training. 500 images have been loaded for evaluation. The dataset is now loaded. Next, we will design the Neural Network. To design the Neural Network, use the \"EDIT\" tab here. In the \"EDIT\" tab, you can find various functions in the \"Components\" section, and you can combine these functions to design the Neural Network. The Neural Network to be designed this time will be a single-layer Logistic Regression, which was also present in the sample project Logistic Regression. Logistic Regression can be composed of these four functions: Input, Affine, Sigmoid, and Binary Cross Entropy. If you want to know how to construct a Neural Network by combining functions, lease see the video \"Basics of Designing Neural Network \" for a detailed explanation. Now, let's design the Neural Network. This time, we will use Input, Affine, Sigmoid, and Binary Cross Entropy, so we need to find these functions and drop them into the \"Main\" area. First, add an \"Input\" layer to the Computational Graph by dragging and dropping it like this. Next, we will use an \"Affine\" layer. Add an \"Affine\" layer like this. Then add \"Sigmoid\", and finally \"BinaryCrossEntropy\". Now the functions have been added.These functions are not yet connected at the moment. To connect, you can drag and drop the output pin of this layer to the input pin of the next layer like this. In addition to the method introduced now, you can drag and drop the next function --------------------------------------- 0:05:15.380,0:05:19.310 just below the function you are connecting from. If you place the functions like this, they will be automatically connected.We'd rather go with this one, but we can see that the Neural Network Console combines the features of two UI's, the user interface of the graphand the interface for connecting blocks. Either way, I hope you will use the method that is intuitive to you. This time, I added Input, Affine, Sigmoid, and Binary Cross Entropy by dragging and dropping, but the easiest way is to add by double-clicking. Like this, double-clicking on \"Input\", \"Affine\", \"Sigmoid\", or \"BinaryCross Entropy\" will add a new layer just below the --------------------------------------------- 0:05:59.760,0:06:06.900 previously selected layer. Also, there are about 100 of these functions, so if you have trouble finding them, you can narrow down your search by entering the text here. For example, if you want to search for a Sigmoid, you can enter \"Sigmoid\" here like this, and the functions such as \"Sigmoid\" function of \"Activation\" will be displayed here as a result of the narrowed search. The necessary functions, \"Input\", \"Affine\", \"Sigmoid\", and \"BinaryCrossEntropy\", are now implemented. Next, we will edit the properties of the area. Our goal now is to input the images of 4 or 9 and determine if it is a 9 or not. So, the number of output neurons required here "}]}