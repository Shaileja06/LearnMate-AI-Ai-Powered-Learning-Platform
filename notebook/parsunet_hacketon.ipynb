{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "7G1ROzVJh0XJ",
        "outputId": "ac54ca85-dc34-4e11-d4f8-4bcf2115ebb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-1.0.7-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting youtube_search\n",
            "  Downloading youtube_search-2.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
            "  Downloading langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting google-generativeai<0.8.0,>=0.7.0 (from langchain-google-genai)\n",
            "  Downloading google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting google-ai-generativelanguage==0.6.6 (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting google-api-core (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-api-python-client (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading google_api_python_client-2.135.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.30.0)\n",
            "Requirement already satisfied: protobuf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.23.4)\n",
            "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.12.2)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.4.0)\n",
            "Collecting distro<2,>=1.7.0 (from groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.1)\n",
            "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
            "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.1)\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-1.0.7-py3-none-any.whl (36 kB)\n",
            "Downloading langchain_groq-0.1.6-py3-none-any.whl (14 kB)\n",
            "Downloading youtube_search-2.1.2-py3-none-any.whl (3.4 kB)\n",
            "Downloading google_generativeai-0.7.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
            "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading google_api_python_client-2.135.0-py2.py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading grpcio_status-1.62.2-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=ddddb9d6dd908dd9d201bbf2ec40e79e99884593046403eee0b8cc549201bd2e\n",
            "  Stored in directory: /home/zeus/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: uritemplate, tenacity, proto-plus, jsonpatch, httplib2, greenlet, googleapis-common-protos, distro, youtube_search, SQLAlchemy, grpcio-status, google-search-results, langsmith, groq, google-auth-httplib2, google-api-core, langchain-core, google-api-python-client, langchain-text-splitters, langchain_groq, google-ai-generativelanguage, langchain, google-generativeai, langchain-google-genai\n",
            "Successfully installed SQLAlchemy-2.0.31 distro-1.9.0 google-ai-generativelanguage-0.6.6 google-api-core-2.19.1 google-api-python-client-2.135.0 google-auth-httplib2-0.2.0 google-generativeai-0.7.1 google-search-results-2.4.2 googleapis-common-protos-1.63.2 greenlet-3.0.3 groq-0.9.0 grpcio-status-1.62.2 httplib2-0.22.0 jsonpatch-1.33 langchain-0.2.6 langchain-core-0.2.10 langchain-google-genai-1.0.7 langchain-text-splitters-0.2.2 langchain_groq-0.1.6 langsmith-0.1.82 proto-plus-1.24.0 tenacity-8.4.2 uritemplate-4.1.1 youtube_search-2.1.2\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.3.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.35.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting crewai_tools\n",
            "  Downloading crewai_tools-0.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (0.2.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (0.2.10)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-community) (8.4.2)\n",
            "Collecting tiktoken<1,>=0.5.2 (from tavily-python)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from crewai) (8.1.7)\n",
            "Collecting embedchain==0.1.109 (from crewai)\n",
            "  Downloading embedchain-0.1.109-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting instructor==1.3.3 (from crewai)\n",
            "  Downloading instructor-1.3.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "INFO: pip is looking at multiple versions of crewai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.32.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.32.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.32.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.30.11-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting embedchain<0.2.0,>=0.1.98 (from crewai)\n",
            "  Downloading embedchain-0.1.113-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting instructor<0.6.0,>=0.5.2 (from crewai)\n",
            "  Downloading instructor-0.5.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.30.10-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.30.8-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.30.5-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: pip is still looking at multiple versions of crewai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading crewai-0.30.4-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.8-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.7-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.6-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading crewai-0.28.5-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.4-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.3-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.27.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.22.5-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.22.4-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.22.3-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.22.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.22.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.19.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.17.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.16.3-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.16.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.16.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.16.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.14.4-py3-none-any.whl.metadata (13 kB)\n",
            "  Downloading crewai-0.14.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting crewai_tools\n",
            "  Downloading crewai_tools-0.0.6-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.14.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading crewai-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading crewai-0.11.2-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading crewai-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading crewai-0.11.0-py3-none-any.whl.metadata (12 kB)\n",
            "  Downloading crewai-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading crewai-0.5.5-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading crewai-0.5.3-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading crewai-0.5.2-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading crewai-0.5.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading crewai-0.1.32-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading crewai-0.1.24-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading crewai-0.1.23-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading crewai-0.1.17-py3-none-any.whl.metadata (9.6 kB)\n",
            "  Downloading crewai-0.1.16-py3-none-any.whl.metadata (9.6 kB)\n",
            "  Downloading crewai-0.1.15-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading crewai-0.1.14-py3-none-any.whl.metadata (7.6 kB)\n",
            "  Downloading crewai-0.1.7-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading crewai-0.1.6-py3-none-any.whl.metadata (5.5 kB)\n",
            "  Downloading crewai-0.1.5-py3-none-any.whl.metadata (5.5 kB)\n",
            "  Downloading crewai-0.1.3-py3-none-any.whl.metadata (5.5 kB)\n",
            "  Downloading crewai-0.1.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "  Downloading crewai-0.1.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "  Downloading crewai-0.1.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "  Downloading langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.52 (from langchain-community)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.10 (from crewai)\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting openai<2.0.0,>=1.13.3 (from crewai)\n",
            "  Downloading openai-1.35.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting opentelemetry-api<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from crewai) (2.7.4)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from crewai) (1.0.1)\n",
            "Collecting regex<2024.0.0,>=2023.12.25 (from crewai)\n",
            "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic<2.0.0,>=1.13.1 (from embedchain==0.1.109->crewai)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from embedchain==0.1.109->crewai) (4.12.3)\n",
            "Collecting chromadb<0.5.0,>=0.4.24 (from embedchain==0.1.109->crewai)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting google-cloud-aiplatform<2.0.0,>=1.26.1 (from embedchain==0.1.109->crewai)\n",
            "  Downloading google_cloud_aiplatform-1.57.0-py2.py3-none-any.whl.metadata (31 kB)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain==0.1.109->crewai)\n",
            "  Downloading gptcache-0.1.43-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain==0.1.109->crewai)\n",
            "  Downloading langchain_cohere-0.1.8-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain==0.1.109->crewai)\n",
            "  Downloading langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting posthog<4.0.0,>=3.0.2 (from embedchain==0.1.109->crewai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from embedchain==0.1.109->crewai)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain==0.1.109->crewai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from embedchain==0.1.109->crewai) (13.7.1)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain==0.1.109->crewai)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting docstring-parser<0.17,>=0.16 (from instructor==1.3.3->crewai)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting jiter<0.5.0,>=0.4.1 (from instructor==1.3.3->crewai)\n",
            "  Downloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from instructor==1.3.3->crewai) (2.18.4)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from instructor==1.3.3->crewai) (0.12.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.10->crewai) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.10->crewai)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting docker<8.0.0,>=7.1.0 (from crewai_tools)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting docx2txt<0.9,>=0.8 (from crewai_tools)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting lancedb<0.6.0,>=0.5.4 (from crewai_tools)\n",
            "  Downloading lancedb-0.5.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pyright<2.0.0,>=1.1.350 (from crewai_tools)\n",
            "  Downloading pyright-1.1.369-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytest<9.0.0,>=8.0.0 (from crewai_tools)\n",
            "  Downloading pytest-8.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pytube<16.0.0,>=15.0.0 (from crewai_tools)\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting selenium<5.0.0,>=4.18.1 (from crewai_tools)\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain==0.1.109->crewai) (2.5)\n",
            "Collecting build>=1.0.3 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.111.0)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.30.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (4.12.2)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting tokenizers>=0.13.2 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (4.66.4)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (7.7.0)\n",
            "Collecting importlib-resources (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (3.10.5)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from docker<8.0.0,>=7.1.0->crewai_tools) (2.2.2)\n",
            "Collecting PyGithub<2.0.0,>=1.59.1 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading PyGithub-1.59.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting gitpython<4.0.0,>=3.1.38 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting youtube-transcript-api<0.7.0,>=0.6.1 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting yt_dlp<2024.0.0,>=2023.11.14 (from embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading yt_dlp-2023.12.30-py2.py3-none-any.whl.metadata (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation (from lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pylance==0.9.18 (from lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting ratelimiter~=1.0 (from lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting retry>=0.9.2 (from lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting semver>=3.0 (from lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: cachetools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lancedb<0.6.0,>=0.5.4->crewai_tools) (5.3.3)\n",
            "Collecting pyarrow>=12 (from pylance==0.9.18->lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (1.33)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain-community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from openai<2.0.0,>=1.13.3->crewai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from openai<2.0.0,>=1.13.3->crewai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from openai<2.0.0,>=1.13.3->crewai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from openai<2.0.0,>=1.13.3->crewai) (1.3.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (1.63.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-proto==1.25.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (4.23.4)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.4.2->crewai) (0.7.0)\n",
            "Collecting nodeenv>=1.6.0 (from pyright<2.0.0,>=1.1.350->crewai_tools)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting iniconfig (from pytest<9.0.0,>=8.0.0->crewai_tools)\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pluggy<2.0,>=1.5 (from pytest<9.0.0,>=8.0.0->crewai_tools)\n",
            "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytest<9.0.0,>=8.0.0->crewai_tools) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytest<9.0.0,>=8.0.0->crewai_tools) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Collecting trio~=0.17 (from selenium<5.0.0,>=4.18.1->crewai_tools)\n",
            "  Downloading trio-0.25.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.18.1->crewai_tools)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from selenium<5.0.0,>=4.18.1->crewai_tools) (1.8.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain==0.1.109->crewai)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.0.4)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (3.1.4)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (2.2.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4.0.0,>=3.1.38->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (2.19.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (2.30.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (1.24.0)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading google_cloud_storage-2.17.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading google_cloud_bigquery-3.25.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting shapely<3.0.0dev (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (0.14.0)\n",
            "Collecting zipp>=0.5 (from importlib-metadata<=7.1,>=6.0->opentelemetry-api<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (2.9.0.post0)\n",
            "Requirement already satisfied: requests-oauthlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (3.2.2)\n",
            "Collecting cohere<6.0,>=5.5.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai)\n",
            "  Downloading cohere-5.5.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-cohere to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain==0.1.109->crewai)\n",
            "  Downloading langchain_cohere-0.1.7-py3-none-any.whl.metadata (6.4 kB)\n",
            "  Downloading langchain_cohere-0.1.5-py3-none-any.whl.metadata (6.4 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain==0.1.109->crewai)\n",
            "  Downloading langchain_openai-0.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (1.12.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (69.5.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain==0.1.109->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from posthog<4.0.0,>=3.0.2->embedchain==0.1.109->crewai) (2.2.1)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools) (2.8.0)\n",
            "Collecting pynacl>=1.4.0 (from PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from retry>=0.9.2->lancedb<0.6.0,>=0.5.4->crewai_tools) (5.1.1)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb<0.6.0,>=0.5.4->crewai_tools)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->embedchain==0.1.109->crewai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14.0.0,>=13.7.0->embedchain==0.1.109->crewai) (2.18.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium<5.0.0,>=4.18.1->crewai_tools)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.18.1->crewai_tools)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.18.1->crewai_tools)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0.0,>=0.9.0->instructor==1.3.3->crewai) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.18.1->crewai_tools)\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.6.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (12.0)\n",
            "Collecting mutagen (from yt_dlp<2024.0.0,>=2023.11.14->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycryptodomex (from yt_dlp<2024.0.0,>=2023.11.14->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting brotli (from yt_dlp<2024.0.0,>=2023.11.14->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai) (1.34.134)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai)\n",
            "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (2.6.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.38->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (1.62.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (4.9)\n",
            "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai)\n",
            "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (2.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->embedchain==0.1.109->crewai) (0.1.2)\n",
            "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pynacl>=1.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools) (1.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain==0.1.109->crewai) (1.3.0)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.134 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai) (1.34.134)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere<0.2.0,>=0.1.4->embedchain==0.1.109->crewai) (0.10.2)\n",
            "Requirement already satisfied: pycparser in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub<2.0.0,>=1.59.1->embedchain[github,youtube]<0.2.0,>=0.1.85->crewai_tools) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain==0.1.109->crewai) (0.6.0)\n",
            "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading crewai-0.35.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.3/76.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embedchain-0.1.109-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.7/194.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n",
            "Downloading crewai_tools-0.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading lancedb-0.5.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.35.7-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyright-1.1.369-py3-none-any.whl (18 kB)\n",
            "Downloading pytest-8.2.2-py3-none-any.whl (339 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.9/339.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_aiplatform-1.57.0-py2.py3-none-any.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading gptcache-0.1.43-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.6/327.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cohere-0.1.5-py3-none-any.whl (30 kB)\n",
            "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n",
            "Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
            "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading PyGithub-1.59.1-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.25.1-py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Downloading yt_dlp-2023.12.30-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading cohere-5.5.8-py3-none-any.whl (173 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigquery-3.25.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl (333 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.7/333.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage-2.17.0-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Downloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
            "Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: docx2txt, pypika\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=12dadb522410c49903637cc8f0cc21869c1d9dcea332f33ebf234ee61b1c79dd\n",
            "  Stored in directory: /home/zeus/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53726 sha256=6c05c3b92d0a98b7d844a04de84fcfca4ad4b59cf2e5c4033704743c72d8f27b\n",
            "  Stored in directory: /home/zeus/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built docx2txt pypika\n",
            "Installing collected packages: sortedcontainers, schema, ratelimiter, pypika, monotonic, mmh3, flatbuffers, docx2txt, brotli, appdirs, zipp, wsproto, wrapt, types-requests, smmap, shapely, semver, regex, pytube, pysocks, pysbd, pyproject_hooks, pypdf, pycryptodomex, pyarrow, py, pulsar-client, pluggy, parameterized, packaging, outcome, opentelemetry-util-http, opentelemetry-proto, nodeenv, mypy-extensions, mutagen, Mako, jsonref, jiter, iniconfig, importlib-resources, humanfriendly, httpx-sse, google-crc32c, fastavro, docstring-parser, chroma-hnswlib, bcrypt, asgiref, yt_dlp, youtube-transcript-api, typing-inspect, trio, tiktoken, retry, pytest, pyright, pynacl, pylance, posthog, opentelemetry-exporter-otlp-proto-common, marshmallow, importlib-metadata, huggingface-hub, gptcache, google-resumable-media, gitdb, docker, deprecation, deprecated, cryptography, coloredlogs, build, alembic, trio-websocket, tokenizers, tavily-python, opentelemetry-api, openai, onnxruntime, lancedb, kubernetes, grpc-google-iam-v1, gitpython, dataclasses-json, selenium, PyGithub, opentelemetry-semantic-conventions, opentelemetry-instrumentation, langchain-core, instructor, google-cloud-core, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langchain-text-splitters, langchain-openai, langchain-community, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, cohere, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, langchain-cohere, langchain, google-cloud-aiplatform, chromadb, embedchain, crewai, crewai_tools\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.10\n",
            "    Uninstalling langchain-core-0.2.10:\n",
            "      Successfully uninstalled langchain-core-0.2.10\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.2\n",
            "    Uninstalling langchain-text-splitters-0.2.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.6\n",
            "    Uninstalling langchain-0.2.6:\n",
            "      Successfully uninstalled langchain-0.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 1.0.7 requires langchain-core<0.3,>=0.2.9, but you have langchain-core 0.1.52 which is incompatible.\n",
            "langchain-groq 0.1.6 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.1.52 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 PyGithub-1.59.1 alembic-1.13.2 appdirs-1.4.4 asgiref-3.8.1 bcrypt-4.1.3 brotli-1.1.0 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.4.24 cohere-5.5.8 coloredlogs-15.0.1 crewai-0.35.0 crewai_tools-0.4.0 cryptography-42.0.8 dataclasses-json-0.6.7 deprecated-1.2.14 deprecation-2.1.0 docker-7.1.0 docstring-parser-0.16 docx2txt-0.8 embedchain-0.1.109 fastavro-1.9.4 flatbuffers-24.3.25 gitdb-4.0.11 gitpython-3.1.43 google-cloud-aiplatform-1.57.0 google-cloud-bigquery-3.25.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.3 google-cloud-storage-2.17.0 google-crc32c-1.5.0 google-resumable-media-2.7.1 gptcache-0.1.43 grpc-google-iam-v1-0.13.1 httpx-sse-0.4.0 huggingface-hub-0.23.4 humanfriendly-10.0 importlib-metadata-7.1.0 importlib-resources-6.4.0 iniconfig-2.0.0 instructor-1.3.3 jiter-0.4.2 jsonref-1.1.0 kubernetes-30.1.0 lancedb-0.5.7 langchain-0.1.20 langchain-cohere-0.1.5 langchain-community-0.0.38 langchain-core-0.1.52 langchain-openai-0.1.7 langchain-text-splitters-0.0.2 marshmallow-3.21.3 mmh3-4.1.0 monotonic-1.6 mutagen-1.47.0 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.18.1 openai-1.35.7 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 outcome-1.3.0.post0 packaging-23.2 parameterized-0.9.0 pluggy-1.5.0 posthog-3.5.0 pulsar-client-3.5.0 py-1.11.0 pyarrow-16.1.0 pycryptodomex-3.20.0 pylance-0.9.18 pynacl-1.5.0 pypdf-4.2.0 pypika-0.48.9 pyproject_hooks-1.1.0 pyright-1.1.369 pysbd-0.3.4 pysocks-1.7.1 pytest-8.2.2 pytube-15.0.0 ratelimiter-1.2.0.post0 regex-2023.12.25 retry-0.9.2 schema-0.7.7 selenium-4.22.0 semver-3.0.2 shapely-2.0.4 smmap-5.0.1 sortedcontainers-2.4.0 tavily-python-0.3.3 tiktoken-0.7.0 tokenizers-0.19.1 trio-0.25.1 trio-websocket-0.11.1 types-requests-2.32.0.20240622 typing-inspect-0.9.0 wrapt-1.16.0 wsproto-1.2.0 youtube-transcript-api-0.6.2 yt_dlp-2023.12.30 zipp-3.19.2\n",
            "Requirement already satisfied: langchain in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.20)\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: langchain-google-genai in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.0.7)\n",
            "Requirement already satisfied: langchain-core in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.52)\n",
            "Collecting langchain-core\n",
            "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: google-generativeai<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-google-genai) (0.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.135.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (2.30.0)\n",
            "Requirement already satisfied: protobuf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.23.4)\n",
            "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.24.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.63.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (1.62.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain-google-genai) (0.6.0)\n",
            "Using cached langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
            "Using cached langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.52\n",
            "    Uninstalling langchain-core-0.1.52:\n",
            "      Successfully uninstalled langchain-core-0.1.52\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.0.2\n",
            "    Uninstalling langchain-text-splitters-0.0.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.0.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.1.20\n",
            "    Uninstalling langchain-0.1.20:\n",
            "      Successfully uninstalled langchain-0.1.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "crewai 0.35.0 requires langchain<0.2.0,>=0.1.10, but you have langchain 0.2.6 which is incompatible.\n",
            "crewai-tools 0.4.0 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.2.6 which is incompatible.\n",
            "embedchain 0.1.109 requires langchain<0.2.0,>=0.1.4, but you have langchain 0.2.6 which is incompatible.\n",
            "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.10 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain google-search-results langchain-google-genai langchain_groq youtube_search\n",
        "!pip install langchain-community tavily-python crewai crewai_tools\n",
        "!pip install --upgrade langchain langchain-google-genai langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IfW4tK4Ah-ES"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "os.environ[\"YOUTUBE_API_KEY\"] = userdata.get('YOUTUBE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TkxI2XyaiHx6"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_ajmNV3tiLhv"
      },
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "from crewai_tools import SerperDevTool,WebsiteSearchTool,YoutubeVideoSearchTool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0g9AEkcLmQdo"
      },
      "outputs": [],
      "source": [
        "from crewai import Agent\n",
        "\n",
        "def planner_Agent(llm, tools, max_iter):\n",
        "    study_planner_agent = Agent(\n",
        "        role='Study Planner Agent',\n",
        "        goal=\"Break down the user's problem into small, achievable steps for learning {topic} in {duration} with daily sessions of {studytime}. Return the steps as a JSON object with day numbers as keys and study tasks as values.\",\n",
        "        backstory=\"\"\"\n",
        "        You are a 'Smart Study Guide' who helps the user create a study plan to learn {topic} in {duration} with daily sessions of {studytime}.\n",
        "        The user's learning style is {style}, their grade is {grade}, and they belong to the {stream} stream. The user has {knowledge} level of prior knowledge about the domain.\n",
        "        And the User has distracktion tollerence is {distraction_tolerance}.\n",
        "        Prepare steps to achieve this goal, ensuring each day includes a new task and the last day includes a small project or revision.\n",
        "\n",
        "        Here's a possible study plan breakdown for learning {topic} in {duration}:\n",
        "\n",
        "        The title should not be more than 5 words.\n",
        "\n",
        "        Make sure to include some exercises for the user to practice.\n",
        "\n",
        "\n",
        "        Example JSON output:\n",
        "        (\n",
        "          'Day 1': 'Task 1',\n",
        "          'Day 2': 'Task 2',\n",
        "          'Day 3': 'Task 3',\n",
        "          ...\n",
        "          'Day N': 'Project or Revision'\n",
        "        )\n",
        "        \"\"\",\n",
        "        llm=llm,\n",
        "        verbose=True,\n",
        "        allow_delegation=True,\n",
        "        max_iter=max_iter,\n",
        "        cache=False\n",
        "    )\n",
        "\n",
        "    return study_planner_agent\n",
        "\n",
        "# Example usage\n",
        "study_planner_agent = planner_Agent(\n",
        "    llm=gemini_llm,\n",
        "    tools=[],\n",
        "    max_iter=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j-ijQo_PmQa5"
      },
      "outputs": [],
      "source": [
        "from crewai.task import TaskOutput\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "os.makedirs('final_outputs', exist_ok=True)\n",
        "\n",
        "# Function to clean the raw output\n",
        "def clean_raw_output(raw_output):\n",
        "    return raw_output.replace(\"```\", \"\").replace(\"json\", \"\").replace(r\"\\n\", '')\n",
        "\n",
        "# Function to parse JSON with fallback\n",
        "def parse_json_with_fallback(cleaned_output, output_description):\n",
        "    try:\n",
        "        return json.loads(cleaned_output)\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Log the error and save the raw output for debugging\n",
        "        with open(f'final_outputs/{output_description}_raw.txt', 'w') as f:\n",
        "            f.write(cleaned_output)\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        print(f\"Raw output stored in {output_description}_raw.txt\")\n",
        "        return None\n",
        "\n",
        "# Callback function for Task 1\n",
        "def planner_callback_function(output: TaskOutput):\n",
        "    print(\"Task completed!\")\n",
        "    task1_output = output.raw_output\n",
        "    cleaned_output = clean_raw_output(task1_output)\n",
        "    output_dict = parse_json_with_fallback(cleaned_output, output.description)\n",
        "    if output_dict is None:\n",
        "        return None\n",
        "\n",
        "    final_output = list(output_dict.values())\n",
        "    with open(f'final_outputs/{output.description}.txt', 'w') as f:\n",
        "        for item in final_output:\n",
        "            f.write(str(item) + '\\n')\n",
        "    return final_output\n",
        "\n",
        "\n",
        "planner_task = Task(\n",
        "    description='Prepare steps to achieve the goal of learning {topic} within {duration}.The title should not be more than 5 words and it should be such that it should be wasy to search it on internet',\n",
        "    expected_output='A json response where the keys are Day Numbers and values are the titles of important things to study.',\n",
        "    agent=study_planner_agent,\n",
        "    callback=planner_callback_function,\n",
        "    output_file='outputs/studyplan.json',\n",
        "    create_directory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC8SW46tmQX-",
        "outputId": "501f0223-6045-4dd0-b3e1-7281819e0a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[95m [2024-06-29 08:53:51][DEBUG]: == Working Agent: Study Planner Agent\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-06-29 08:53:51][INFO]: == Starting Task: Prepare steps to achieve the goal of learning Deep Learning within 5 days.The title should not be more than 5 words and it should be such that it should be wasy to search it on internet\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I now can give a great answer.\n",
            "Final Answer:\n",
            "```json\n",
            "{\n",
            "  \"Day 1\": \"Deep Learning Introduction & Neural Networks\",\n",
            "  \"Day 2\": \"Feedforward Neural Networks and Backpropagation\",\n",
            "  \"Day 3\": \"Convolutional Neural Networks (CNNs) for Image Recognition\",\n",
            "  \"Day 4\": \"Recurrent Neural Networks (RNNs) for Sequential Data\",\n",
            "  \"Day 5\": \"Deep Learning Project: Image Classification with CNN\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Task completed!\n",
            "\u001b[1m\u001b[92m [2024-06-29 08:53:53][DEBUG]: == [Study Planner Agent] Task output: ```json\n",
            "{\n",
            "  \"Day 1\": \"Deep Learning Introduction & Neural Networks\",\n",
            "  \"Day 2\": \"Feedforward Neural Networks and Backpropagation\",\n",
            "  \"Day 3\": \"Convolutional Neural Networks (CNNs) for Image Recognition\",\n",
            "  \"Day 4\": \"Recurrent Neural Networks (RNNs) for Sequential Data\",\n",
            "  \"Day 5\": \"Deep Learning Project: Image Classification with CNN\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[00m\n",
            "\n",
            "{\n",
            "  \"Day 1\": \"Deep Learning Introduction & Neural Networks\",\n",
            "  \"Day 2\": \"Feedforward Neural Networks and Backpropagation\",\n",
            "  \"Day 3\": \"Convolutional Neural Networks (CNNs) for Image Recognition\",\n",
            "  \"Day 4\": \"Recurrent Neural Networks (RNNs) for Sequential Data\",\n",
            "  \"Day 5\": \"Deep Learning Project: Image Classification with CNN\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crew definition\n",
        "from crewai.process import Process\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[study_planner_agent],\n",
        "    tasks=[planner_task],\n",
        "    process=Process.sequential,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Input data\n",
        "data_input = {\n",
        "    'topic': 'Deep Learning',\n",
        "    'duration': '5 days',\n",
        "    'studytime': '2 hrs/day',\n",
        "    'style': 'blogs',\n",
        "    'grade': 'First year',\n",
        "    'stream': 'Computer Science',\n",
        "    'knowledge': 'Python advance and Machine Learning',\n",
        "    'learning Time': 'Evening',\n",
        "    'distraction_tolerance': 'High',\n",
        "    'feedback frequency': 'Daily'\n",
        "}\n",
        "\n",
        "result = crew.kickoff(data_input)\n",
        "print(clean_raw_output(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkgsaFSmmQUr",
        "outputId": "9baef6de-6bc0-4473-92d0-10b15cfe8e53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Day 1': 'Deep Learning Introduction & Neural Networks',\n",
              " 'Day 2': 'Feedforward Neural Networks and Backpropagation',\n",
              " 'Day 3': 'Convolutional Neural Networks (CNNs) for Image Recognition',\n",
              " 'Day 4': 'Recurrent Neural Networks (RNNs) for Sequential Data',\n",
              " 'Day 5': 'Deep Learning Project: Image Classification with CNN'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json.loads(clean_raw_output(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zgvdR7OY_mp_"
      },
      "outputs": [],
      "source": [
        "study_plan = json.loads(clean_raw_output(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KcNtAMOj_q1z",
        "outputId": "8be2496c-bade-43d5-cc68-827b18f6f5d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"get_related_blogs(study_plan)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Day\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Day 2\",\n          \"Day 5\",\n          \"Day 3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Feedforward Neural Networks and Backpropagation\",\n          \"Deep Learning Project: Image Classification with CNN\",\n          \"Convolutional Neural Networks (CNNs) for Image Recognition\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Blogs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ecb44b2e-01be-4c89-b9ab-0733826bbe62\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Title</th>\n",
              "      <th>Blogs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Day 1</td>\n",
              "      <td>Deep Learning Introduction &amp; Neural Networks</td>\n",
              "      <td>[https://sebastianraschka.com/blog/2021/dl-cou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Day 2</td>\n",
              "      <td>Feedforward Neural Networks and Backpropagation</td>\n",
              "      <td>[https://www.baeldung.com/cs/neural-networks-b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Day 3</td>\n",
              "      <td>Convolutional Neural Networks (CNNs) for Image...</td>\n",
              "      <td>[https://www.edge-ai-vision.com/2015/11/using-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Day 4</td>\n",
              "      <td>Recurrent Neural Networks (RNNs) for Sequentia...</td>\n",
              "      <td>[https://shelf.io/blog/recurrent-neural-networ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Day 5</td>\n",
              "      <td>Deep Learning Project: Image Classification wi...</td>\n",
              "      <td>[https://github.com/On-Power-Studio/Image-clas...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecb44b2e-01be-4c89-b9ab-0733826bbe62')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ecb44b2e-01be-4c89-b9ab-0733826bbe62 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ecb44b2e-01be-4c89-b9ab-0733826bbe62');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11e29c7c-1520-4111-8419-cd7397ec9fc2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11e29c7c-1520-4111-8419-cd7397ec9fc2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11e29c7c-1520-4111-8419-cd7397ec9fc2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Day                                              Title  \\\n",
              "0  Day 1       Deep Learning Introduction & Neural Networks   \n",
              "1  Day 2    Feedforward Neural Networks and Backpropagation   \n",
              "2  Day 3  Convolutional Neural Networks (CNNs) for Image...   \n",
              "3  Day 4  Recurrent Neural Networks (RNNs) for Sequentia...   \n",
              "4  Day 5  Deep Learning Project: Image Classification wi...   \n",
              "\n",
              "                                               Blogs  \n",
              "0  [https://sebastianraschka.com/blog/2021/dl-cou...  \n",
              "1  [https://www.baeldung.com/cs/neural-networks-b...  \n",
              "2  [https://www.edge-ai-vision.com/2015/11/using-...  \n",
              "3  [https://shelf.io/blog/recurrent-neural-networ...  \n",
              "4  [https://github.com/On-Power-Studio/Image-clas...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_related_blogs(study_plan):\n",
        "  tavilytool = TavilySearchResults()\n",
        "  final_output = {}\n",
        "  for day,title in study_plan.items():\n",
        "    search_query = f\"Find Relevant blogs for {title}\"\n",
        "    search_results = tavilytool.invoke(search_query)  # Ensure tavilytool is properly initialized\n",
        "    blogs = [result['url'] for result in search_results]  # Extract URLs from search results\n",
        "    final_output[title] = blogs\n",
        "  df = pd.DataFrame()\n",
        "  df['Day'] = list(study_plan.keys())\n",
        "  df['Title'] = list(study_plan.values())\n",
        "  df['Blogs'] = list(final_output.values())\n",
        "  return df\n",
        "\n",
        "get_related_blogs(study_plan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cJa2fEIqEUgb"
      },
      "outputs": [],
      "source": [
        "blog_df = get_related_blogs(study_plan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpwsvLEf1iDQ"
      },
      "source": [
        "# Blog Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qkfg5OWOQXsD",
        "outputId": "94b8e23a-dff6-496e-cd30-a31303e1c580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Deep Learning Introduction & Neural Networks': [{'https://sebastianraschka.com/blog/2021/dl-course.html': ''}, {'https://www.dataquest.io/blog/tutorial-introduction-to-deep-learning/': 'Tutorial: Introduction to Deep Learning Dashboard Learning Path Catalog Full Catalog Career Paths Skill Paths Individual Courses Data Science Projects Success Stories Resources How to Learn Data Science A Better Way to Learn Understanding Data Roles Live Project Walkthroughs Learning Resources For Teams Sign In Start Free Profile Account Subscription Teams Help Logout March 31, 2023 Tutorial: Introduction to Deep Learning'}, {'https://www.datacamp.com/tutorial/introduction-to-deep-neural-networks': ''}, {'https://www.datacamp.com/blog/how-to-learn-deep-learning': ''}, {'https://towardsdatascience.com/intro-to-deep-learning-c025efd92535': ''}], 'Feedforward Neural Networks and Backpropagation': [{'https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/': 'Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations | I, Deep Learning I, Deep Learning About Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations Dec 10, 2021 This post is the first of a three-part series in which we set out to derive the mathematics behind feedforward neural networks. They have an input and an output layer with at least one hidden layer in between, fully-connected layers, which means that each node in one layer connects to every node in the following layer, and ways to introduce nonlinearity by means of activation functions. We start with forward propagation, which involves computing predictions and the associated cost of these predictions. Forward Propagation Settling on what notations to use is tricky since we only have so many letters in the Roman alphabet. As you browse the Internet, you will likely find derivations that have used different notations than the ones we are about to introduce. However, and fortunately, there is no right or wrong here; it is just a matter of taste. In particular, the notations used in this series take inspiration from Andrew Ngs Standard notations for Deep Learning. If you make a comparison, you will find that we only change a couple of the details. Now, whatever we come up with, we have to support multiple layers, several nodes in each layer, various activation functions, various types of cost functions, and mini-batches of training examples. As a result, our definition of a node ends up introducing a fairly large number of notations: \\\\} &= \\\\sum_k w_{j, k}^{} a_{k, i}^{} + b_j^{}, \\\\label{eq:z_scalar} \\\\\\\\ a_{j, i}^{} &= g_j^{}(z_{1, i}^{}, \\\\dots, z_{j, i}^{}, \\\\dots, z_{n^{}, i}^{}). \\\\label{eq:a_scalar} \\\\end{align}\\\\] Does the node definition look intimidating to you at first glance? Do not worry. Hopefully, it will make more sense once we have explained the notations, which we shall do next: Entity Description \\\\(l\\\\) The current layer \\\\(l = 1, \\\\dots, L\\\\), where \\\\(L\\\\) is the number of layers that have weights and biases. We use \\\\(l = 0\\\\) and \\\\(l = L\\\\) to denote the input and output layers. \\\\(n^{}\\\\) The number of nodes in the current layer. \\\\(n^{}\\\\) The number of nodes in the previous layer. \\\\(j\\\\) The \\\\(j\\\\)th node of the current layer, \\\\(j = 1, \\\\dots, n^{}\\\\).'}, {'https://www.geeksforgeeks.org/feedforward-neural-network/': 'Feedforward neural network - GeeksforGeeks Skip to content'}, {'https://www.baeldung.com/cs/neural-networks-backprop-vs-feedforward': 'Differences Between Backpropagation and Feedforward Networks | Baeldung on Computer Science Start HereTopics  Core Concepts Fundamental concepts in Computer Science Operating Systems Learn about the types of OSs used and the basic services they provide. Artificial Intelligence Explore the concepts and algorithms at the foundation of modern artificial intelligence Graph Theory Learn how GPS systems find the shortest routes, how engineers design integrated circuits and more real-world uses of graphs Latex A powerful preparation tool for creating high-quality document. About  Full Archive The high level overview of all the articles on the site. About Baeldung About Baeldung. Differences Between Backpropagation and Feedforward Networks Last updated: March 18, 2024 Written by: Saulo Barreto Reviewed by: Milos Simic Deep LearningMachine Learning Neural Networks Training 1. Introduction Many deep-learning frameworks provide us with intuitive interfaces to set the layers, tune the hyper-parameters and evaluate our models. But to discuss the results properly, and more importantly, to understand how the networks work, we need to be familiar with fundamental concepts. In this tutorial, well talk about Backpropagation (or Backprop) and Feedforward Neural Networks. 2. Feedforward Neural Networks Feedforward networks are the quintessential deep learning models. Theyre made up of artificial neurons that are organized in layers. 2.1. How Does an Artificial Neuron Work? A neuron is a rule that transforms an input vector into the output signal we call the activation value : (1) where is the neurons bias, the are the weights specific to the neuron, and is the activation function such as ReLU or sigmoid. For example, heres how the -th neuron in a layer computes its output after receiving two input values:: 2.2. Propagating Forward A layer is an array of neurons. A network can have any number of layers between the input and the output ones. For instance:'}, {'https://automaticaddison.com/artificial-feedforward-neural-network-with-backpropagation-from-scratch/': 'Artificial Feedforward Neural Network With Backpropagation From Scratch  Automatic Addison Skip to content Automatic Addison Build the Future Menu Home Need Help? Tutorials About Me LinkedIn YouTube Artificial Feedforward Neural Network With Backpropagation From Scratch In this post, I will walk you through how to build an artificial feedforward neural network trained with backpropagation, step-by-step. We will not use any fancy machine learning libraries, only basic Python libraries like Pandas and Numpy. Our end goal is to evaluate the performance of an artificial feedforward neural network trained with backpropagation and to compare the performance using no hidden layers, one hidden layer, and two hidden layers. Five different data sets from the UCI Machine Learning Repository are used to compare performance: Breast Cancer, Glass, Iris, Soybean (small), and Vote. We will use our neural network to do the following: Predict if someone has breast cancerIdentify glass typeIdentify flower speciesDetermine soybean disease typeClassify a representative as either a Democrat or Republican based on their voting patterns I hypothesize that the neural networks with no hidden layers will outperform the networks with two hidden layers. My hypothesis is based on the notion that the simplest solutions are often the best solutions (i.e. Occams Razor). The classification accuracy of the algorithms on the data sets will be evaluated as follows, using five-fold stratified cross-validation: Accuracy = (number of correct predictions)/(total number of predictions) Title image source: Wikimedia commons'}, {'https://www.geeksforgeeks.org/backpropagation-in-neural-network/': 'Backpropagation in Neural Network - GeeksforGeeks Skip to content'}], 'Convolutional Neural Networks (CNNs) for Image Recognition': [{'https://www.edge-ai-vision.com/2015/11/using-convolutional-neural-networks-for-image-recognition/': \"Using Convolutional Neural Networks for Image Recognition - Edge AI and Vision Alliance Skip to content Main Menu HomeThe AllianceMenu Toggle About History Events Members Become a Member Press Information Edge AI and Vision Awards Vision Accelerator Program Contact ResourcesMenu Toggle About Edge AI + Vision Technologies Applications Functions Videos Articles Blog Posts Market Analysis Webinars Industry Map Developer Survey Privacy The SummitMenu Toggle Speak at the Summit Become a Sponsor The Summit Experience Program Pricing Summit ReplaysMenu Toggle May 2024 Summit May 2023 Summit May 2022 Summit May 2021 Summit September 2020 Summit May 2019 Summit May 2018 Summit May 2017 Summit May 2016 Summit May 2015 Summit May 2014 Summit MembersMenu Toggle Alliance Member Companies Become a Member Members Area NewsMenu Toggle Newsletter RegistrationLog In If you're building AI or vision-enabled products, you've come to the right place. Search for: Search Using Convolutional Neural Networks for Image Recognition Algorithms, Articles, Cadence, Face Recognition, Object Identification, Optical Character Recognition, Processors, Technical Articles / November 12, 2015\"}, {'https://medium.com/intuitive-deep-learning/build-your-first-convolutional-neural-network-to-recognize-images-84b9c78fe0ce': ''}, {'https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns': ''}, {'https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/': \"Image Classification Using CNN (Convolutional Neural Networks) Explore Discover Blogs Unpacking the latest trends in AI - A knowledge capsule Leadership Podcasts Know the perspective of top leaders Expert Sessions Go deep with industry leaders in live, interactive sessions Comprehensive Guides Master complex topics with comprehensive, step-by-step resources Learn Free Courses Kickstart your AI journey with our comprehensive courses Certified AI & ML BlackBelt Plus Program Master AI like a pro with our mentor-driven program Learning Paths Expert-curated paths for every goal Gen AI Pinnacle Program Learn cutting-edge skills for your AI career Engage Thriving Community Share knowledge, spark ideas, and build connections Exciting Events Dive into the hottest trends and network with industry leaders Challenging Hackathons Test your skills, unleash your creativity, and win big DataHack Summit 2024 India's Most Futuristic GenAI Conference Contribute Become a Wordsmith Share your voice, and win big in blogathons Become a Mentor Craft careers by sharing your knowledge Become a Speaker Inspire minds, share your expertise Become an Instructor Shape next-gen innovators through our programs Corporate Our Offerings Build a data-powered and data-driven workforce Trainings Bridge your team's data skills with targeted training Analytics maturity Unleash the power of analytics for smarter outcomes Data Culture Break down barriers and democratize data access and usage Login Switch Mode Logout d : h : m : s Home Algorithm Image Classification Using CNN (Convolutional Neural Networks) Image Classification Using CNN (Convolutional Neural Networks)  Sanad 06 Jun, 2024 13 min read Introduction Convolutional neural networks (CNN)  the concept behind recent breakthroughs and developments in deep learning.\"}, {'https://www.learndatasci.com/tutorials/convolutional-neural-networks-image-classification/': 'Convolutional Neural Networks  Image Classification w. Keras  LearnDataSciCookie PolicyWe use cookies to operate this website, improve usability, personalize your experience, and improve our marketing. Privacy Policy.By clicking \"Accept\" or further use of this website, you agree to allow cookies.AcceptLearn Machine Learning by Doing Learn NowThe internet\\'s best courses on: Data Science Machine Learning PythonToggle navigation DailyLearning CurriculumTutorialsArticlesGlossaryBooksSolutionsCourses Data ScienceData AnalyticsMachine LearningPythonSQLExcelAITeam2.6K554You are reading tutorials18SHARES Author: James McDermott Data Science ConsultantConvolutional Neural Networks  Image Classification w. KerasLearnDataSci is reader-supported. When you purchase through links on our site, earned commissions help support our team of writers, researchers, and designers at no extra cost to you.You should already know:You should be fairly comfortable with Python and have a basic grasp of regular Neural Networks for this tutorial. The Neural Networks and Deep Learning course on Coursera is a great place to start.Introduction to Image ClassificationThere\\'s no shortage of smartphone apps today that perform some sort of Computer Vision task. Computer Vision is a domain of Deep Learning that centers on the fundamental problem in training a computer to see as a human does.The way in which we perceive the world is not an easy feat to replicate in just a few lines of code. We are constantly recognizing, segmenting, and inferring objects and faces that pass our vision. Subconsciously taking in information, the human eye is a marvel in itself. Computer Vision deals in studying the phenomenon of human vision and perception by tackling several \\'tasks\\', to name just a few:Object DetectionImage ClassificationImage ReconstructionFace RecognitionSemantic SegmentationThe research behind these tasks is growing at an exponential rate, given our digital age. The accessibility of high-resolution imagery through smartphones is unprecedented, and what better way to leverage this surplus of data than by studying it in the context of Deep Learning.In this article, we will tackle one of the Computer Vision tasks mentioned above, Image Classification.Image Classification attempts to connect an image to a set of class labels. It is a supervised learning problem, wherein a set of pre-labeled training data is fed to a machine learning algorithm. This algorithm attempts| to learn the visual'}], 'Recurrent Neural Networks (RNNs) for Sequential Data': [{'https://shelf.io/blog/recurrent-neural-networks/': ''}, {'https://neptune.ai/blog/recurrent-neural-network-guide': \"Recurrent Neural Network Guide: a Deep Dive in RNN Are you training large models?Explore Neptune Scale: The experiment tracker for foundation model trainingRead more Product Overview Walk-through Deployment optionsSecurityRoadmapCompare Neptune vs WandBNeptune vs MLflowNeptune vs TensorBoardOther comparisons neptune.ai demo See Neptune in action with Aurimas Gricinas, Neptune's CPO and one of LinkedIns most-followed Data Engineering and ML Systems expert. Solutions By role Data ScientistML Team LeadML Platform EngineerAcademia & KagglersBy use case Compare experimentsMonitor trainingReproduce experimentsCollaborate with a team Case studyHow deepsense.ai Tracked and Analyzed 120K+ Models Using Neptune Case studyHow ReSpo.Vision Uses Neptune to Easily Track Training Pipelines at Scale See all case studies Developers Menu Item DocumentationQuickstartIntegrationsCode examplesResources Use Neptune Play with public sandboxProduct demoCase studiesExample projectsVideo tutorialsAll Neptune resourcesLearn MLOps MLOps BlogExperiment Tracking Learn HubMLOps Learn HubML Platform Podcast ArticleBuilding a Machine Learning PlatformA comprehensive guide created as a result of conversations with platform engineers and public resources from platform teams. PodcastLearnings From Building the ML Platform at MailchimpLatest episode of the ML Platform Podcast, with Mikiko Bazeley, Head of MLOps at Featureform PricingEnterpriseCompany Menu Item About usCustomersCareersIn the newsSecurityContact us What do you want to find? Search Log in Sign up Contact us > Blog > ML Model Development Topics Categories ML Model DevelopmentMLOpsLLMOpsML ToolsComputer VisionCategories Natural Language ProcessingReinforcement LearningTabular DataTime Series Search in Blog... Search in Blog... Product Overview Walk-through Deployment optionsSecurityRoadmapCompare Neptune vs WandBNeptune vs MLflowNeptune vs TensorBoardOther comparisons neptune.ai demo See Neptune in action with Aurimas Gricinas, Neptune's CPO and one of LinkedIns most-followed Data Engineering and ML Systems expert.\"}, {'https://libraria.ai/blog/recurrent-neural-networks-understanding-sequential-data/': 'Recurrent Neural Networks: Understanding Sequential Data Skip to content Menu Why Libraria AI? Pricing Discord Ecommerce Blog Get Started Developer API Why Libraria AI? Pricing Discord Ecommerce Blog Get Started Developer API Menu Why Libraria AI? Pricing Discord Ecommerce Blog Get Started Developer API Contact Signup Recurrent Neural Networks: Understanding Sequential Data Join Discord Build Chatbot In the era of artificial intelligence and deep learning, there is a powerful tool that stands out for its ability to analyze and process sequential data  Recurrent Neural Networks (RNNs). These neural network models have revolutionized the field of sequential data analysis, enabling breakthroughs in various industries, including natural language processing, time series analysis, and speech recognition. RNNs are designed to mimic the functionality of the human brain, allowing them to remember past inputs and use that knowledge to make predictions about future data points. This unique feature makes them particularly well-suited for tasks that involve sequential data, where understanding context and dependencies is crucial. In this article, we will delve into the workings of recurrent neural networks, exploring their architecture, training process, and different types. We will also discuss the advantages and disadvantages of RNNs and highlight some specialized neural network models that have emerged to address their limitations. Finally, we will explore the diverse applications of recurrent neural networks in various industries. Key Takeaways: Recurrent Neural Networks (RNNs) are a type of neural network algorithm used for processing sequential data. RNNs have the ability to remember past inputs and use that information to make predictions about future data points. RNNs have become essential in the field of deep learning, driving advancements in natural language processing and time series prediction. There are different types of RNN architectures, including one-to-one, one-to-many, many-to-one, and many-to-many. RNNs face challenges such as exploding and vanishing gradients, which can be addressed through techniques like gradient clipping and specialized RNN variants like Long Short-Term Memory (LSTM) networks. What are Recurrent Neural Networks (RNNs)?'}, {'https://datasciencectraining.medium.com/sequential-data-analysis-with-recurrent-neural-networks-e3d8f544d416': ''}, {'https://medium.com/@data-overload/unlocking-sequential-understanding-recurrent-neural-networks-rnns-and-transformers-dcf83fbe0c9': ''}], 'Deep Learning Project: Image Classification with CNN': [{'https://github.com/On-Power-Studio/Image-classification-with-Deep-Learning-using-CNN-': 'GitHub - On-Power-Studio/Image-classification-with-Deep-Learning-using-CNN-: \"Delve into the realm of deep learning with my projectImage Classification using CNNs. Focused on pet images, this project explores Convolutional Neural Networks, achieving notable accuracy through meticulous training and fine-tuning. Join me in the magic of image classification!  #DeepLearning #CNN #ImageClassification\" Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Resources Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search'}, {'https://www.analyticsvidhya.com/blog/2021/06/develop-your-first-image-classification-project-with-convolutional-neural-network/': \"Develop your First Image Classification Project with CNN! - Analytics Vidhya Explore Discover Blogs Unpacking the latest trends in AI - A knowledge capsule Leadership Podcasts Know the perspective of top leaders Expert Sessions Go deep with industry leaders in live, interactive sessions Comprehensive Guides Master complex topics with comprehensive, step-by-step resources Learn Free Courses Kickstart your AI journey with our comprehensive courses Certified AI & ML BlackBelt Plus Program Master AI like a pro with our mentor-driven program Learning Paths Expert-curated paths for every goal Gen AI Pinnacle Program Learn cutting-edge skills for your AI career Engage Thriving Community Share knowledge, spark ideas, and build connections Exciting Events Dive into the hottest trends and network with industry leaders Challenging Hackathons Test your skills, unleash your creativity, and win big DataHack Summit 2024 India's Most Futuristic GenAI Conference Contribute Become a Wordsmith Share your voice, and win big in blogathons Become a Mentor Craft careers by sharing your knowledge Become a Speaker Inspire minds, share your expertise Become an Instructor Shape next-gen innovators through our programs Corporate Our Offerings Build a data-powered and data-driven workforce Trainings Bridge your team's data skills with targeted training Analytics maturity Unleash the power of analytics for smarter outcomes Data Culture Break down barriers and democratize data access and usage Login Switch Mode Logout d : h : m : s Home Advanced Develop your First Image Processing Project with Convolutional Neur... Develop your First Image Processing Project with Convolutional Neural Network! Raghav Agrawal 26 Aug, 2021 7 min read\"}, {'https://github.com/iadaarsh/Image-Classification-with-Convolutional-Neural-Network': 'GitHub - iadaarsh/Image-Classification-with-Convolutional-Neural-Network: This project explores image classification using CNNs on the CIFAR-10 dataset. It optimizes CNN architecture for high accuracy, showcasing its efficacy in diverse real-world applications. Skip to content Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Resources Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips'}, {'https://www.analyticsvidhya.com/blog/2021/01/image-classification-using-convolutional-neural-networks-a-step-by-step-guide/': \"Image Classification Using CNN with Keras & CIFAR-10 Explore Discover Blogs Unpacking the latest trends in AI - A knowledge capsule Leadership Podcasts Know the perspective of top leaders Expert Sessions Go deep with industry leaders in live, interactive sessions Comprehensive Guides Master complex topics with comprehensive, step-by-step resources Learn Free Courses Kickstart your AI journey with our comprehensive courses Certified AI & ML BlackBelt Plus Program Master AI like a pro with our mentor-driven program Learning Paths Expert-curated paths for every goal Gen AI Pinnacle Program Learn cutting-edge skills for your AI career Engage Thriving Community Share knowledge, spark ideas, and build connections Exciting Events Dive into the hottest trends and network with industry leaders Challenging Hackathons Test your skills, unleash your creativity, and win big DataHack Summit 2024 India's Most Futuristic GenAI Conference Contribute Become a Wordsmith Share your voice, and win big in blogathons Become a Mentor Craft careers by sharing your knowledge Become a Speaker Inspire minds, share your expertise Become an Instructor Shape next-gen innovators through our programs Corporate Our Offerings Build a data-powered and data-driven workforce Trainings Bridge your team's data skills with targeted training Analytics maturity Unleash the power of analytics for smarter outcomes Data Culture Break down barriers and democratize data access and usage Login Switch Mode Logout d : h : m : s Home Advanced Image Classification Using CNN with Keras & CIFAR-10 Image Classification Using CNN with Keras & CIFAR-10 D Devansh Sharma 24 Jun, 2024 7 min read\"}, {'https://www.kaggle.com/code/arbazkhan971/image-classification-using-cnn-94-accuracy': 'Image Classification using CNN (94%+ Accuracy) | Kaggle'}]}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "class SimpleDocument:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata or {}\n",
        "\n",
        "def scrape_blog(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        return soup.get_text()\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra whitespaces\n",
        "    cleaned_text = ' '.join(text.split())\n",
        "\n",
        "    # Remove HTML artifacts\n",
        "    cleaned_text = BeautifulSoup(cleaned_text, \"html.parser\").get_text()\n",
        "\n",
        "    # Remove non-textual content (if any)\n",
        "    cleaned_text = re.sub(r'\\[[^\\]]*\\]', '', cleaned_text)  # Remove text within square brackets\n",
        "    cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_text)\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def selecting_best_blog(blog_df):\n",
        "    # Initialize the dictionary to store the results\n",
        "    result_dict = {}\n",
        "\n",
        "    # Convert the dataframe to a list of dictionaries\n",
        "    selecting_blog = blog_df[['Title', 'Blogs']].to_dict('records')\n",
        "\n",
        "    # Loop through each blog entry\n",
        "    for dic in selecting_blog:\n",
        "        title = dic['Title']\n",
        "        urls = dic['Blogs'] # Assuming URLs are comma-separated strings\n",
        "\n",
        "        # Initialize list to hold content for each URL under the same topic\n",
        "        content_list = []\n",
        "\n",
        "        for url in urls:\n",
        "            url = url.strip()  # Remove any extra whitespace\n",
        "            if url:  # Ensure URL is not empty\n",
        "                # Scrape the content\n",
        "                full_text = scrape_blog(url)\n",
        "\n",
        "                # Create a SimpleDocument object\n",
        "                doc = SimpleDocument(page_content=full_text)\n",
        "\n",
        "                # Split the documents to get the first 2500 words\n",
        "                ts = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\n",
        "                fd = ts.split_documents([doc])\n",
        "\n",
        "                # Store only the first chunk (first 2500 words)\n",
        "                first_2500_words = clean_text(fd[0].page_content) if fd else \"\"\n",
        "\n",
        "                # Add the URL and extracted content to the content list\n",
        "                content_list.append({url: first_2500_words})\n",
        "\n",
        "        # Store the result in the dictionary\n",
        "        result_dict[title] = content_list\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "result = selecting_best_blog(blog_df)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EpN7VcqTnkAT"
      },
      "outputs": [],
      "source": [
        "#result['Django Models: Creating Data Structures']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "c8qr3kmjv3c0",
        "outputId": "bf704411-39a5-47a2-fed5-cb34ee290b43"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\nclass BestURL(BaseModel):\\n    \"\"\"Best Url \"\"\"\\n    Title: str = Field(description=\"The name of the title\")\\n    url: str = Field(description=\"the best url among the urls\")\\n\\nstructured_llm = gemini_llm.with_structured_output(BestURL)\\n\\ntemplate = PromptTemplate(\\n  input_variables=[\\'topic\\',\\'distraction_tolerance\\',\\'content\\'],\\n  template=\"Select the best 1 blog url on the topic {topic} by analysing the {content}\"\\n)\\nprompt = template.format(topic=\\'Django Models: Creating Data Structures\\',distraction_tolerance=5,content=result[\\'Django Models: Creating Data Structures\\'])\\n\\nprint(structured_llm.invoke(prompt))'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class BestURL(BaseModel):\n",
        "    \"\"\"Best Url \"\"\"\n",
        "    Title: str = Field(description=\"The name of the title\")\n",
        "    url: str = Field(description=\"the best url among the urls\")\n",
        "\n",
        "structured_llm = gemini_llm.with_structured_output(BestURL)\n",
        "\n",
        "template = PromptTemplate(\n",
        "  input_variables=['topic','distraction_tolerance','content'],\n",
        "  template=\"Select the best 1 blog url on the topic {topic} by analysing the {content}\"\n",
        ")\n",
        "prompt = template.format(topic='Django Models: Creating Data Structures',distraction_tolerance=5,content=result['Django Models: Creating Data Structures'])\n",
        "\n",
        "print(structured_llm.invoke(prompt))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b9eN_MNsW_v",
        "outputId": "c6b4a5b8-bd18-4442-cc29-4c656026da8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([{'Title': 'Deep Learning Introduction & Neural Networks', 'blog_url': 'https://www.dataquest.io/blog/tutorial-introduction-to-deep-learning/'}, {'Title': 'Feedforward Neural Networks and Backpropagation', 'blog_url': 'https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/'}, {'Title': 'CNNs for Image Recognition', 'blog_url': 'https://www.edge-ai-vision.com/2015/11/using-convolutional-neural-networks-for-image-recognition/'}, {'Title': 'Recurrent Neural Networks (RNNs) for Sequential Data', 'blog_url': 'https://neptune.ai/blog/recurrent-neural-network-guide'}, {'Title': 'Deep Learning Project: Image Classification with CNN', 'blog_url': 'https://www.analyticsvidhya.com/blog/2021/06/develop-your-first-image-classification-project-with-convolutional-neural-network/'}])\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import time\n",
        "\n",
        "class BestURL(BaseModel):\n",
        "  \"\"\"Best Url \"\"\"\n",
        "  Title: str = Field(description=\"The name of the title\")\n",
        "  url: str = Field(description=\"the best url among the urls\")\n",
        "\n",
        "structured_llm = gemini_llm.with_structured_output(BestURL)\n",
        "\n",
        "# Modified template to avoid multiple function calls\n",
        "template = PromptTemplate(\n",
        "  input_variables=['topic', 'distraction_tolerance', 'content'],\n",
        "  template=\"Select the best blog 1 url on the topic {topic} by analyzing the {content}\"\n",
        ")\n",
        "\n",
        "best_video_urls = {}\n",
        "\n",
        "for topic, content in result.items():\n",
        "  prompt = template.format(topic=topic, distraction_tolerance=5, content=content)\n",
        "\n",
        "  try:\n",
        "    response = structured_llm.invoke(prompt)\n",
        "\n",
        "    # Check if response is None or not\n",
        "    if response:\n",
        "      if response.url.startswith(\"https://\"):\n",
        "        best_video_urls[topic] = {'Title': response.Title, 'blog_url': response.url}\n",
        "      else:\n",
        "        # Handle non-YouTube URLs (optional)\n",
        "        # You can choose to ignore them, log them, or take other actions\n",
        "        pass\n",
        "    else:\n",
        "      # Handle case where response is None (optional)\n",
        "      best_video_urls[topic] = {'Title': topic, 'blog_url': content[0].keys()[0]}\n",
        "\n",
        "  except:\n",
        "    # Handle potential exceptions (optional)\n",
        "    pass\n",
        "\n",
        "best_blog_urls = best_video_urls.values()\n",
        "print(best_blog_urls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yk8FhT7FAYtv",
        "outputId": "d544a3d7-3d69-44bc-8e58-54cc632c4bcd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Feedforward Neural Networks and Backpropagation\",\n          \"Deep Learning Project: Image Classification with CNN\",\n          \"CNNs for Image Recognition\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blog_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/\",\n          \"https://www.analyticsvidhya.com/blog/2021/06/develop-your-first-image-classification-project-with-convolutional-neural-network/\",\n          \"https://www.edge-ai-vision.com/2015/11/using-convolutional-neural-networks-for-image-recognition/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-465feceb-0cd3-4f3f-a9d5-ae1e61008d30\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>blog_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Deep Learning Introduction &amp; Neural Networks</td>\n",
              "      <td>https://www.dataquest.io/blog/tutorial-introdu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feedforward Neural Networks and Backpropagation</td>\n",
              "      <td>https://jonaslalin.com/2021/12/10/feedforward-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CNNs for Image Recognition</td>\n",
              "      <td>https://www.edge-ai-vision.com/2015/11/using-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recurrent Neural Networks (RNNs) for Sequentia...</td>\n",
              "      <td>https://neptune.ai/blog/recurrent-neural-netwo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Deep Learning Project: Image Classification wi...</td>\n",
              "      <td>https://www.analyticsvidhya.com/blog/2021/06/d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-465feceb-0cd3-4f3f-a9d5-ae1e61008d30')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-465feceb-0cd3-4f3f-a9d5-ae1e61008d30 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-465feceb-0cd3-4f3f-a9d5-ae1e61008d30');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7c8e12bd-03d0-45b5-953d-86dee2d596e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c8e12bd-03d0-45b5-953d-86dee2d596e9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7c8e12bd-03d0-45b5-953d-86dee2d596e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_494c24a5-7d48-45f3-be3a-877aa4c60f6c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_494c24a5-7d48-45f3-be3a-877aa4c60f6c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Title  \\\n",
              "0       Deep Learning Introduction & Neural Networks   \n",
              "1    Feedforward Neural Networks and Backpropagation   \n",
              "2                         CNNs for Image Recognition   \n",
              "3  Recurrent Neural Networks (RNNs) for Sequentia...   \n",
              "4  Deep Learning Project: Image Classification wi...   \n",
              "\n",
              "                                            blog_url  \n",
              "0  https://www.dataquest.io/blog/tutorial-introdu...  \n",
              "1  https://jonaslalin.com/2021/12/10/feedforward-...  \n",
              "2  https://www.edge-ai-vision.com/2015/11/using-c...  \n",
              "3  https://neptune.ai/blog/recurrent-neural-netwo...  \n",
              "4  https://www.analyticsvidhya.com/blog/2021/06/d...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(best_blog_urls)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g20__Yg1l_x"
      },
      "source": [
        "# Video Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l4DMSLeKMyt",
        "outputId": "2aa1f123-e6c5-425f-888e-b048b44ae3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'Deep Learning Introduction & Neural Networks': ['https://www.youtube.com/watch?v=aircAruvnKk', 'https://www.youtube.com/watch?v=jmmW0F0biz0', 'https://www.youtube.com/watch?v=q6kJ71tEYqM', 'https://www.youtube.com/watch?v=oV3ZY6tJiA0', 'https://www.youtube.com/watch?v=CqOfi41LfDw']}, {'Feedforward Neural Networks and Backpropagation': ['https://www.youtube.com/watch?v=Ilg3gGewQ5U', 'https://www.youtube.com/watch?v=S5AGN9XfPK4', 'https://www.youtube.com/watch?v=y0wNuFFPGuI', 'https://www.youtube.com/watch?v=jTzJ9zjC8nU', 'https://www.youtube.com/watch?v=CqOfi41LfDw']}, {'Convolutional Neural Networks (CNNs) for Image Recognition': ['https://www.youtube.com/watch?v=QzY57FaENXg', 'https://www.youtube.com/watch?v=K_BHmztRTpA', 'https://www.youtube.com/watch?v=pj9-rr1wDhM', 'https://www.youtube.com/watch?v=KuXjwB4LzSA', 'https://www.youtube.com/watch?v=CYvBjQTOdf4']}, {'Recurrent Neural Networks (RNNs) for Sequential Data': ['https://www.youtube.com/watch?v=AsNTP8Kwu80', 'https://www.youtube.com/watch?v=Or9QSDqzOK0', 'https://www.youtube.com/watch?v=5fdy-hBeWCI', 'https://www.youtube.com/watch?v=6niqTuYFZLQ', 'https://www.youtube.com/watch?v=OuYtk9Ymut4']}, {'Deep Learning Project: Image Classification with CNN': ['https://www.youtube.com/watch?v=K_BHmztRTpA', 'https://www.youtube.com/watch?v=taC5pMCm70U', 'https://www.youtube.com/watch?v=QzY57FaENXg', 'https://www.youtube.com/watch?v=ZTCoMo_gDYo', 'https://www.youtube.com/watch?v=LsdxvjLWkIY']}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "def request_video(topic: str, api_key: str):\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "    params = {\n",
        "        'part': 'snippet',\n",
        "        'q': topic,\n",
        "        'type': 'video',\n",
        "        'maxResults': 5,\n",
        "        'videoCaption': 'closedCaption',  # Filter for videos with captions\n",
        "        'key': api_key\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status()\n",
        "    result = response.json()\n",
        "    video_links = []\n",
        "    for item in result.get('items', []):\n",
        "        title = item['snippet']['title']\n",
        "        video_id = item['id']['videoId']\n",
        "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "        video_links.append(video_url)\n",
        "    return {topic: video_links}\n",
        "\n",
        "def get_related_videos(study_plan, api_key: str):\n",
        "    study_topics = list(study_plan.values())\n",
        "    video_links = []\n",
        "    for topic in study_topics:\n",
        "        video_links.append(request_video(topic, api_key))\n",
        "    return video_links\n",
        "\n",
        "video_data = get_related_videos(study_plan, userdata.get('YOUTUBE_API_KEY'))\n",
        "print(video_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2MO_HDk55teQ",
        "outputId": "813ba4bf-dace-4385-b8f7-83825bcf8da1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"make_df_for_videos(video_data)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Feedforward Neural Networks and Backpropagation\",\n          \"Deep Learning Project: Image Classification with CNN\",\n          \"Convolutional Neural Networks (CNNs) for Image Recognition\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Videos\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b45ff9df-3e8a-4225-a0cc-163e5eb2296b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Videos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Deep Learning Introduction &amp; Neural Networks</td>\n",
              "      <td>[https://www.youtube.com/watch?v=aircAruvnKk, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feedforward Neural Networks and Backpropagation</td>\n",
              "      <td>[https://www.youtube.com/watch?v=Ilg3gGewQ5U, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Convolutional Neural Networks (CNNs) for Image...</td>\n",
              "      <td>[https://www.youtube.com/watch?v=QzY57FaENXg, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recurrent Neural Networks (RNNs) for Sequentia...</td>\n",
              "      <td>[https://www.youtube.com/watch?v=AsNTP8Kwu80, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Deep Learning Project: Image Classification wi...</td>\n",
              "      <td>[https://www.youtube.com/watch?v=K_BHmztRTpA, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b45ff9df-3e8a-4225-a0cc-163e5eb2296b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b45ff9df-3e8a-4225-a0cc-163e5eb2296b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b45ff9df-3e8a-4225-a0cc-163e5eb2296b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-290411fd-9ea3-4c43-a22b-ea47e5f2c306\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-290411fd-9ea3-4c43-a22b-ea47e5f2c306')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-290411fd-9ea3-4c43-a22b-ea47e5f2c306 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Title  \\\n",
              "0       Deep Learning Introduction & Neural Networks   \n",
              "1    Feedforward Neural Networks and Backpropagation   \n",
              "2  Convolutional Neural Networks (CNNs) for Image...   \n",
              "3  Recurrent Neural Networks (RNNs) for Sequentia...   \n",
              "4  Deep Learning Project: Image Classification wi...   \n",
              "\n",
              "                                              Videos  \n",
              "0  [https://www.youtube.com/watch?v=aircAruvnKk, ...  \n",
              "1  [https://www.youtube.com/watch?v=Ilg3gGewQ5U, ...  \n",
              "2  [https://www.youtube.com/watch?v=QzY57FaENXg, ...  \n",
              "3  [https://www.youtube.com/watch?v=AsNTP8Kwu80, ...  \n",
              "4  [https://www.youtube.com/watch?v=K_BHmztRTpA, ...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def make_df_for_videos(video_data):\n",
        "  keys = []\n",
        "  values = []\n",
        "  for dic in video_data:\n",
        "    for k,v in dic.items():\n",
        "      keys.append(k)\n",
        "      values.append(v)\n",
        "  df = pd.DataFrame({'Title':keys,'Videos':values})\n",
        "  return df\n",
        "make_df_for_videos(video_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hLsoa0YwsW7S"
      },
      "outputs": [],
      "source": [
        "video_df = make_df_for_videos(video_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4CVMzRPbsW3h",
        "outputId": "09da075a-f340-44b9-cacb-38afffa0b35d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Ej7LqssWsr",
        "outputId": "ed8672e4-b948-4cd7-9286-fca9ebf1a25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 1000 words of transcript:\n",
            "[Music] we're no strangers to love you know the rules and so do I I full commitments while I'm thinking of you wouldn't get this from any other guy I just want to tell you how I'm feeling got to make you understand Never Going To Give You Up never going to let you down never going to run around and desert you never going to make you cry never going to say goodbye never going to tell a lie and hurt you we've known each other for so long your heart's been aching but your to sh to say it inside we both know what's been going on we know the game and we're going to playing and if you ask me how I'm feeling don't tell me you're too my you see Never Going To Give You Up never going to let you down never to run around and desert you never going to make you cry never going to say goodbye never going to tell a lie and hurt you never going to give you up never going to let you down never going to run around and desert you never going to make you cry never going to sing goodbye going to tell a lie and hurt you give you give you going to give going to give you going to give going to give you we've known each other for so long your heart's been aching but you're too sh to say inside we both know what's been going on we the game and we're going to play it I just want to tell you how I'm feeling got to make you understand Never Going To Give You Up never going to let you down never going to run around and desert you never going to make you cry never going to say goodbye never going to tell you my and Hurt You Never Going To Give You Up never going to let you down never going to run around and desert you never going to make you C never going to say goodbye never going to tell and Hur You Never Going To Give You Up never going to let you down never going to run around and desert you never going to make you going to [Music] goodbye and \n"
          ]
        }
      ],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "def extract_video_id(video_link):\n",
        "  \"\"\"Extracts the video ID from a YouTube video link.\"\"\"\n",
        "  video_id = video_link.split(\"v=\")[1]\n",
        "  return video_id\n",
        "\n",
        "def request_data_using_api(video_id):\n",
        "  \"\"\"Requests transcript data for a YouTube video using the YouTube Transcript API.\n",
        "\n",
        "  Handles potential exceptions like disabled subtitles and generic errors.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "  except Exception as e:\n",
        "    if \"Subtitles are disabled for this video\" in str(e):\n",
        "      transcript = 'Subtitles are disabled for this video'\n",
        "    else:\n",
        "      transcript = f'An unexpected error occurred: {e}'\n",
        "  return transcript\n",
        "\n",
        "def extract_text(transcript):\n",
        "  \"\"\"Extracts text from the transcript data.\n",
        "\n",
        "  If the transcript is a list, iterates through each dictionary and concatenates the text.\n",
        "  Handles cases where the transcript is not a list (e.g., error message).\n",
        "\n",
        "  Returns the first 1500 words of the extracted text and the full transcript.\n",
        "  \"\"\"\n",
        "  if isinstance(transcript, list):\n",
        "    video_text = ''\n",
        "    word_count = 0\n",
        "    for dictionary in transcript:\n",
        "      data = dictionary['text'].strip()  # Remove leading/trailing whitespace\n",
        "      words = data.split()\n",
        "      word_count += len(words)\n",
        "      video_text += ' '.join(words) + ' '  # Add space between sentences\n",
        "      if word_count >= 1000:\n",
        "        break\n",
        "    return video_text, transcript\n",
        "  else:\n",
        "    video_text = ''\n",
        "    return video_text, transcript\n",
        "\n",
        "def return_first_1000_words(video_link):\n",
        "  \"\"\"Returns the first 1000 words of the transcript text and the full transcript.\n",
        "\n",
        "  Calls the helper functions to extract video ID, request transcript data,\n",
        "  and extract the desired portion of the text.\n",
        "  \"\"\"\n",
        "  video_id = extract_video_id(video_link)\n",
        "  transcript = request_data_using_api(video_id)\n",
        "  first_1000_words, full_transcript = extract_text(transcript)\n",
        "  return first_1000_words, full_transcript\n",
        "\n",
        "# Example usage\n",
        "video_link = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"  # Replace with your video link\n",
        "first_1000_words, full_transcript = return_first_1000_words(video_link)\n",
        "\n",
        "print(f\"First 1000 words of transcript:\\n{first_1000_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFBvPxkV5iVP",
        "outputId": "76f6501d-805c-4416-faa1-657b309fdbd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Deep Learning Introduction & Neural Networks': [{'https://www.youtube.com/watch?v=aircAruvnKk': \"This is a 3. It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, but your brain has no trouble recognizing it as a 3. And I want you to take a moment to appreciate how crazy it is that brains can do this so effortlessly. I mean, this, this and this are also recognizable as 3s, even though the specific values of each pixel is very different from one image to the next. The particular light-sensitive cells in your eye that are firing when you see this 3 are very different from the ones firing when you see this 3. But something in that crazy-smart visual cortex of yours resolves these as representing the same idea, while at the same time recognizing other images as their own distinct ideas. But if I told you, hey, sit down and write for me a program that takes in a grid of 28x28 pixels like this and outputs a single number between 0 and 10, telling you what it thinks the digit is, well the task goes from comically trivial to dauntingly difficult. Unless you've been living under a rock, I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present and to the future. But what I want to do here is show you what a neural network actually is, assuming no background, and to help visualize what it's doing, not as a buzzword but as a piece of math. My hope is that you come away feeling like the structure itself is motivated, and to feel like you know what it means when you read, or you hear about a neural network quote-unquote learning. This video is just going to be devoted to the structure component of that, and the following one is going to tackle learning. What we're going to do is put together a neural network that can learn to recognize handwritten digits. This is a somewhat classic example for introducing the topic, and I'm happy to stick with the status quo here, because at the end of the two videos I want to point you to a couple good resources where you can learn more, and where you can download the code that does this and play with it on your own computer. There are many many variants of neural networks, and in recent years there's been sort of a boom in research towards these variants, but in these two introductory videos you and I are just going to look at the simplest plain vanilla form with no added frills. This is kind of a necessary prerequisite for understanding any of the more powerful modern variants, and trust me it still has plenty of complexity for us to wrap our minds around. But even in this simplest form it can learn to recognize handwritten digits, which is a pretty cool thing for a computer to be able to do. And at the same time you'll see how it does fall short of a couple hopes that we might have for it. As the name suggests neural networks are inspired by the brain, but let's break that down. What are the neurons, and in what sense are they linked together? Right now when I say neuron all I want you to think about is a thing that holds a number, specifically a number between 0 and 1. It's really not more than that. For example the network starts with a bunch of neurons corresponding to each of the 28x28 pixels of the input image, which is 784 neurons in total. Each one of these holds a number that represents the grayscale value of the corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels. This number inside the neuron is called its activation, and the image you might have in mind here is that each neuron is lit up when its activation is a high number. So all of these 784 neurons make up the first layer of our network. Now jumping over to the last layer, this has 10 neurons, each representing one of the digits. The activation in these neurons, again some number that's between 0 and 1, represents how much the system thinks that a given image corresponds with a given digit. There's also a couple layers in between called the hidden layers, which for the time being should just be a giant question mark for how on earth this process of recognizing digits is going to be handled. In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's kind of an arbitrary choice. To be honest I chose two layers based on how I want to motivate the structure in just a moment, and 16, well that was just a nice number to fit on the screen. In practice there is a lot of room for experiment with a specific structure here. The way the network operates, activations in one layer determine the activations of the next layer. And of course the heart of the network as an information processing mechanism comes down to exactly how those activations from one layer bring about activations in the next layer. It's meant to be loosely analogous to how in biological networks of neurons, some groups of neurons firing cause certain others to fire. Now the network I'm showing here has already been trained to recognize digits, and let me show you what I mean by that. It means if you feed in an image, lighting up all 784 neurons of the input layer according to the brightness of each pixel in the image, that pattern of activations causes some very specific pattern in the next layer which causes some pattern in the one after it, which finally gives some pattern in the output layer. And the brightest neuron of that output layer is the network's choice, so to speak, for what digit this image represents. \"}, {'https://www.youtube.com/watch?v=jmmW0F0biz0': \"Here are five things to know about neural networks in under five minutes. Number one: neural networks are composed of node layers. There is an input node layer, there is a hidden layer, and there is an output layer. And these neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI and deep learning. In fact, we should be describing this as an artificial neural network, or an ANN, to distinguish it from the very un-artificial neural network that's operating in our heads. Now, think of each node, or artificial neuron, as its own linear regression model. That's number two. Linear regression is a mathematical model that's used to predict future events. The weights of the connections between the nodes determines how much influence each input has on the output. So each node is composed of input data, weights, a bias, or a threshold, and then an output. Now data is passed from one layer in the neural network to the next in what is known as a feed forward network -- number three. To illustrate this, let's consider what a single node in our neural network might look like to decide -- should we go surfing. The decision to go or not is our predicted outcome or known as our yhat. Let's assume there are three factors influencing our decision. Are the wave's good, 1 for yes or 0 for no. The waves are pumping, so x1 equals 1, 1 for yes. Is the lineup empty, well unfortunately not, so that gets a 0. And then let's consider is it shark-free out there, that's x3 and yes, no shark attacks have been reported. Now to each decision we assign a weight based on its importance on a scale of 0 to 5. So let's say that the waves, we're going to score that one, eh, so this is important, let's give it a 5. And for the crowds, that's w2. Eh, not so important, we'll give that a 2. And sharks, well, we'll give that a score of a 4. Now we can plug in these values into the formula to get the desired output. So yhat equals (1 * 5) + (0 * 2) + (1 * 4), then minus 3, that's our threshold, and that gives us a value of 6. Six is greater than 0, so the output of this node is 1 -- we're going surfing. And if we adjust the weights or the threshold, we can achieve different outcomes. Number four. Well, yes, but but but number four, neural networks rely on training data to learn and improve their accuracy over time. We leverage supervised learning on labeled datasets to train the algorithm. As we train the model, we want to evaluate its accuracy using something called a cost function. Ultimately, the goal is to minimize our cost function to ensure the correctness of fit for any given observation, and that happens as the model adjusts its weights and biases to fit the training data set, through what's known as gradient descent, allowing the model to determine the direction to take to reduce errors, or more specifically, minimize the cost function. And then finally, number five: there are multiple types of neural networks beyond the feed forward neural network that we've described here. For example, there are convolutional neural networks, known as CNNs, which have a unique architecture that's well suited for identifying patterns like image recognition. And there are recurrent neural networks, or RNNs, which are identified by their feedback loops and RNNs are primarily leveraged using time series data to make predictions about future events like sales forecasting. So, five things in five minutes. To learn more about neural networks, check out these videos. Thanks for watching. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please Like and Subscribe. \"}, {'https://www.youtube.com/watch?v=q6kJ71tEYqM': \"look fair warning if you're feeling a little hungry right now you might want to pause this video and grab a snack before continuing because i'm going to explain the difference between machine learning and deep learning by talking about pizza delicious tasty pizza now before we get to that let's let's address the fundamental question here what is the difference between these two terms well put simply deep learning is a subset of machine learning actually the the hierarchy goes like this at the top we have a i or artificial intelligence now a subfield of a i is ml or machine learning beneath that then we have n n or neural networks and they make up the backbone of deep learning algorithms dl and here on the ibm technology channel we have a whole bunch of videos on these topics you might want to consider subscribing now machine learning algorithms leverage structured labeled data to make predictions so let's build one a model to determine whether we should order pizza for dinner there are three main factors that influence that decision so let's map those out as inputs the first of those inputs we'll call x1 and x1 asks will it save time by ordering out we can say yes with a one or no with a zero yes it will so x that equals one now x two that input says will i lose weight by ordering pizza that's a zero i'm i'm ordering all the toppings and x3 will it save me money actually i have a coupon for a free pizza today so that's a one now look these binary responses ones and zeros i'm using them for simplicity but neurons in a network can represent values from well everything to everything negative infinity to positive infinity with our inputs defined we can assign weights to determine importance larger weights make a single inputs contribution to the output more significant compared to other inputs now my threshold here is five so let's weight each one of these w1 well i'm going to give this a full five because i value my time and w2 this was the will i lose weight 1 i'm going to rate this a 3 because i have some interest in keeping in shape and for w3 i'm going to give this a 2 because like either way this isn't going to break the bank to order dinner now we plug these weights into our model and using an activation function we can calculate the output which in this case is the decision to order pizza or not so to calculate that we're going to calculate the y hat and we're going to use these weights and these inputs so here we've got 1 times 5 we've got 0 times 3 and we've got 1 times 2. and we need to consider as well our threshold which was 5. so that gives us if we just add these up 1 times 5 that's 5 plus 0 times 3 that's 0 plus 1 times 2 that's 2 minus 5. well that gives us a total of positive 2. and because the output is a positive number this correlates to pizza night okay so that's machine learning but what differentiates deep learning well the answer to that is more than three as in a neural network is considered a deep neural network if it consists of more than three layers and that includes the input and the output layer so we've got our input and output we have multiple layers in the middle and this would be considered a deep learning network classical machine learning is more dependent on human intervention to learn human experts well they determine a hierarchy of features to understand the differences between data inputs so if i showed you a series of images of different types of fast food like pizza burger and taco you could label these in a data set for processing by the neural network a human expert here has determined the characteristics which distinguish each picture as the specific fast food type so for example it might be the bread of each food type might be a distinguishing feature across each picture now this is known as supervised learning because the process incorporates human intervention or human supervision deep machine learning doesn't necessarily require a labeled data set it can ingest unstructured data in its raw form like text and images and it can automatically determine the set of features which distinguish pizza burger and taco from one another by observing patterns in the data a deep learning model can cluster inputs appropriately these algorithms discover hidden patterns of data groupings without the need for human intervention and they're known as unsupervised learning most deep neural networks are feed forward that means that they go in one direction from the input to the output however you can also train your model through something called a back propagation that is it moves in the opposite direction from output to input back propagation allows us to calculate and attribute the error associated with each neuron and allows us to adjust and fit the algorithm appropriately so when we talk about machine learning and deep learning we're essentially talking about the same field of study neural networks they're the foundation of both types of learning and both are considered subfields of a i the main distinction between the two are that number of layers in a neural network more than three and whether or not human intervention is required to label data pizza burgers tacos yeah that's uh that's enough for today it's time for lunch oh oh and before i go if you did enjoy this video here are some others you might also like if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching \"}, {'https://www.youtube.com/watch?v=oV3ZY6tJiA0': 'Hi, I’m Jabril, and welcome to CrashCourse AI! In the supervised learning episode, we taught John Green-bot to learn using a perceptron, a program that imitates one neuron. But our brains make decisions with 100 billion neurons, which have trillions of connections between them! We can actually do a lot more with AI if we connect a bunch of perceptrons together, to create what’s called an artificial neural network. Neural networks are better than other methods for certain tasks like, image recognition. The secret to their success is their hidden layers, and they’re mathematically very elegant. Both of these reasons are why neural networks are one of the most dominant machine learning technologies used today. [INTRO] Not that long ago, a big challenge in AI was real-world image recognition, like recognizing a dog from a cat, and a car from a plane from a boat. Even though we do it every day, it’s really hard for computers. That’s because computers are good at literal comparisons, like matching 0s and 1s, one at a time. It’s easy for a computer to tell that these images are the same by matching the pixels. But before AI, a computer couldn’t tell that these images are of the same dog, and had no hope of telling that all of these different images are dogs. So, a professor named Fei-Fei Li and a group of other machine learning and computer vision researchers wanted to help the research community develop AI that could recognize images. The first step was to create a huge public dataset of labeled real-world photos. That way, computer scientists around the world could come up with and test different algorithms. They called this dataset ImageNet. It has 3.2 million labeled images, sorted into 5,247 nested categories of nouns. Like for example, the “dog” label is nested under “domestic animal,” which is nested under “animal.” Humans are the best at reliably labeling data. But if one person did all this labeling, taking 10 seconds per label, without any sleep or snack breaks, it would take them over a year! So ImageNet used crowd-sourcing and leveraged the power of the Internet to cheaply spread the work between thousands of people. Once the data was in place, the researchers started an annual competition in 2010 to get people to contribute their best solutions to image recognition. Enter Alex Krizhevsky, who was a graduate student at the University of Toronto. In 2012, he decided to apply a neural network to ImageNet, even though similar solutions hadn’t been successful in the past. His neural network, called AlexNet, had a couple of innovations that set it apart. He used a lot of hidden layers, which we’ll get to in a minute. He also used faster computation hardware to handle all the math that neural networks do. AlexNet outperformed the next best approaches by over 10%. It only got 3 out of every 20 images wrong. In grade terms, it was getting a solid B while other techniques were scraping by with a low C. Since 2012, neural network solutions have taken over the annual competition, and the results keep getting better and better. Plus, AlexNet sparked an explosion of research into neural networks, which we started to apply to lots of things beyond image recognition. To understand how neural networks can be used for these classification problems, we have to understand their architecture first. All neural networks are made up of an input layer, an output layer, and any number of hidden layers in between. There are many different arrangements but we’ll use the classic multi-layer perceptron as an example. The input layer is where the neural network receives data represented as numbers. Each input neuron represents a single feature, which is some characteristic of the data. Features are straightforward if you’re talking about something that’s already a number, like grams of sugar in a donut. But, really, just about anything can be converted to a number. Sounds can be represented as the amplitudes of the sound wave. So each feature would have a number that represents the amplitude at a moment in time. Words in a paragraph can be represented by how many times each word appears. So each feature would have the frequency of one word. Or, if we’re trying to label an image of a dog, each feature would represent information about a pixel. So for a grayscale image, each feature would have a number representing how bright a pixel is. But for a color image, we can represent each pixel with three numbers: the amount of red, green, and blue, which can be combined to make any color on your computer screen. Once the features have data, each one sends its number to every neuron in the next layer, called the hidden layer. Then, each hidden layer neuron mathematically combines all the numbers it gets. The goal is to measure whether the input data has certain components. For an image recognition problem, these components may be a certain color in the center, a curve near the top, or even whether the image contains eyes, ears, or fur. Instead of answering yes or no, like the simple Perceptron from the previous episode, each neuron in the hidden layer does some slightly more complicated math and outputs a number. And then, each neuron sends its number to every neuron in the next layer, which could be another hidden layer or the output layer. The output layer is where the final hidden layer outputs are mathematically combined to answer the problem. So, let’s say we’re just trying to label an image as a dog. We might have a single output neuron representing a single answer - that the image is of a dog or not. But if there are many answers, like for example if we’re labeling a bunch of images, we’ll need a lot of output neurons. Each output neuron will correspond to the probability for each label -- like for example, dog, car, spaghetti, and more. '}, {'https://www.youtube.com/watch?v=CqOfi41LfDw': \"Neural networks... seem so complicated, but they're not! StatQuest! Hello! I'm Josh Starmer and welcome to StatQuest! Today, we're going to talk about neural networks, part one: inside the black box! Neural networks, one of the most popular algorithms in machine learning, cover a broad range of concepts and techniques. however, people call them a black box because it can be hard to understand what they're doing. the goal of this series is to take a peek into the black box by breaking down each concept and technique into its components and walking through how they fit together, step by step. in this first part, we will learn about what neural networks do, and how they do it. in part two, we'll talk about how neural networks are fit to data with backpropagation. then, we will talk about variations on the simple neural network presented in this part, including deep learning. note: crazy awesome news! i have a new way to think about neural networks that will help beginners and seasoned experts alike gain a deep insight into what neural networks do. for example, most tutorials use cool looking, but hard to understand graphs, and fancy mathematical notation to represent neural networks. in contrast, i'm going to label every little thing on the neural network to make it easy to keep track of the details. and the math will be as simple as possible, while still being true to the algorithm. these differences will help you develop a deep understanding of what neural networks actually do. so, with that said, let's imagine we tested a drug that was designed to treat an illness and we gave the drug to three different groups of people, with three different dosages: low, medium, and high. the low dosages were not effective so we set them to zero on this graph. in contrast, the medium dosages were effective so we set them to one. and the high dosages were not effective, so those are set to zero. now that we have this data, we would like to use it to predict whether or not a future dosage will be effective. however we can't just fit a straight line to the data to make predictions, because no matter how we rotate the straight line, it can only accurately predict two of the three dosages. the good news is that a neural network can fit a squiggle to the data. the green squiggle is close to zero for low dosages, close to one for medium dosages, and close to zero for high dosages. and even if we have a really complicated dataset like this, a neural network can fit a squiggle to it. in this StatQuest we're going to use this super simple dataset and show how this neural network creates this green squiggle. but first, let's just talk about what a neural network is. a neural network consists of nodes and connections between the nodes. note: the numbers along each connection represent parameter values that were estimated when this neural network was fit to the data. for now, just know that these parameter estimates are analogous to the slope and intercept values that we solve for when we fit a straight line to data. likewise, a neural network starts out with unknown parameter values that are estimated when we fit the neural network to a dataset using a method called backpropagation. and we will talk about how backpropagation estimates these parameters in part 2 in this series. but, for now, just assume that we've already fit this neural network to this specific dataset, and that means we have already estimated these parameters. also, you may have noticed that some of the nodes have curved lines inside of them. these bent or curved lines are the building blocks for fitting a squiggle to data. the goal of this StatQuest is to show you how these identical curves can be reshaped by the parameter values and then added together to get a green squiggle that fits the data. note: there are many common bent or curved lines that we can choose for a neural network. this specific curved line is called soft plus, which sounds like a brand of toilet paper. alternatively, we could use this bent line, called ReLU, which is short for rectified linear unit, and sounds like a robot. or, we could use a sigmoid shape, or any other bent or curved line. oh no! it's the dreaded terminology alert! the curved or bent lines are called activation functions. when you build a neural network you have to decide which activation function, or functions, you want to use. when most people teach neural networks they use the sigmoid activation function. however, in practice, it is much more common to use the ReLU activation function, or the soft plus activation function. so we'll use the soft plus activation function in this StatQuest. anyway, we'll talk more about how you choose activation functions later in this series. note: this specific neural network is about as simple as they get. it only has one input node, where we plug in the dosage, only one output node to tell us the predicted effectiveness, and only two nodes between the input and output nodes. however, in practice, neural networks are usually much fancier and have more than one input node, more than one output node, different layers of nodes between the input and output nodes, and a spider web of connections between each layer of nodes. oh no! it's another terminology alert! these layers of nodes between the input and output nodes are called hidden layers. when you build a neural network one of the first things you do is decide how many hidden layers you want and how many nodes go into each hidden layer. although there are rules of thumb for making decisions about the hidden layers, you essentially make a guess and see how well the neural network performs, adding more layers and nodes if needed. \"}], 'Feedforward Neural Networks and Backpropagation': [{'https://www.youtube.com/watch?v=Ilg3gGewQ5U': \"Here, we tackle backpropagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. We can't directly change those activations, we only have influence on the weights and biases. But it's helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. So if you were to increase one of those weights, \"}, {'https://www.youtube.com/watch?v=S5AGN9XfPK4': 'We\\'re going to take a look at back propagation. It\\'s central to the functioning of neural networks, helping them to learn and adapt. And we\\'re going to cover it in simple but instructive terms. So even if your only knowledge of neural networks is \"Isn\\'t that something to do with chatGPT?\" Well, we\\'ve got you covered. Now, a neural network fundamentally comprises multiple layers of neurons interconnected by weights. So I\\'m going to draw some neurons here, and I\\'m organizing them in layers. And these neurons are also known as nodes. Now, the layers here are categorized. So that\\'s let\\'s do that, the categorization. We have a layer here called the input layer. These two layers in the middle here are the hidden layer and the layer on the end here, that is the output layer. And these neurons are all interconnected with each other across the layers. So each neuron is connected to each other neuron in the next layer. So you can see that here. Okay, so now we have our basic neural network. And during a process called forward propagation, the input data traverses through these layers where the weights, biases and activation functions transform the data until an output is produced. So, let\\'s define those terms. Weights, what is that when we\\'re talking about a neural network? Well, the weights define the strength of the connections between each of the neurons. Then we have the activation function, and the activation function is applied to the weighted sum of the inputs at each neuron to introduce non-linearity into the network, and that allows it to make complex relationships. And that\\'s really where we can use activation functions. Commonly, you\\'ll see activation functions used such as sigmoid, for example. And then finally, biases. So biases really are the additional parameter that shift the activation function to the left or the right, and that aids the network\\'s flexibility. So, consider a single training instance with its associated input data. Now, this data propagates forward through the network, causing every neutron to calculate a weighted sum of the inputs, which is then passed through its activation function. And the final result is the network\\'s output. Great! So where does back propagation come in? Well, the initial output might not be accurate. The network needs to learn from its mistakes and adjust its weights to improve. And back propagation is essentially an algorithm used to train neural networks, applying the principle of error correction. So, after forward propagation, the output error, which is the difference between the network\\'s output and the actual output, is computed. Now that\\'s something called a loss function. And the error is distributed back through the network, providing each neuron in the network a measure of its contribution to total error. Using these measures, back propagation adjusts the weights and the biases of the network to minimize that error. And the objective here is to improve the accuracy of the network\\'s output during subsequent forward propagation. It\\'s a process of optimization, often employing a technique known as gradient descent. Now, gradient descent, that\\'s the topic of a whole video of its own, but essentially, gradient descent is an algorithm used to find the optimal weights and biases that minimize the lost function. It iteratively adjusts the weights and biases in the direction that reduces the error most rapidly. And that means the steepest descent. Now, back propagation is widely used in many neural networks. So let\\'s consider a speech recognition system. We provide as input a spoken word, and it outputs a written transcript of that word. Now, if during training our spoken inputs, it turns out that it doesn\\'t match the written outputs, then back propagation may be able to help. Look, I speak with a British accent, but I\\'ve lived in the US for years. But when locals here ask for my name-- Martin --they often hear it as something different entirely, like Marvin or Morton or Mark. If this neural network had made the same mistake, we\\'d calculate the error by using the loss function to quantify the difference between the predicted output \"Marvin\" and the actual output \"Martin\". We\\'d compute the gradient of the loss function with respect to the weight and biases in the network and update the weighting biases in the network accordingly. Then we\\'d undergo multiple iterations of forward propagation and back propagation, tinkering with those weights and biases until we reach convergence-- a time where the network could reliably translate Martin into M-A-R-T-I-N. This is can\\'t be applied to people, can it? Well, but anyway, let\\'s just talk about one more thing with back propagation, and that\\'s the distinction between static and recurrent back propagation networks. Let\\'s start with static. So static back propagation is employed in a feed-forward neural networks where the data moves in a single direction from input layer to output layer. Some example use cases of this, well, we can think of OCR, or optical character recognition, where the goal is to identify and classify the letters and numbers in a given image. Another common example is with spam detection, and here we are looking to use a neural network to learn from features such as the emails, content and the sender\\'s email address to classify an email as spam or not spam. Now back propagation can also be applied to recurrent neural networks as well, or RNNs. Now these networks have loops, and this type of back propagation is slightly more complex given the recursive nature of these networks. Now, some use cases? If we think about sentiment analysis, that\\'s a common use case for this. And that\\'s a good example of where RNNs are used to analyze the sentiment of a piece of text, like a customer product review. Another good example is time series prediction. So predicting things like stock prices or weather patterns. Ultimately, back propagation is the backbone of the learning in neural networks. It tests for errors, working its way back from the output layer to the input layer, '}, {'https://www.youtube.com/watch?v=y0wNuFFPGuI': 'So, we are going to continue this session into the next stage where we want to solve different kinds of problems. Earlier we were only talking about linearly separable problems and then how we can use perceptron to solve those kinds of problems, but in the real situation we get into various data sets you know speech. So, let us first talk about these speech, the one that I am talking right now is recorded and then I want to be able to recognize what I am talking. For that first of all I need to find out where is my word boundary in all of those and then take the signal values related to that of word and then process it in some way. And then finally, say that what I am saying is this let us say speech is one word ok. In the same fashion you know you create various word boundaries and then identify each word. And finally, take it to the application where you would like to say for example, the Alexa or the Google home or Siri or whatever you know those applications are actually listening to your speech and then figuring out what you are really talking with respect to the string of words. And finally, convert this into the sentence, maybe you search the web or you know tries to figure out what exactly you are asking for. And finally, list those item that you are really looking for right in some fashion. So, this is one application where the data is coming in a sequential fashion. So, I should be able to take these as the input and finally, do something with that. And, then documents we spoke about this several time you know, we been talking about tokenizing the documents and then using the word as one element for us to process, but that is not going to be the case all the time right. So, we need to be able to understand a sentence, for you to understand a sentence we need to have the string of words. And, then using those string of words you should be able to understand that as a sentence and then what that sentence means and then finally, provide what that actually is asking. For example, if that sentence is a question asking the system to find out a various documents related to natural language processing right; related to natural language processing. So, the data is coming in as sequence of words and this contains both audio, video and so on. So, both speech and the images are coming in as a sequence. So, we should be able to process it and then provide the necessary output. It is a weather forecast again based on the historical information; it is not an isolated incident. The same with the stock market deals with at least several different variables based on which you predict what is going to be the stock price as of today for a given scrip ok. So, how do you process these? Ok. So, before that you know we also need to mention a few things about the artificial neural networks. As we had seen earlier right it is possible to process these values for example, the computation of this the x 1 W 1 1. So, these are all independent of each one of those. So, you can see that there is a massive parallelism in this. So, it is possible to utilize the parallel processing to really compute all these values independently and then later combine them here right. And, then we also had seen that it is possible to learn from the training samples. We were able to really figure out that even though we fed about 5,000, 6,000 sentiment words, the weights are adjusted in such a way that it really generalized the weights for all the input sentiment words right. It is not related to just one word, the weights are not related to just one word; it is related to all words that we had input. So that means, it has generalized the weight, it also figures out the latent patterns in the data. We will talk about this and finally, it generalizes and associates the data sets in some fashion like I mentioned earlier ok; going back to this. So, in order for us to identify those patterns we require the neural net. For example so, why this scrip is down today, is there any similar situation that I am able to find out from the historical information? Since, the number of parameters are huge in order for you to find that out, we want to use the neural network which really is good in terms of identifying those patterns which are inherent in the data set. The same fashion I want to find out whether it is going to rain today or not because, of these kinds of input parameter that I am having today with respect to the temperature, humidity all that and the pressure and so on. Is there any similar situation that existed earlier that caused rain? Then we can say with some probability that today it may rain and so on right. So, for that again we require lots of data set and system, if it is trained using the neural network it should be able to figure out those patterns in the historical information. And, then give you with some value that you can use it to predict, same with the videos you know the data is sequential in nature ok. So, for all of this we need a neural net of different type than what we had seen in the perceptron case ok. So, we will talk about those one by one in these subsequent lectures. So, first we will talk about artificial neural network, where we have a feed forward model and back propagation model. The feed forward is something that we had seen earlier in the '}, {'https://www.youtube.com/watch?v=jTzJ9zjC8nU': 'In this video we look at a general version of the neural network called the feedforward network. In the previous videos we had seen what an artificial neuron was. A feedforward network or a simple neural network, the term that you would have heard most commonly is basically a collection of neurons. Each of these units here is neuron. Now each of these neurons or each of these layers which are vertically concatenated have specific name. The very first layer is called the input layer. We had seen this during logistic regression even in linear regression. So you will have multiple features. This is the input vector. The intermediate layers here are called hidden layers. You could have multiple hidden layers. If the number of hidden layers is greater than 1 then it is called the deep network. Hence the name deep learning. So deep network is simply a network with the number of hidden layers greater than 1. The final layer where you actually get the output you are interested in is called output layer. So you have our predictions. We have our predictions here, y1 hat (remember hat is used for predictions), uptil whatever is the number of classes that we are predicting for. Remember k in general need not be equal to n and in general each layer might have a different size. So these are the elements. So each of these elements here is an artificial neuron, Ok. Technically speaking we can even treat the input layers as if they were neurons, but generally it is only after the input layer that we look at each of these neurons and call them an artificial neuron. Remember that within each neuron we have 2 portions. We have a linear combination and we have nonlinear activation function sitting there. Now if I look at any neuron here, so for example this neuron it has inputs coming from all the previous entities in the input layer, Ok. So for example, so this neuron here has n inputs plus, even though not explicitly shown here, you will have a bias unit which will be coming in here. So for this neuron you have n weights from the input layer. So let us take a general neuron or a general set of neurons in some hidden layer. So let us say this is layer l, Ok for example this one would be layer 1. This one would be layer 2, hidden layer 1, hidden layer 2. Let us say we have layer l where all these neurons are there. And you have a prior layer. This is layer l minus 1. Let us say further that this layer had n neurons and this layer l has m neurons. So this means the total number of weights required, assuming every neuron is connected to every other neuron would be n times m plus the bias units. Now how many bias units do we have in such a case, Ok? Now if I consider this neuron for example, this takes input from all these n, plus one bias. Now if I take this one, it also takes all these n, plus a different bias. So each neuron gets a different bias. So in this case if we look at these m neurons, you have n times m which are normal weights, the number of bias weights will be equal to m, because each of these neurons has a different bias. So the total number of weights in such a case is nm plus m. In a feedforward network, all you need to do is you give all these xi’s, if you give x vector and all the weights in every layer, we can find out y hat vector. So this is called the feedforward process. Basically you are feeding the x, you also give all the ws of every single layer here and simply by taking a linear combination, nonlinearity; linear combination, nonlinearity; linear combination, nonlinearity; you can predict all the ys. This is called the feedforward process. Such a network where all neurons are connected to every other neuron are called fully connected networks. In the general case you need not have all connections active. In fact we will see later in convolutional neural networks that we have only some of these weights which are non-zero and most of them are 0, which means each neuron is connected only to a few other neurons in the previous layer. So that would be the special case but in the most general case you can think of a fully connected network, sometimes simply called FC network. The assumption behind the feedforward process is you know all the weights. And you know all the input neurons. Later on we will see when we come to back propagation how to actually determine these weights. '}, {'https://www.youtube.com/watch?v=CqOfi41LfDw': \"Neural networks... seem so complicated, but they're not! StatQuest! Hello! I'm Josh Starmer and welcome to StatQuest! Today, we're going to talk about neural networks, part one: inside the black box! Neural networks, one of the most popular algorithms in machine learning, cover a broad range of concepts and techniques. however, people call them a black box because it can be hard to understand what they're doing. the goal of this series is to take a peek into the black box by breaking down each concept and technique into its components and walking through how they fit together, step by step. in this first part, we will learn about what neural networks do, and how they do it. in part two, we'll talk about how neural networks are fit to data with backpropagation. then, we will talk about variations on the simple neural network presented in this part, including deep learning. note: crazy awesome news! i have a new way to think about neural networks that will help beginners and seasoned experts alike gain a deep insight into what neural networks do. for example, most tutorials use cool looking, but hard to understand graphs, and fancy mathematical notation to represent neural networks. in contrast, i'm going to label every little thing on the neural network to make it easy to keep track of the details. and the math will be as simple as possible, while still being true to the algorithm. these differences will help you develop a deep understanding of what neural networks actually do. so, with that said, let's imagine we tested a drug that was designed to treat an illness and we gave the drug to three different groups of people, with three different dosages: low, medium, and high. the low dosages were not effective so we set them to zero on this graph. in contrast, the medium dosages were effective so we set them to one. and the high dosages were not effective, so those are set to zero. now that we have this data, we would like to use it to predict whether or not a future dosage will be effective. however we can't just fit a straight line to the data to make predictions, because no matter how we rotate the straight line, it can only accurately predict two of the three dosages. the good news is that a neural network can fit a squiggle to the data. the green squiggle is close to zero for low dosages, close to one for medium dosages, and close to zero for high dosages. and even if we have a really complicated dataset like this, a neural network can fit a squiggle to it. in this StatQuest we're going to use this super simple dataset and show how this neural network creates this green squiggle. but first, let's just talk about what a neural network is. a neural network consists of nodes and connections between the nodes. note: the numbers along each connection represent parameter values that were estimated when this neural network was fit to the data. for now, just know that these parameter estimates are analogous to the slope and intercept values that we solve for when we fit a straight line to data. likewise, a neural network starts out with unknown parameter values that are estimated when we fit the neural network to a dataset using a method called backpropagation. and we will talk about how backpropagation estimates these parameters in part 2 in this series. but, for now, just assume that we've already fit this neural network to this specific dataset, and that means we have already estimated these parameters. also, you may have noticed that some of the nodes have curved lines inside of them. these bent or curved lines are the building blocks for fitting a squiggle to data. the goal of this StatQuest is to show you how these identical curves can be reshaped by the parameter values and then added together to get a green squiggle that fits the data. note: there are many common bent or curved lines that we can choose for a neural network. this specific curved line is called soft plus, which sounds like a brand of toilet paper. alternatively, we could use this bent line, called ReLU, which is short for rectified linear unit, and sounds like a robot. or, we could use a sigmoid shape, or any other bent or curved line. oh no! it's the dreaded terminology alert! the curved or bent lines are called activation functions. when you build a neural network you have to decide which activation function, or functions, you want to use. when most people teach neural networks they use the sigmoid activation function. however, in practice, it is much more common to use the ReLU activation function, or the soft plus activation function. so we'll use the soft plus activation function in this StatQuest. anyway, we'll talk more about how you choose activation functions later in this series. note: this specific neural network is about as simple as they get. it only has one input node, where we plug in the dosage, only one output node to tell us the predicted effectiveness, and only two nodes between the input and output nodes. however, in practice, neural networks are usually much fancier and have more than one input node, more than one output node, different layers of nodes between the input and output nodes, and a spider web of connections between each layer of nodes. oh no! it's another terminology alert! these layers of nodes between the input and output nodes are called hidden layers. when you build a neural network one of the first things you do is decide how many hidden layers you want and how many nodes go into each hidden layer. although there are rules of thumb for making decisions about the hidden layers, you essentially make a guess and see how well the neural network performs, adding more layers and nodes if needed. \"}], 'Convolutional Neural Networks (CNNs) for Image Recognition': [{'https://www.youtube.com/watch?v=QzY57FaENXg': \"OK, pop quiz. What am I drawing? I'm going to make three predictions here. Firstly. You think at your house, you'd be right? Secondly, that that just came pretty easily to you, it was effortless. And thirdly, you're thinking that I'm not much of an artist and you'd be right on all counts there. But how can we look at this set of geometric shapes and think, Oh, how? If you live in a house, I bet it looks nothing like this. Well, that ability to perform object identification that comes so easily to us does not come so easily to a computer, but that is where we can apply something called convolutional neural networks to the problem. Now, a convolutional neural network or a. See, and and. Is a area of deep learning that specializes in pattern recognition. My name is Martin Keane, and I work in the IBM garage at IBM. Now let's take a look at how CNN works at a high level. Well, let's break it down. CNN convolutional neural network Well, let's start with the artificial neural network part. This is a standard network that consists of multiple layers that are interconnected, and each layer receives some input. Transforms that input to something else and passes an output to the next layer, that's how neural networks work and see an end is a particular part of the neural network or a section of layers that say it's these three layers here and within these layers, we have something called filters. And it's the filters that perform the pattern recognition that CNN is so good at. So let's apply this to our house example now. If this house were an actual image, it would be a series of pixels, just like any image. And if we zoom in on a particular part of this house, let's say we zoom in around here, then we would get, well, the window. And what we're saying here is that a window consists of some perfectly straight lines. Almost perfectly straight lines. But, you know, a window doesn't need to look like that window could equally look like this, and we would still say it was a window. The cool thing about CNN is that using filters. CNN could also say that these two objects represent the same thing. The way they do that, then, is through the application of these filters. So let's take a look at how that works. Now, a filter is basically just a three by three block. And within that block, we can specify a pattern to look for. So we could say, let's look for. Pattern like this, a right angle in our image. So what we do is we take this filter and it's a three by three block here. We will analyze the equivalent three by three block up here as well. So. We'll look at first of all, these first. Group of three by three pixels, and we will see how close are they to the filter shape? And we'll get that numeric score, then we will move across one, come to the right and look at the next three by three block of pixels and score how close they are to the filter shape. And we will continue to slide over or vote over all of these pixel layers until we have not every three by three block. Now, that's just for one filter. But what that will give us is an array of numbers that say how closely and the image matches filter, but we can add more filters so we could add another three by three filter here. And perhaps this one looks for a shape like this. And we could add a third filter here, and perhaps this looks for a different kind of right angle shape. If we take the numeric arrays from all of these filters and combine them together in a process called pooling, then we have a much better understanding of what is contained within this series of pixels. Now that's just the first layer of the CNN. And as we go deeper into the neural network, the filters become more abstract all they can do more. So the second layer of filters perhaps can perform tasks like basic object recognition. So we can have filters here that might be able to recognize the presence of a window or the presence of a door or the presence of a roof. And as we go deeper into the sea and into the next leg, well, maybe these filters can perform even more abstract tasks, like being able to determine whether we're looking at a house or we're looking at an apartment or whether we're looking at a skyscraper. So you can see the application of these filters increases as we go through the network and can perform more and more tasks. And that's a very high level basic overview of what CNN is. It has a ton of business applications. Think of OCR, for example, for understanding handwritten documents. Think of visual recognition and facial detection and visual search. Think of medical imagery and looking at that and determining what is being shown in an imaging scan. And even think of the fact that we can apply a CNN to perform object identification for. Body drawn houses, if you have any questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe. Thanks for watching. \"}, {'https://www.youtube.com/watch?v=K_BHmztRTpA': \"(light music) Narrator: What is a convolutional neural network? Let's start with the basics. A convolutional neural network is a type of neural network that is most often applied to image processing problems. You've probably seen them in action anywhere a computer is identifying objects in an image, but you can also use convolutional neural networks in natural language processing projects too. The fact that they are useful for these fast-growing areas is one of the main reasons they're so important in deep learning and artificial intelligence today. Once you understand how a convolutional neural network works and what makes it unique from other neural networks, you can see why they're so effective for processing and classifying images. But let's first take a regular neural network. A regular neural network has an input layer, hidden layers, and an output layer. The input layer accepts inputs in different forms while the hidden layers perform calculations on these inputs. The output layer then delivers the outcome of the calculations and extractions. Each of these layers contain neurons that are connected to neurons in the previous layer and each neuron has its own weight. This means you aren't making any assumptions about the data being fed into the network. Great usually but not if you're working with images or a language. Convolutional neural networks work differently as they treat data as spatial. Instead of neurons being connected to every neuron in the previous layer, they are instead only connected to neurons close to it and all have the same weight. The simplification in the connections means the network upholds the spatial aspect of the data set. It means your network doesn't think an eye is all over the image. The word convolutional refers to the filtering process that happens in this type of network. Think of it this way, an image is complex. A convolutional neural network simplifies it, so it can be better processed and understood. Let's look now at what's inside a convolutional neural network. Like a normal neural network, a convolutional neural network is made up of multiple layers. There are a couple of layers that make it unique, the convolutional layer and the pooling layer. However, like other neural networks, it will also have a ReLU or a rectified linear unit layer and a fully connected layer. The ReLU layer acts as an activation function, ensuring non-linearity as the data moves through each layer in the network. Without it, the data being fed into each layer would lose the dimensionality that we want to maintain. The fully connected layer meanwhile allows you to perform classification on your data set. The convolutional layer is the most important, so let's start there. It works by placing a filter over an array of image pixels. This then creates what's called a convolved feature map. It's a bit like looking at an image through a window, which allows you to see specific features you might not otherwise be able to see. Next, we have the pooling layer. This downsamples or reduces the sample size of a particular feature map. This also makes processing much faster as it reduces the number of parameters the network needs to process. The output of this is a pooled feature map. There are two ways of doing this: max pooling, which takes the maximum input of a particular convolved feature, or average pooling, which simply takes the average. These steps amount to feature extraction whereby the network builds up a picture of the image data according to its own mathematical rules. If you want to perform classification, you'll need to move into the fully connected layer. To do this, you'll need to flatten things out. Remember, a neural network with a more complex set of connections can only process linear data. There are a number of ways you can train a convolutional neural network. If you're working with unlabeled data, you can use unsupervised learning methods. One of the best popular ways of doing this is using autoencoders. This allows you to squeeze data in a space with low dimensions, performing calculations in the first part of the convolutional neural network. Once this is done, you'll then need to reconstruct with additional layers that upsample the data you have. Another option is to use generative adversarial networks or GANs. With a GAN, you train two networks. The first gives you artificial data samples that should resemble data in the training set while the second is a discriminative network. It should distinguish between the artificial and the true model. A lot of people seem to be asking what the difference is between a convolutional neural network and a recurrent network. It's actually quite simple. Whereas a convolutional neural network is a feedforward network that filters spatial data, a recurrent neural network, as the name implies, feeds data back into itself. From this perspective, recurrent neural networks are better suited to sequential data. Think of it like this, a convolutional network is able to perceive patterns across space, a recurrent neural network can see them over time. If you want to get started with convolutional neural networks, Python and TensorFlow are great tools to begin with. It's worth exploring MNIST data set too. This is a database of handwritten digits that you can use to get started with building your first convolutional neural network. If you want to learn more about convolutional neural networks, deep learning, and AI, visit Packt today. \"}, {'https://www.youtube.com/watch?v=pj9-rr1wDhM': \"To support the production of more high quality content, consider supporting us on Patreon or a YouTube membership. Additionally, consider visiting our parent company EarthOne, for sustainable living made simple. Throughout this deep learning series, we have gone from the origins of the field and how the structure of the artificial neural network was conceived to working through an intuitive example, covering the main aspects and some of the many complexities of deep learning. Now all of these videos have only been focused on one type of neural network, the feedforward network. The focus of this video then will be to initiate discussion on another very popular and important neural network architecture, the convolutional neural network. For our discussions on convolutional neural networks, we will go with a very common example, number recognition. The reason this example was chosen is because of this great interactive resource by Adam Harley, a robotics PhD at Carnegie Mellon University. I have linked the resource in the description so that you too can experiment with it and see the internals of a convolutional network. Now, if you can recall from our example on an image pattern recognizer using a feedforward network, we set up an idealized system and made the assumption that each hidden layer in our network would build upon further layers of abstraction, from vertical lines to combinations of vertical lines. However, in actuality, as we explained, this wouldn't happen, and the receptive fields in the network would be a lot more random to the human eye due to the architecture of feedforward networks and how they compute. With convolutional networks, however, this isn't the case, and we could to some extent see these layers of abstraction building up. Before we delve into why and how this happens, let's first set up a structure for the network we will be using. As with our previous example in image recognition, the input will be the individual pixels of our image and the output the patterns we are trying to classify. In this example's case, we have 10 outputs, numbers zero to nine. For the layers in between, we will have two convolutional layers, two pooling layers, and two fully connected layers. Now, that was a lot of new terminology thrown around. So let's break it down layer by layer, starting with the input. While we have already stated that the input is the individual pixels of the image, I want to mention some important details about how a computer views this image. Every image in a digital device is stored as a matrix of pixel values. This is referred to as a channel, a certain component of an image. Now, with a typical digital camera, every image will have three channels, red, green, and blue, RGB, which you can imagine as three 2D matrices stacked upon one another. For the sake of simplicity, we will assume our input has just one channel, the luminance of the image, with the value of each pixel represented in eight bits. In other words, the pixel values will range from zero to 255, with zero indicating darker transparent luminance and 255 as bright. As a side note, digital systems store all types of sensor data and other information in this fashion. For example, for a convolutional network that would operate on speech, we could represent that speech as a matrix of frequency values. Coming back on topic, now that we understand the format of the input, let's finally delve into where the real magic happens, the convolutional layer. As you can assume, this layer and the network as a whole get their names from the convolutional operator, which in layman terms is a mathematical combination, in other words, the dot product of two functions to produce a third function. In CNNs, this operation is implemented in what is referred to as a feature detector, filter, or most commonly, a kernel. You can think of a kernel as a mini matrix, orders of magnitude smaller than the input. In a convolution operation, then, the kernel moves across the input image, taking the dot product of the two matrices and then saving the values to a new matrix, dubbed the feature map of the original image. At this point, you may be wondering how this is able to detect any features at all. Well, if a kernel is initialized with the values in a specific configuration, they can be used to transform an input image and find various patterns. Take these kernels, for instance. When loaded with the appropriate values and convolved with the input image produces an output which highlights various edges in the photo. Now I won't delve into the specifics of the various types of kernels in this video, but there are many resources that talk about them extensively, such as Computerphile and their various videos on kernels, like the Sobel edge detector. Coming back on topic, as you can see, kernels are essentially ways to do fast computation and produce a new output. In this case, we're doing it with an image, with an effect or features highlighted. Programs like Photoshop use these simple kernels for effects like blurs, for instance. Now, when we look at a typical image, take in how much is actually going on. There are various edges, shapes, textures, et cetera, that stack together to make various objects in the overall image we are looking at. At the start of a convolutional network, the types of kernels we use would be quite simple and more geometric, detecting things such as edges, corners, and simple shapes and patterns, like a circle, for instance. Additionally, each convolutional layer can have multiple kernels that produce multiple feature maps of their own. So for our example, we can see that our first layer has six kernels, and these kernels are detecting simple patterns, like horizontal lines, vertical lines, and corners on our drawn numbers. As a side note, for each of the convolved pixels in our feature map, a non-linearity function \"}, {'https://www.youtube.com/watch?v=KuXjwB4LzSA': \"Suppose I give you two different lists of numbers, or maybe two different functions, and I ask you to think of all the ways you might combine those two lists to get a new list of numbers, or combine the two functions to get a new function. Maybe one simple way that comes to mind is to simply add them together term by term. Likewise with the functions, you can add all the corresponding outputs. In a similar vein, you could also multiply the two lists term by term and do the same thing with the functions. But there's another kind of combination just as fundamental as both of those, but a lot less commonly discussed, known as a convolution. But unlike the previous two cases, it's not something that's merely inherited from an operation you can do to numbers. It's something genuinely new for the context of lists of numbers or combining functions. They show up all over the place, they are ubiquitous in image processing, it's a core construct in the theory of probability, they're used a lot in solving differential equations, and one context where you've almost certainly seen it, if not by this name, is multiplying two polynomials together. As someone in the business of visual explanations, this is an especially great topic, because the formulaic definition in isolation and without context can look kind of intimidating, but if we take the time to really unpack what it's saying, and before that actually motivate why you would want something like this, it's an incredibly beautiful operation. And I have to admit, I actually learned a little something while putting together the visuals for this project. In the case of convolving two different functions, I was trying to think of different ways you might picture what that could mean, and with one of them I had a little bit of an aha moment for why it is that normal distributions play the role that they do in probability, why it's such a natural shape for a function. But I'm getting ahead of myself, there's a lot of setup for that one. In this video, our primary focus is just going to be on the discrete case, and in particular building up to a very unexpected but very clever algorithm for computing these. And I'll pull out the discussion for the continuous case into a second part. It's very tempting to open up with the image processing examples, since they're visually the most intriguing, but there are a couple bits of finickiness that make the image processing case less representative of convolutions overall, so instead let's kick things off with probability, and in particular one of the simplest examples that I'm sure everyone here has thought about at some point in their life, which is rolling a pair of dice and figuring out the chances of seeing various different sums. And you might say, not a problem, not a problem. Each of your two dice has six different possible outcomes, which gives us a total of 36 distinct possible pairs of outcomes, and if we just look through them all we can count up how many pairs have a given sum. And arranging all the pairs in a grid like this, one pretty nice thing is that all of the pairs that have a constant sum are visible along one of these different diagonals. So simply counting how many exist on each of those diagonals will tell you how likely you are to see a particular sum. And I'd say, very good, very good, but can you think of any other ways that you might visualize the same question? Other images that can come to mind to think of all the distinct pairs that have a given sum? And maybe one of you raises your hand and says, yeah, I've got one. Let's say you picture these two different sets of possibilities each in a row, but you flip around that second row. That way all of the different pairs which add up to seven line up vertically like this. And if we slide that bottom row all the way to the right, then the unique pair that adds up to two, the snake eyes, are the only ones that align, and if I schlunk that over one unit to the right, the pairs which align are the two different pairs that add up to three. And in general, different offset values of this lower array, which remember I had to flip around first, reveal all the distinct pairs that have a given sum. As far as probability questions go, this still isn't especially interesting because all we're doing is counting how many outcomes there are in each of these categories. But that is with the implicit assumption that there's an equal chance for each of these faces to come up. But what if I told you I have a special set of dice that's not uniform? Maybe the blue die has its own set of numbers describing the probabilities for each face coming up, and the red die has its own unique distinct set of numbers. In that case, if you wanted to figure out, say, the probability of seeing a 2, you would multiply the probability that the blue die is a 1 times the probability that the red die is a 1. And for the chances of seeing a 3, you look at the two distinct pairs where that's possible, and again multiply the corresponding probabilities and then add those two products together. Similarly, the chances of seeing a 4 involves multiplying together three different pairs of possibilities and adding them all together. And in the spirit of setting up some formulas, let's name these top probabilities a 1, a 2, a 3, and so on, and name the bottom ones b 1, b 2, b 3, and so on. And in general, this process where we're taking two different arrays of numbers, flipping the second one around, and then lining them up at various different \"}, {'https://www.youtube.com/watch?v=CYvBjQTOdf4': \"Hello World, it's Rahul. Welcome back guys to Deep Learning tutorial. So guys here today we are gonna talk about Convolutional Neural Networks So what is CNN & why is it important to us? They are a kind of deep neural network which were designed from biologically driven models. So what researchers found was how a human perceives an image into the brain is in different layers & that's how this CNN model has designed & hence it has been very efficient for all the image processing pattern recognition kind of applications & here is a very basic structure of how Convolutional neural network looks like. So your input is an image & the task that you are dealing with here is recognising a bird & what you see here is that you have taken an input which is a 2D input & then the next layer you actually see bunch of convolutional layer i.e. 1,2,3,4 & 5 outputs. So what does that mean? You are using 5 filters apply them throughout the filter throughout the image, each filter give you 1 feature map & so in this layer you have 5 such features map. so as name says, it is the core block of CNN & what it does is it takes bunch of filters which will get applied to the given input image & create different activation features in that input image. so what we have we have input as a parameter which will be let say width times height of the input for 2D images. then we have kernels & these could be of any dimension. let say we have a three by three filter & it will have the same depth as the input. then we have number of kernels, which is basically number of activation filters which we are going to generate for given input image so let say that is n. once these set of kernels are applied to the input & output is generated from the convolutional which will have the same spatial dimension as the input & depth of the output would be same as the number of kernels. now two things here to talk about the spatial dimension that it is also dependent on a factor called strides & padding. strides decide how you want to process your pixel so if i have stride value of 1, i am processing each & every pixel so dimension is same as input but let's say if i have a stride of two then I will consider every other pixel in the image which will you know take my output dimension W by 2 to H by 2 so stride will basically act as a down sampling. so here one filter could be extracting the edges of the an image one filter could be extracting the colours of an image & so on so forth. So as i explained just now that after passing filter through our input image we are getting the output but with lower dimension. so you must have question that here we are loosing some of our data then how can we train our model on those sorts of information. so guys here padding comes into picture to preserve the same information. So if you want to make the output also 5 * 5 what we have to do is we have to add the padding all the four side to an input image & then apply the filter over this so that we can get 5 * 5 matrix & don't loose any of the information. let's understand how is this working with an example. so here we have an input image of 5 by 5. let's divide this image into pixel value which will be total 25 vector points & i am using 3 by 3 convolution or you can say it as filter as well. So in order to apply convolution all we have to do is that place this filter on top of input image. so here we have 3 cross 3 filter. let's take this 3 cross 3 initially. so when we take this 3 cross 3 now you see over here all we have to do is that multiply this term with respect to this particular cell. so in the first case you will see that 2 will be multiplied with 0 so i will get 0 then we will add the next value this 0 will get multiplied with the 1 & we will get as 0 then 2 will be multiplied with 3 & we get it as 6 now we will go to the next row & this 0 will be multiplied with 2 & get it as 0 & then this 2 will be multiplied with 1 & we get it as 2 then multiply 0 with 2 & we get it as 0. again we will go to the next cell. now 1 multiplied by 2 will be 2, 2 multiplied with 2 will be 4 & for last cell we will multiply this 1 with 3 & get it as 3 so by taking summation of all these values we will get 17 & this 17 will be placed in the new matrix which will be passed to the next layer or we can call it as output layer of our convolution. now what we will do is we will place the filter to the next column as we are considering our stride value as one so we have our filter only with 1 value. so it means we will go & select our next cell by jumping 1 to the right like this. now let's do this process for each & every cells. we will make the strides jump one by one but once it comes over the end we have to come step down. that basically means we have to start it from here. every time we have to do one step down after completing the end of the image. \"}], 'Recurrent Neural Networks (RNNs) for Sequential Data': [{'https://www.youtube.com/watch?v=AsNTP8Kwu80': \"Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about recurrent neural networks, and they're going to be clearly explained! Lightning and Grid are totally cool. Check them out when you've got some time. NOTE: This StatQuest assumes that you are already familiar with the main ideas behind neural networks, backpropagation and the ReLU activation function. If not, check out the 'Quest. ALSO NOTE: Although basic, or vanilla recurrent neural networks are awesome, they are usually thought of as a stepping stone to understanding fancier things like Long Short-Term Memory Networks and Transformers, which we will talk about in future StatQuests. In other words, every Quest worth taking take steps, and this is the first step. So with that said, let's say Hi to StatSquatch. Hi! And StatSquatch says, Hello! The other day I bought stock in a company called Get Rich Quick, but the next day their stock price went down and I lost money. Bummer. So, I was thinking, maybe we could create a neural network to predict stock prices. Wouldn't that be cool? That sure would be cool 'Squatch, unfortunately the actual stock market is crazy complicated and we'd probably both get in a lot of trouble if we offered advice on how to make money with it, but if we go to that mystical place called StatLand things are much simpler and there are far fewer lawyers. So let's build a neural network that predicts stock prices in StatLand. However, first let's just talk about stock market data in general. When we look at stock prices, they tend to change over time. For example, the price of this stock went up for four days before going down. Also, the longer a company has been traded on the stock market, the more data we'll have for it. For example, we have more time points for the company represented by the blue line then we have for the company represented by the red line. What that means is, if we want to use a neural network to predict stock prices, then we need a neural network that works with different amounts of sequential data. In other words, if we want to predict the stock price for the Blue Line company on day 10, then we might want to use the data from all nine of the preceding days. In contrast, if we wanted to predict the stock price for the Red Line company on day 10, then we would only have data for the preceding five days. So we need the neural network to be flexible in terms of how much sequential data we use to make a prediction. This is a big difference compared to the other neural networks we've looked at in this series. For example, in Neural Networks Clearly Explained, we examined a neural network that made predictions using one input value, no more and no less. And if you saw the StatQuest on neural networks with multiple inputs and outputs, you saw this neural network that made predictions using two input values, no more and no less. And in the StatQuest on Deep Learning Image Classification, you saw a neural network that made a prediction using an image that was six pixels by six pixels, no bigger and no smaller. However, now we need a neural network that can make a prediction using the nine values we have for the blue company and make a prediction using the five values we have for the red company. The good news is that one way to deal with the problem of having different amounts of input values is to use a Recurrent Neural Network. Just like the other neural networks that we've seen before, recurrent neural networks have weights, biases, layers and activation functions. The big difference is that recurrent neural networks also have feedback loops. And, although this neural network may look like it only takes a single input value, the feedback loop makes it possible to use sequential input values, like stock market prices collected over time, to make predictions. To understand how, exactly, this recurrent neural network can make predictions with sequential input values, let's run some of StatLand's stock market data through it. In StatLand, if the price of a stock is low for two days in a row, then, more often than not, the price remains low on the next day. In other words, if yesterday and today's stock price is low, then tomorrow's price should also be low. In contrast, if yesterday's price was low and today's price is medium, then tomorrow's price should be even higher. And when the price decreases from high to medium, then tomorrow's price will be even lower. Lastly, if the price stays high for two days in a row, then the price will be high tomorrow. Now that we see the general trends in stock prices in StatLand, we can talk about how to run yesterday and today's data through a recurrent neural network to predict tomorrow's price. The first thing we'll do is scale the prices so that low equals 0, medium equals 0.5, and high equals 1. Now let's run the values for yesterday and today through this recurrent neural network and see if it can correctly predict tomorrow's value. Now, because the recurrent neural network has a feedback loop, we can enter yesterday and today's values into the input sequentially. We'll start by plugging yesterday's value into the input. Now we can do the math just like we would for any other neural network. Beep. Boop. Beep. Boop. Boop. At this point, the output from the activation function, the y axis coordinate that we will call Y sub 1, can go two places. First Y sub 1 can go towards the output. And if we go that way and do the math beep, boop, boop then the output is the predicted value for today. However, we're not interested in the predicted value for today because we already have the actual value for today. Instead, we want \"}, {'https://www.youtube.com/watch?v=Or9QSDqzOK0': ''}, {'https://www.youtube.com/watch?v=5fdy-hBeWCI': \"Alright, let's now talk about how we can modify multi layer perceptrons to capture sequence information. So in particular, we are going to talk about sequence modeling with recurrent neural networks. So but before we get to that, how can we tell whether our model already uses sequence information? So for instance, if you think of logistic regression or multi layer perceptrons, do these types of models actually use sequence information? So the answer is no. And how can you know? I mean, there are two ways or two types of sequence information that might be encoded in our training set. One type is across the training example, access and the other type is across the feature access. So maybe to illustrate that let's revisit the iris data set. I know talking about iris is a little bit boring. But I think it's a nice simple data set to illustrate these types of problems. So in iris, what we had, we had, let's call it sepal length, sepal width, petal length, and petal width. So we have this tabular data set. And we have the training examples like 1234 up to 150. Now, when you have this data set, and so let's say this is your data set, and you split it into a training set and test set. And then you train the model on this training set. Let's call it train test, you can shuffle actually, all the records in the test set. So you assume, of course, also, this is shuffled because you want before you split in iris, usually you have the 51st flowers are setosa, the second 50 are Reginica and the third 50 are vertical. So let's assume you split them in a way that they're equally distributed now in the training and test set. But now, given the test set, you can actually shuffle all the records in the test set. And when you evaluate your model on the test set, you should still get the same performance. So this is kind of like a way of saying, the model doesn't really use any sequence information, it regards the considers the data as so called iid. iid means that the data is independent and identically distributed. So this means that each training record is independent of each other. So it has been sampled independently. And also, it's from the same distribution. So distribution of flowers, virus flaws. So how to tell whether your model uses sequence information across the training access is really by let's say, doing this thought experiment of shuffling the test set. And you can, of course, probably tell that whether you shuffle the test set or not, the performance of the model should be exactly the same on the test set. If you use multi layer perceptrons or logistic regression. Another type of sequence information might be encoded in the features, right? So the order here of the features. So what you can also think about is if what happens if you swap columns, so let's say you use this, this original Iris data set with these columns here, then you train the model on this data set. And then you test it. Let's say you get 90% accuracy. Now, let's say, just out of fun, you are swapping these two columns here, like a simple length and petal width. Now in your modified data set, you have on let's say, petal width here, and sample length here. And then again, you split this the same way into training and test like you did before using the same records for each data set. Now, if you train the model and test it, you should get exactly the same 90% accuracy, you can try this in practice, you will find the model performance should be exactly the same. And this is because let's say a multi layer perceptrons, and logistic regression, they don't use sequence information in across the features, they regard the features as independent here. So in that way, they don't have to occur in a certain order. And if the features occur in a certain order, this information is ignored. This is because I mean, you can simply test this by swapping columns, and then training the model on the data set with a small columns, and testing it on a data set with swapped columns, and you will find there is no difference. This can be a problem, though, when you think back of our back of words model. So the back of words model had this vocabulary. And it essentially gets rid of the word order in each training example in the feature vector. So if you think of an example here, where I have written down a just a spontaneous sentence saying the movie, my friend has not seen is good. This is of course different than the movie my friend has seen is not good. So two different meanings. So first meaning is, in the first sentence that the friend has not seen a movie, which is good. And the second one, it has seen the friend has seen the movie, but the movie is not good. So here, we have a good movie. And here we have a not say a bad movie. So two different sentences. But if we would use the back of words model, where we have the text as the word frequency, this would get lost because both both sentences would result in the exactly same feature vector. So in this case, we have this between in the inputs here, and the features, we have this ordering information that it really depends on really depends on which order words occur. So not seen is very different from not good, like the ordering really matters here. So, and recurrent neural networks can help us capture this ordering information. Yeah, and here are some examples of sequence data, for example, text classification, which is something we will be focusing on \"}, {'https://www.youtube.com/watch?v=6niqTuYFZLQ': \"- Okay. Can everyone hear me? Okay. Sorry for the delay. I had a bit of technical difficulty. Today was the first time I was trying to use my new touch bar Mac book pro for presenting, and none of the adapters are working. So, I had to switch laptops at the last minute. So, thanks. Sorry about that. So, today is lecture 10. We're talking about recurrent neural networks. So, as of, as usual, a couple administrative notes. So, We're working hard on assignment one grading. Those grades will probably be out sometime later today. Hopefully, they can get out before the A2 deadline. That's what I'm hoping for. On a related note, Assignment two is due today at 11:59 p.m. so, who's done with that already? About half you guys. So, you remember, I did warn you when the assignment went out that it was quite long, to start early. So, you were warned about that. But, hopefully, you guys have some late days left. Also, as another reminder, the midterm will be in class on Tuesday. If you kind of look around the lecture hall, there are not enough seats in this room to seat all the enrolled students in the class. So, we'll actually be having the midterm in several other lecture halls across campus. And we'll be sending out some more details on exactly where to go in the next couple of days. So a bit of a, another bit of announcement. We've been working on this sort of fun bit of extra credit thing for you to play with that we're calling the training game. This is this cool browser based experience, where you can go in and interactively train neural networks and tweak the hyper parameters during training. And this should be a really cool interactive way for you to practice some of these hyper parameter tuning skills that we've been talking about the last couple of lectures. So this is not required, but this, I think, will be a really useful experience to gain a little bit more intuition into how some of these hyper parameters work for different types of data sets in practice. So we're still working on getting all the bugs worked out of this setup, and we'll probably send out some more instructions on exactly how this will work in the next couple of days. But again, not required. But please do check it out. I think it'll be really fun and a really cool thing for you to play with. And will give you a bit of extra credit if you do some, if you end up working with this and doing a couple of runs with it. So, we'll again send out some more details about this soon once we get all the bugs worked out. As a reminder, last time we were talking about CNN Architectures. We kind of walked through the time line of some of the various winners of the image net classification challenge, kind of the breakthrough result. As we saw was the AlexNet architecture in 2012, which was a nine layer convolutional network. It did amazingly well, and it sort of kick started this whole deep learning revolution in computer vision, and kind of brought a lot of these models into the mainstream. Then we skipped ahead a couple years, and saw that in 2014 image net challenge, we had these two really interesting models, VGG and GoogLeNet, which were much deeper. So VGG was, they had a 16 and a 19 layer model, and GoogLeNet was, I believe, a 22 layer model. Although one thing that is kind of interesting about these models is that the 2014 image net challenge was right before batch normalization was invented. So at this time, before the invention of batch normalization, training these relatively deep models of roughly twenty layers was very challenging. So, in fact, both of these two models had to resort to a little bit of hackery in order to get their deep models to converge. So for VGG, they had the 16 and 19 layer models, but actually they first trained an 11 layer model, because that was what they could get to converge. And then added some extra random layers in the middle and then continued training, actually training the 16 and 19 layer models. So, managing this training process was very challenging in 2014 before the invention of batch normalization. Similarly, for GoogLeNet, we saw that GoogLeNet has these auxiliary classifiers that were stuck into lower layers of the network. And these were not really needed for the class to, to get good classification performance. This was just sort of a way to cause extra gradient to be injected directly into the lower layers of the network. And this sort of, this again was before the invention of batch normalization and now once you have these networks with batch normalization, then you no longer need these slightly ugly hacks in order to get these deeper models to converge. Then we also saw in the 2015 image net challenge was this really cool model called ResNet, these residual networks that now have these shortcut connections that actually have these little residual blocks where we're going to take our input, pass it through the residual blocks, and then add that output of the, then add our input to the block, to the output from these convolutional layers. This is kind of a funny architecture, but it actually has two really nice properties. One is that if we just set all the weights in this residual block to zero, then this block is competing the identity. So in some way, it's relatively easy for this model to learn not to use the layers that it doesn't need. In addition, it kind of adds this interpretation to L2 regularization in the context of these neural networks, cause once you put L2 regularization, remember, on your, on the weights of your network, that's going to drive all the parameters towards zero. \"}, {'https://www.youtube.com/watch?v=OuYtk9Ymut4': 'LAURENCE MORONEY: Hi, and welcome to this episode in \"Natural Language Processing, Zero to Hero\" with TensorFlow. In the previous videos in this series, you saw how to tokenize text and how to use sequences of tokens to train a neural network. In particular, you saw how to create a neural network that classified text by sentiments. And in this case, you trained a classifier on sarcasm headlines. But the next step I\\'m often asked when it comes to text is, what about generating text? Can a neural network create text based on the corpus that it\\'s trained on, and can we get an AI to write poetry? Well, the answer to this is yes. And over the next few videos, I\\'ll show you a simple example on how you can achieve this. Before we can do that, though, an important concept that you\\'ll need to understand is recurrent neural networks. This type of neural network takes the sequence of data into account when it\\'s learning. So for example, in the case of a classifier for text that we just saw, the order in which the words appear in the sentence doesn\\'t really matter. What determined the sentiment was the vector that resulted in adding up all of the individual vectors for the individual words. The direction of that vector roughly gave us the sentiments. But if we\\'re going to generate text, the order does matter. For example, consider this sentence. \"Today the weather is gorgeous, and I see a beautiful blue\"-- something. If you were trying to predict the next word-- and the concept of creating text really boils down to predicting the next word-- you\\'d probably say, \"sky,\" because that comes after \"beautiful\" and \"blue,\" and the context is the weather, which we saw earlier in the sentence. So how do we fit this to neural networks? Let\\'s take a look at what\\'s involved in changing from sequence list data to sequential data. Neural networks for classification or regression tend to look like this. It\\'s kind of like a function that you feed in data and labels, and it infers the rules that fits the data to the labels. But you could also express it like this. The f of data and labels equals the rules. But there\\'s no sequence inherent in this. So let\\'s take a look at some numeric sequences and explore the anatomy of them. And here\\'s a very famous one called the Fibonacci sequence. To describe the rules that make this sequence, let\\'s describe the numbers using a variable. So for example, we can say n0 for the first number, n1 for the next, and so on. And the rule that then defines the sequence is that any number in the sequence is the sum of the two numbers before it. So if we start with 1 and 2, the next number is 1 plus 2, which is 3. The next number is 5, which is 2 plus 3, and so on. We could also try to visualize it like this on a computation graph. If the function is plus, we feed in 1 and 2 to get 3. We also pass this answer and the second parameter, which in this case was 2, onto the next computation. This gives us 2 plus 3, which is 5. This gets fed into the next computation along with the second parameter, so 5 plus 3 get added to get 8, and so on. So every number is in essence contextualized into every other number. We started with 1, and added it to 2 to get 3. The 1 and the 3 still exists. And when added to 2 again, we get 5. That 1 still continues to exist throughout the series. Thus, a numeric value can recur throughout the life of the series. And this is the basis of the concept of a recurrent neural network. Let\\'s take a look at this type of network in a little more detail. Typically, a recurrent neuron is drawn like this. There\\'s a function that gets an input value that produces an output value. In addition to the output, it also produces another feed-forward value that gets passed to the next neuron. So a bunch of them together can look like this. And reading from left to right, we can feed x0 into the neuron, and it calculates a result, y0, as well as a value that gets passed to the next neuron. That gets x1 along with the fed-forward value from the previous neuron and calculates y1. And its output is combined with x to get y2 and a feed-forward value to the next neuron, and so on. Thus, sequence is encoded into the outputs, a little bit like the Fibonacci sequence. This recurrence of data gives us the name recurrent neural networks. So that\\'s all very well. And you may have seen a little catch in how this could work with natural language processing. A simple RNN like the one that I\\'ve just shown is a bit like the Fibonacci sequence in that the sequence can be very strong, but it weakens as the context spreads. The number at the position 1 has very little impact on the number at the position 100, for example. It\\'s there, but it\\'s tiny. And that could be useful for predicting text where the signal to determine the text is close by, for example, the beautiful blue something that we mentioned earlier. It\\'s easy for us to see that \"sky\" is the next word. But what about a sentence like this? \"I lived in Ireland, so they taught me how to speak\"-- something. Now, you might think it\\'s \"Irish,\" but the correct answer is \"Gaelic.\" But think about how you predicted that word. The key word that dictated it was much further back in the sentence, and it\\'s the word \"Ireland.\" If we were only predicting based on the words that are close to the desired one, we\\'d miss that completely, '}], 'Deep Learning Project: Image Classification with CNN': [{'https://www.youtube.com/watch?v=K_BHmztRTpA': \"(light music) Narrator: What is a convolutional neural network? Let's start with the basics. A convolutional neural network is a type of neural network that is most often applied to image processing problems. You've probably seen them in action anywhere a computer is identifying objects in an image, but you can also use convolutional neural networks in natural language processing projects too. The fact that they are useful for these fast-growing areas is one of the main reasons they're so important in deep learning and artificial intelligence today. Once you understand how a convolutional neural network works and what makes it unique from other neural networks, you can see why they're so effective for processing and classifying images. But let's first take a regular neural network. A regular neural network has an input layer, hidden layers, and an output layer. The input layer accepts inputs in different forms while the hidden layers perform calculations on these inputs. The output layer then delivers the outcome of the calculations and extractions. Each of these layers contain neurons that are connected to neurons in the previous layer and each neuron has its own weight. This means you aren't making any assumptions about the data being fed into the network. Great usually but not if you're working with images or a language. Convolutional neural networks work differently as they treat data as spatial. Instead of neurons being connected to every neuron in the previous layer, they are instead only connected to neurons close to it and all have the same weight. The simplification in the connections means the network upholds the spatial aspect of the data set. It means your network doesn't think an eye is all over the image. The word convolutional refers to the filtering process that happens in this type of network. Think of it this way, an image is complex. A convolutional neural network simplifies it, so it can be better processed and understood. Let's look now at what's inside a convolutional neural network. Like a normal neural network, a convolutional neural network is made up of multiple layers. There are a couple of layers that make it unique, the convolutional layer and the pooling layer. However, like other neural networks, it will also have a ReLU or a rectified linear unit layer and a fully connected layer. The ReLU layer acts as an activation function, ensuring non-linearity as the data moves through each layer in the network. Without it, the data being fed into each layer would lose the dimensionality that we want to maintain. The fully connected layer meanwhile allows you to perform classification on your data set. The convolutional layer is the most important, so let's start there. It works by placing a filter over an array of image pixels. This then creates what's called a convolved feature map. It's a bit like looking at an image through a window, which allows you to see specific features you might not otherwise be able to see. Next, we have the pooling layer. This downsamples or reduces the sample size of a particular feature map. This also makes processing much faster as it reduces the number of parameters the network needs to process. The output of this is a pooled feature map. There are two ways of doing this: max pooling, which takes the maximum input of a particular convolved feature, or average pooling, which simply takes the average. These steps amount to feature extraction whereby the network builds up a picture of the image data according to its own mathematical rules. If you want to perform classification, you'll need to move into the fully connected layer. To do this, you'll need to flatten things out. Remember, a neural network with a more complex set of connections can only process linear data. There are a number of ways you can train a convolutional neural network. If you're working with unlabeled data, you can use unsupervised learning methods. One of the best popular ways of doing this is using autoencoders. This allows you to squeeze data in a space with low dimensions, performing calculations in the first part of the convolutional neural network. Once this is done, you'll then need to reconstruct with additional layers that upsample the data you have. Another option is to use generative adversarial networks or GANs. With a GAN, you train two networks. The first gives you artificial data samples that should resemble data in the training set while the second is a discriminative network. It should distinguish between the artificial and the true model. A lot of people seem to be asking what the difference is between a convolutional neural network and a recurrent network. It's actually quite simple. Whereas a convolutional neural network is a feedforward network that filters spatial data, a recurrent neural network, as the name implies, feeds data back into itself. From this perspective, recurrent neural networks are better suited to sequential data. Think of it like this, a convolutional network is able to perceive patterns across space, a recurrent neural network can see them over time. If you want to get started with convolutional neural networks, Python and TensorFlow are great tools to begin with. It's worth exploring MNIST data set too. This is a database of handwritten digits that you can use to get started with building your first convolutional neural network. If you want to learn more about convolutional neural networks, deep learning, and AI, visit Packt today. \"}, {'https://www.youtube.com/watch?v=taC5pMCm70U': \"In computer vision, image classification is not the only problem that we try to solve. There are two other problems that we solve using computer vision, which is object detection and image segmentation. In this short video, I will explain you the difference between these three using a simple example. For image classification you try to classify entire image as one of the classes, for example is this a dog or not. Image classification with localization means you not only classify the image as one of the class but you find the location of that object within the image. So this is called image classification with localization. If you have multiple objects in your image such as cat and dog, you might want to find their location and you can try drawing these bounding boxes or rectangles specifying the location of these classes. This is called object detection. In the image, you're trying to detect an object, which in this case is cat and dog and you're drawing these bounding boxes or rectangles, so this is object detection. You can go one more level up and classify each pixel as one of the classes. For example, here all the red pixels belongs to dog and yellow pixels belongs to cat. This is more fine grained classification where you're exactly specifying each pixel belonging to each class. So just to summarize when you classify the entire image as one of the classes is called Image Classification. When you detect the objects within an image with rectangular bounding boxes, it's called Object Detection; and when you classify each of the pixels as one of the classes it is called Image Segmentation. Image Segmentation is little more involved, I just gave a very simple definition of it. The the image segmentation which, I showed you is called instance segmentation. There is semantic segmentation as well but for this video just to keep things simple you can leave this, with this understanding, and I hope you like this video and I hope you are enjoying this deep learning series. In the furthest further videos we'll go more into object detection image segmentation and we'll try to write python code using tensorflow. Thank you for watching! \"}, {'https://www.youtube.com/watch?v=QzY57FaENXg': \"OK, pop quiz. What am I drawing? I'm going to make three predictions here. Firstly. You think at your house, you'd be right? Secondly, that that just came pretty easily to you, it was effortless. And thirdly, you're thinking that I'm not much of an artist and you'd be right on all counts there. But how can we look at this set of geometric shapes and think, Oh, how? If you live in a house, I bet it looks nothing like this. Well, that ability to perform object identification that comes so easily to us does not come so easily to a computer, but that is where we can apply something called convolutional neural networks to the problem. Now, a convolutional neural network or a. See, and and. Is a area of deep learning that specializes in pattern recognition. My name is Martin Keane, and I work in the IBM garage at IBM. Now let's take a look at how CNN works at a high level. Well, let's break it down. CNN convolutional neural network Well, let's start with the artificial neural network part. This is a standard network that consists of multiple layers that are interconnected, and each layer receives some input. Transforms that input to something else and passes an output to the next layer, that's how neural networks work and see an end is a particular part of the neural network or a section of layers that say it's these three layers here and within these layers, we have something called filters. And it's the filters that perform the pattern recognition that CNN is so good at. So let's apply this to our house example now. If this house were an actual image, it would be a series of pixels, just like any image. And if we zoom in on a particular part of this house, let's say we zoom in around here, then we would get, well, the window. And what we're saying here is that a window consists of some perfectly straight lines. Almost perfectly straight lines. But, you know, a window doesn't need to look like that window could equally look like this, and we would still say it was a window. The cool thing about CNN is that using filters. CNN could also say that these two objects represent the same thing. The way they do that, then, is through the application of these filters. So let's take a look at how that works. Now, a filter is basically just a three by three block. And within that block, we can specify a pattern to look for. So we could say, let's look for. Pattern like this, a right angle in our image. So what we do is we take this filter and it's a three by three block here. We will analyze the equivalent three by three block up here as well. So. We'll look at first of all, these first. Group of three by three pixels, and we will see how close are they to the filter shape? And we'll get that numeric score, then we will move across one, come to the right and look at the next three by three block of pixels and score how close they are to the filter shape. And we will continue to slide over or vote over all of these pixel layers until we have not every three by three block. Now, that's just for one filter. But what that will give us is an array of numbers that say how closely and the image matches filter, but we can add more filters so we could add another three by three filter here. And perhaps this one looks for a shape like this. And we could add a third filter here, and perhaps this looks for a different kind of right angle shape. If we take the numeric arrays from all of these filters and combine them together in a process called pooling, then we have a much better understanding of what is contained within this series of pixels. Now that's just the first layer of the CNN. And as we go deeper into the neural network, the filters become more abstract all they can do more. So the second layer of filters perhaps can perform tasks like basic object recognition. So we can have filters here that might be able to recognize the presence of a window or the presence of a door or the presence of a roof. And as we go deeper into the sea and into the next leg, well, maybe these filters can perform even more abstract tasks, like being able to determine whether we're looking at a house or we're looking at an apartment or whether we're looking at a skyscraper. So you can see the application of these filters increases as we go through the network and can perform more and more tasks. And that's a very high level basic overview of what CNN is. It has a ton of business applications. Think of OCR, for example, for understanding handwritten documents. Think of visual recognition and facial detection and visual search. Think of medical imagery and looking at that and determining what is being shown in an imaging scan. And even think of the fact that we can apply a CNN to perform object identification for. Body drawn houses, if you have any questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe. Thanks for watching. \"}, {'https://www.youtube.com/watch?v=ZTCoMo_gDYo': ''}, {'https://www.youtube.com/watch?v=LsdxvjLWkIY': \"Transfer learning has become quite popular in the field of image classification and Natural Language Processing. Here we take a pre-trained model and then we try to retrain it for the new problem. So if you remember from our data augmentation tutorial, we had flowers dataset where we are trying to classify five type of flowers. So in this video we will use a Mobilenet pre-trained model from Google's Tensorflow hub and we will use that pre-trained model to classify our flowers dataset and you will see that previously it used to take many epochs to train the complete model and achieve high accuracy. In this case using a pre-trained model it takes only like two or five iteration or epochs to get a superb accuracy. So using transfer learning saves lot of computation power because many times these pre-trained models that you can get from places like Tensorflow hub they are trained on millions of images. If you try to train that model on your computer it might take days or even months. But all you're doing is you're taking that pre-trained model, you're getting all the weights and everything and then you kind of change only the last layer or last few layers for your new problem and then uh you can get a superb high accuracy uh with this kind of approach. So let's get started we'll go over some theory and then we'll uh do coding. This is Wikipedia's definition of Transfer Learning which is you focus on storing knowledge gained while solving one problem and apply it to a different but related problem. For example if you have a model that can recognize cars it can be used to recognize trucks as well because the basic features, for example the tires, the steering wheel and some of the components between cars and trucks will be still similar. So you can use this knowledge of this visual world to transfer that knowledge into solving a different problem. In today's coding problem what we are going to do is we will take a Google's trained Mobilenet V2 model which is trained on 1.4 million images and total thousand classes. So this is a deep learning model that is trained at Google it would have taken a long time and a lot of computational resources you can see 1.5 4 million images is pretty huge dataset and the output is 1000 classes and these classes are little diverse you know. You have a goldfish, shark, some animals then some Hammerhead military uniform so you have it's not just the animals it's animals and some other objects total thousand classes and when this model is trained it will have input layer, then some deep layers and hidden layers in between then in the end you have a softmax layer which is just you know classifying it into thousand categories. In deep learning what happens is we freeze all the layers except the last one. So you know all these layers that you see, we will freeze it and then we'll use this model to classify flowers which could be one of the five flower types which I have shown here and we are going to use same dataset that we use in our data eggman augmentation tutorial. So when you freeze this layer what happens is the model weights don't change. So now when I'm performing my training, so by the way you take the model and then you still have to perform the training that's very important. But when you're performing a training the the weights in these frozen layers are not changing. So it almost you know looks like a con equation. So you are having this one big non-linear equation so you are passing your flower and this is a training phase, and then during using this weight you will get a feature vector. You are almost doing a feature engineering and then you use soft mix to classify into five classes instead of thousand. So I hope you get an idea that you're almost generating the features feature vector using this frozen layers. So during the training none of the weights nothing changes okay, and omitting the last layer is a very common approach in some approaches they also freeze only let's say three layers or two layers and remaining layers uh go through the usual neural network training okay? So we're going to now uh do a python coding uh to use Mobilenet V2 model and then use it to classify the flowers. We will download a pre-trained model from a place called Tensorflow hub. So Google has come up with this Tensorflow hub where you can get an access of different pre-trained models. So right now for tax domain problems they have all these models, for example for embedding they have 176 models, for image classification they have 188 models. So these are like pre-trained models which are trained on a huge dataset which you can import and directly use it. For video also see they have some video and audio so they have some problem. So if I look at image classification here there is this model called Mobilenet V2 okay? So this is the model we are going to use so this model as I said is trained on 1.4 million uh images and 1000 different classes, and the image is like 2224 by 224. you know it is that dimension. So now here in my jupyter notebook I have imported all essential libraries, and the first thing I am going to do is create is basically import that Mobilenet V2 classification model. So this is how we import it. So I have imported Tensorflow hub now this doesn't come with your regular tensorflow installation you have to install it separately. So make sure you run pip install Tensorflow hub otherwise it will give you model not found error. Here I am creating a classifier directly using this particular Mobilenet. So if you look at \"}]}\n"
          ]
        }
      ],
      "source": [
        "def prepare_data_for_best_video(video_df):\n",
        "    # Initialize the dictionary to store the results\n",
        "    result_dict = {}\n",
        "    selecting_video = video_df[['Title', 'Videos']].to_dict('records')\n",
        "\n",
        "    # Loop through each blog entry\n",
        "    for dic in selecting_video:\n",
        "        title = dic['Title']\n",
        "        urls = dic['Videos']  # Assuming URLs are comma-separated strings\n",
        "\n",
        "        # Initialize list to hold content for each URL under the same topic\n",
        "        content_list = []\n",
        "\n",
        "        for url in urls:\n",
        "            url = url.strip()  # Remove any extra whitespace\n",
        "            first_1500_words, full_transcript = return_first_1000_words(url)\n",
        "            # Add the URL and extracted content to the content list\n",
        "            content_list.append({url: first_1500_words})\n",
        "\n",
        "        # Store the result in the dictionary\n",
        "        result_dict[title] = content_list\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "result_dict = prepare_data_for_best_video(video_df)\n",
        "print(result_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "q5R6G5-v7xcN"
      },
      "outputs": [],
      "source": [
        "#result_dict['Django Basics: Setting up Environment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "s2Ek6CBf7J97",
        "outputId": "c652f8b6-2400-498b-f881-03ed6e9f3796"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'from langchain.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\n\\nclass BestURL(BaseModel):\\n    \"\"\"Best Url \"\"\"\\n    Title: str = Field(description=\"The name of the title\")\\n    url: str = Field(description=\"the best url among the urls\")\\n\\nstructured_llm = gemini_llm.with_structured_output(BestURL)\\n\\ntemplate = PromptTemplate(\\n  input_variables=[\\'topic\\',\\'distraction_tolerance\\',\\'content\\'],\\n  template=\"Select the best 1 video url on the topic {topic} by analysing the {content}\"\\n)\\nprompt = template.format(topic=\\'Django Basics: Setting up Environment\\',distraction_tolerance=5,content=result_dict[\\'Django Basics: Setting up Environment\\'])\\n\\nprint(structured_llm.invoke(prompt))'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class BestURL(BaseModel):\n",
        "    \"\"\"Best Url \"\"\"\n",
        "    Title: str = Field(description=\"The name of the title\")\n",
        "    url: str = Field(description=\"the best url among the urls\")\n",
        "\n",
        "structured_llm = gemini_llm.with_structured_output(BestURL)\n",
        "\n",
        "template = PromptTemplate(\n",
        "  input_variables=['topic','distraction_tolerance','content'],\n",
        "  template=\"Select the best 1 video url on the topic {topic} by analysing the {content}\"\n",
        ")\n",
        "prompt = template.format(topic='Django Basics: Setting up Environment',distraction_tolerance=5,content=result_dict['Django Basics: Setting up Environment'])\n",
        "\n",
        "print(structured_llm.invoke(prompt))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifZ8Yh8_5tSJ",
        "outputId": "8c4deb70-3514-401e-8887-0e2ba0e07fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_values([{'Title': 'Deep Learning Introduction & Neural Networks', 'youtube_url': 'https://www.youtube.com/watch?v=aircAruvnKk'}, {'Title': 'Feedforward Neural Networks and Backpropagation', 'youtube_url': 'https://www.youtube.com/watch?v=S5AGN9XfPK4'}, {'Title': 'CNNs for Image Recognition', 'youtube_url': 'https://www.youtube.com/watch?v=K_BHmztRTpA'}, {'Title': 'Recurrent Neural Networks (RNNs) for Sequential Data', 'youtube_url': 'https://www.youtube.com/watch?v=Or9QSDqzOK0'}])\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import time\n",
        "\n",
        "class BestURL(BaseModel):\n",
        "  \"\"\"Best Url \"\"\"\n",
        "  Title: str = Field(description=\"The name of the title\")\n",
        "  url: str = Field(description=\"the best url among the urls\")\n",
        "\n",
        "structured_llm = gemini_llm.with_structured_output(BestURL)\n",
        "\n",
        "# Modified template to avoid multiple function calls\n",
        "template = PromptTemplate(\n",
        "  input_variables=['topic', 'distraction_tolerance', 'content'],\n",
        "  template=\"Select the best 1 url on the topic {topic} by analyzing the {content}\"\n",
        ")\n",
        "\n",
        "best_video_urls = {}\n",
        "\n",
        "for topic, content in result_dict.items():\n",
        "  prompt = template.format(topic=topic, distraction_tolerance=5, content=content)\n",
        "\n",
        "  try:\n",
        "    response = structured_llm.invoke(prompt)\n",
        "\n",
        "    # Check if response is None or not\n",
        "    if response:\n",
        "      if response.url.startswith(\"https://www.youtube.com/watch?\"):\n",
        "        best_video_urls[topic] = {'Title': response.Title, 'youtube_url': response.url}\n",
        "      else:\n",
        "        # Handle non-YouTube URLs (optional)\n",
        "        # You can choose to ignore them, log them, or take other actions\n",
        "        pass\n",
        "    else:\n",
        "      # Handle case where response is None (optional)\n",
        "      best_video_urls[topic] = {'Title': topic, 'youtube_url': content[0].keys()[0]}\n",
        "\n",
        "  except:\n",
        "    # Handle potential exceptions (optional)\n",
        "    pass\n",
        "\n",
        "best_video_urls = best_video_urls.values()\n",
        "print(best_video_urls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Pbhz1lfRP2mZ",
        "outputId": "1343378b-c860-4776-ea8b-411af2b8af18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df2\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Feedforward Neural Networks and Backpropagation\",\n          \"Recurrent Neural Networks (RNNs) for Sequential Data\",\n          \"Deep Learning Introduction & Neural Networks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"youtube_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"https://www.youtube.com/watch?v=S5AGN9XfPK4\",\n          \"https://www.youtube.com/watch?v=Or9QSDqzOK0\",\n          \"https://www.youtube.com/watch?v=aircAruvnKk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df2"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fef24d72-03b8-4555-acb3-153182ff84f6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>youtube_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Deep Learning Introduction &amp; Neural Networks</td>\n",
              "      <td>https://www.youtube.com/watch?v=aircAruvnKk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feedforward Neural Networks and Backpropagation</td>\n",
              "      <td>https://www.youtube.com/watch?v=S5AGN9XfPK4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CNNs for Image Recognition</td>\n",
              "      <td>https://www.youtube.com/watch?v=K_BHmztRTpA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recurrent Neural Networks (RNNs) for Sequentia...</td>\n",
              "      <td>https://www.youtube.com/watch?v=Or9QSDqzOK0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fef24d72-03b8-4555-acb3-153182ff84f6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fef24d72-03b8-4555-acb3-153182ff84f6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fef24d72-03b8-4555-acb3-153182ff84f6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52d98e2b-2596-45f6-ae00-7b4d39527031\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52d98e2b-2596-45f6-ae00-7b4d39527031')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52d98e2b-2596-45f6-ae00-7b4d39527031 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_813e2535-bee3-4c5c-a015-fab865aca849\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df2')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_813e2535-bee3-4c5c-a015-fab865aca849 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df2');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Title  \\\n",
              "0       Deep Learning Introduction & Neural Networks   \n",
              "1    Feedforward Neural Networks and Backpropagation   \n",
              "2                         CNNs for Image Recognition   \n",
              "3  Recurrent Neural Networks (RNNs) for Sequentia...   \n",
              "\n",
              "                                   youtube_url  \n",
              "0  https://www.youtube.com/watch?v=aircAruvnKk  \n",
              "1  https://www.youtube.com/watch?v=S5AGN9XfPK4  \n",
              "2  https://www.youtube.com/watch?v=K_BHmztRTpA  \n",
              "3  https://www.youtube.com/watch?v=Or9QSDqzOK0  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2 = pd.DataFrame(best_video_urls)\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "fhm715AiP2jw",
        "outputId": "ea82a3da-4b73-42ce-85eb-220bfabcbedb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"final_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Feedforward Neural Networks and Backpropagation\",\n          \"Recurrent Neural Networks (RNNs) for Sequential Data\",\n          \"Deep Learning Introduction & Neural Networks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blog_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/\",\n          \"https://neptune.ai/blog/recurrent-neural-network-guide\",\n          \"https://www.dataquest.io/blog/tutorial-introduction-to-deep-learning/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"youtube_url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"https://www.youtube.com/watch?v=S5AGN9XfPK4\",\n          \"https://www.youtube.com/watch?v=Or9QSDqzOK0\",\n          \"https://www.youtube.com/watch?v=aircAruvnKk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "final_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c9db623e-259e-41f0-b489-674412069dfc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>blog_url</th>\n",
              "      <th>youtube_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Deep Learning Introduction &amp; Neural Networks</td>\n",
              "      <td>https://www.dataquest.io/blog/tutorial-introdu...</td>\n",
              "      <td>https://www.youtube.com/watch?v=aircAruvnKk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feedforward Neural Networks and Backpropagation</td>\n",
              "      <td>https://jonaslalin.com/2021/12/10/feedforward-...</td>\n",
              "      <td>https://www.youtube.com/watch?v=S5AGN9XfPK4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CNNs for Image Recognition</td>\n",
              "      <td>https://www.edge-ai-vision.com/2015/11/using-c...</td>\n",
              "      <td>https://www.youtube.com/watch?v=K_BHmztRTpA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Recurrent Neural Networks (RNNs) for Sequentia...</td>\n",
              "      <td>https://neptune.ai/blog/recurrent-neural-netwo...</td>\n",
              "      <td>https://www.youtube.com/watch?v=Or9QSDqzOK0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9db623e-259e-41f0-b489-674412069dfc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9db623e-259e-41f0-b489-674412069dfc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9db623e-259e-41f0-b489-674412069dfc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb878b90-a85e-4789-9de9-8418c5c6cb01\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb878b90-a85e-4789-9de9-8418c5c6cb01')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb878b90-a85e-4789-9de9-8418c5c6cb01 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8ccc625b-281a-464b-b223-cffd746797d8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8ccc625b-281a-464b-b223-cffd746797d8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Title  \\\n",
              "0       Deep Learning Introduction & Neural Networks   \n",
              "1    Feedforward Neural Networks and Backpropagation   \n",
              "2                         CNNs for Image Recognition   \n",
              "3  Recurrent Neural Networks (RNNs) for Sequentia...   \n",
              "\n",
              "                                            blog_url  \\\n",
              "0  https://www.dataquest.io/blog/tutorial-introdu...   \n",
              "1  https://jonaslalin.com/2021/12/10/feedforward-...   \n",
              "2  https://www.edge-ai-vision.com/2015/11/using-c...   \n",
              "3  https://neptune.ai/blog/recurrent-neural-netwo...   \n",
              "\n",
              "                                   youtube_url  \n",
              "0  https://www.youtube.com/watch?v=aircAruvnKk  \n",
              "1  https://www.youtube.com/watch?v=S5AGN9XfPK4  \n",
              "2  https://www.youtube.com/watch?v=K_BHmztRTpA  \n",
              "3  https://www.youtube.com/watch?v=Or9QSDqzOK0  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df = df.merge(df2, on='Title')\n",
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-n-lHQzhP2gC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cjHBox83HXPk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2EMcCrvvHXML"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "g3O9C-uIHYD7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_DRnIm_uHYAT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RxnjqDkGHkHh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
